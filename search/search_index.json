{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"demos/codellama-continue/codellama-continue/","title":"AI Assistant in Code Server","text":"<p>Who has never dreamed of having a know-it-all assistant that can help you writing boilerplate code, explain a function you got from a colleague but that remains obscure to you, or quickly parse through tons of error logs to fetch the one relevant line that can help you debug you application?</p> <p>With the advent of AI and LLM, such services have become pretty common (Copilot, CodeWhisperer,...). However, you may wonder what happens with all the code you are sending to those services? Or more than often, you're simply not allowed to use them...</p> <p>In this article, we will show you how to integrate a code assistant in Code Server, leveraging the CodeLlama model and the Continue plugin (an open source assistant for VSCode similar to Copilot).</p> <p>All those solutions are open source, you fully control how they are deployed and what they do, and your code will never leave your environment!</p>"},{"location":"demos/codellama-continue/codellama-continue/#requirements","title":"Requirements","text":"<ul> <li>A model that has been trained for Code Generation. Many are available. The Big Code Leaderboard will give you information about the latest and greatest models. In this demo, we are going to use CodeLlama-7b-Instruct-hf because it's good enough for a demo and fits in our available GPU. For large real-world production workloads, you would of course need to consider a bigger version of this model (13B, 70B), or another model.</li> <li>You must serve your model and have its inference endpoint. Refer to the LLM Serving section to learn how to deploy your model on RHOAI/ODH.</li> <li>A Code Server workbench image available in your RHOAI or ODH environment. Starting at RHOAI 2.8, this image is available out of the box. Prior to that, you would have to import it as a custom image, like this one.</li> <li>As Code Server is fully open source, it does not include the Microsoft Marketplace. So you must download the Continue extension file from the Open VSX Registry. Click on the Download button and select the Linux x64 version. You will get a file named <code>Continue.continue-0.9.91@linux-x64.vsix</code> (or whatever version you download).</li> </ul> <p>Note</p> <p>There is a repository containing the files that can be used to deploy some prerequisites for this demo. You can find it here.</p>"},{"location":"demos/codellama-continue/codellama-continue/#installation","title":"Installation","text":"<ul> <li> <p>Create and Start your Workbench based on Code Server:</p> <p></p> </li> <li> <p>Upload the extension file:</p> <p></p> </li> <li> <p>Once uploaded, Code server will try to open the file and complain about its size, just close the tab:1</p> <p></p> </li> <li> <p>From the Extension menu on the sidebar, click on the three dots on the top right and select Install from VSIX:</p> <p></p> </li> <li> <p>From the menu that opens, select the file you uploaded:</p> <p></p> </li> <li> <p>Wait a few seconds... The installation completes and you're greeted with a welcome message:</p> <p></p> </li> <li> <p>You can close all the tabs or follow the tutorial.</p> </li> </ul>"},{"location":"demos/codellama-continue/codellama-continue/#configuration","title":"Configuration","text":"<p>By default, Continue is configured with different providers and models to test. But of course we want to add our own inference endpoint to use the model we have deployed.</p> <ul> <li> <p>In the Sidebar, click on the new icon that was added at the bottom, the one from Continue (it may trigger an error about Headers not being defined, you can ignore it for now). At the bottom right of the Continue panel, click on the Gear icon to open the configuration file.</p> <p></p> </li> <li> <p>In the models section, add the following configuration (replace your inference endpoint with the right value, and eventually the name of your model):</p> <pre><code>{\n    \"title\": \"CodeLlama-7b\",\n    \"model\": \"codellama/CodeLlama-7b-Instruct-hf\",\n    \"apiBase\": \"https://your-inference-endpoint/v1/\",\n    \"completionOptions\": {\n    \"temperature\": 0.1,\n    \"topK\": 1,\n    \"topP\": 1,\n    \"presencePenalty\": 0,\n    \"frequencyPenalty\": 0\n    },\n    \"provider\": \"openai\",\n    \"apiKey\": \"none\"\n}\n</code></pre> </li> <li> <p>You can also remove the other pre-defined models if you don't want to use them. You should end up with something like this:</p> <p></p> </li> <li> <p>In the tabAutocompleteModel section, add the following configuration (replace your inference endpoint with the right value, and eventually the name of your model), and add/modify the options to your liking (see documentation for all possible values):</p> <pre><code>\"tabAutocompleteModel\": {\n    \"title\": \"CodeLlama-7b\",\n    \"model\": \"codellama/CodeLlama-7b-Instruct-hf\",\n    \"apiBase\": \"https://your-inference-endpoint/v1/\",\n    \"completionOptions\": {\n    \"temperature\": 0.1,\n    \"topK\": 1,\n    \"topP\": 1,\n    \"presencePenalty\": 0,\n    \"frequencyPenalty\": 0\n    },\n    \"provider\": \"openai\",\n    \"apiKey\": \"none\"\n},\n\"tabAutocompleteOptions\": {\n    \"useCopyBuffer\": false,\n    \"maxPromptTokens\": 1024,\n    \"prefixPercentage\": 0.5\n},\n</code></pre> </li> <li> <p>You can also disable telemetry by setting the parameter to false. You should end up with something like this:</p> <p></p> </li> <li> <p>Once the configuration is finished (file is auto-saved), you should see the model name available in the drop-down at the bottom of the Continue extension pane:</p> <p></p> </li> </ul> <p>Continue is now ready to use!</p>"},{"location":"demos/codellama-continue/codellama-continue/#usage","title":"Usage","text":"<p>The best way to learn how to use it is to read the short documentation and experiment.</p> <p>Here is a small example of what you can do: Fast Edit (ask for some code generation), then Tab completion (let the assistant suggest the next piece of code), then Explain or do something on the side:</p>"},{"location":"demos/credit-card-fraud-detection-mlflow/credit-card-fraud/","title":"Credit Card Fraud Detection Demo using MLFlow and Red Hat OpenShift AI","text":"<p>Info</p> <p>The full source and instructions for this demo are available in this repo</p>"},{"location":"demos/credit-card-fraud-detection-mlflow/credit-card-fraud/#demo-description-architecture","title":"Demo Description &amp; Architecture","text":"<p>The goal of this demo is to demonstrate how RHOAI and MLFlow can be used together to build an end-to-end MLOps platform where we can:</p> <ul> <li>Build and train models in RHOAI</li> <li>Track and store those models with MLFlow</li> <li>Serve a model stored in MLFlow using RHOAI Model Serving (or MLFlow serving)</li> <li>Deploy a model application in OpenShift that runs sends data to the served model and displays the prediction</li> </ul> <p>The architecture looks like this: </p> <p>Description of each component:</p> <ul> <li>Data Set: The data set contains the data used for training and evaluating the model we will build in this demo.</li> <li>RHOAI Notebook: We will build and train the model using a Jupyter Notebook running in RHOAI.</li> <li>MLFlow Experiment tracking: We use MLFlow to track the parameters and metrics (such as accuracy, loss, etc) of a model training run. These runs can be grouped under different \"experiments\", making it easy to keep track of the runs.</li> <li>MLFlow Model registry: As we track the experiment we also store the trained model through MLFlow so we can easily version it and assign a stage to it (for example Staging, Production, Archive).</li> <li>S3 (ODF): This is where the models are stored and what the MLFlow model registry interfaces with. We use ODF (OpenShift Data Foundation) according to the MLFlow guide, but it can be replaced with another solution.</li> <li>RHOAI Model Serving: We recommend using RHOAI Model Serving for serving the model. It's based on ModelMesh and allows us to easily send requests to an endpoint for getting predictions.</li> <li>Application interface: This is the interface used to run predictions with the model. In our case, we will build a visual interface (interactive app) using Gradio and let it load the model from the MLFlow model registry.</li> </ul> <p>The model we will build is a Credit Card Fraud Detection model, which predicts if a credit card usage is fraudulent or not depending on a few parameters such as: distance from home and last transaction, purchase price compared to median, if it's from a retailer that already has been purchased from before, if the PIN number is used and if it's an online order or not.</p>"},{"location":"demos/credit-card-fraud-detection-mlflow/credit-card-fraud/#deploying-the-demo","title":"Deploying the demo","text":""},{"location":"demos/credit-card-fraud-detection-mlflow/credit-card-fraud/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Have Red Hat OpenShift AI (RHOAI) running in a cluster</li> </ul> <p>Note</p> <p>Note: You can use Open Data Hub instead of RHOAI, but some instructions and screenshots may not apply</p> <ul> <li>Have MLFlow running in a cluster</li> </ul>"},{"location":"demos/credit-card-fraud-detection-mlflow/credit-card-fraud/#11-mlflow-route-through-the-visual-interface","title":"1.1: MLFlow Route through the visual interface","text":"<p>Start by finding your route to MLFlow. You will need it to send any data to MLFlow.</p> <ul> <li>Go to the OpenShift Console as a Developer</li> <li>Select your mlflow project</li> <li>Press Topology</li> <li>Press the mlflow-server circle<ul> <li>While you are at it, you can also press the little \"Open URL\" button in the top right corner of the circle to open up the MLFlow UI in a new tab - we will need it later.</li> </ul> </li> <li>Go to the Resources tab</li> <li>Press mlflow-server under Services</li> <li>Look at the Hostname and mlflow-server Port.</li> </ul> <p>Note</p> <p>This route and port only work internally in the cluster.</p> <p> </p>"},{"location":"demos/credit-card-fraud-detection-mlflow/credit-card-fraud/#12-get-the-mlflow-route-using-command-line","title":"1.2: Get the MLFlow Route using command-line","text":"<p>Alternatively, you can use the OC command to get the hostname through: <code>oc get svc mlflow-server -n mlflow -o go-template --template='{{.metadata.name}}.{{.metadata.namespace}}.svc.cluster.local{{println}}'</code></p> <p>The port you will find with: <code>oc get svc mlflow-server -n mlflow -o yaml</code> </p>"},{"location":"demos/credit-card-fraud-detection-mlflow/credit-card-fraud/#2-create-a-rhoai-workbench","title":"2: Create a RHOAI workbench","text":"<p>Start by opening up RHOAI by clicking on the 9 square symbol in the top menu and choosing \"Red Hat OpenShift AI\".</p> <p></p> <p>Then create a new Data Science project (see image), this is where we will build and train our model. This will also create a namespace in OpenShift which is where we will be running our application after the model is done. I'm calling my project 'Credit Card Fraud', feel free to call yours something different but be aware that some things further down in the demo may change.</p> <p></p> <p>After the project has been created, create a workbench where we can run Jupyter. There are a few important settings here that we need to set:</p> <ul> <li>Name: Credit Fraud Model</li> <li>Notebook Image: Standard Data Science</li> <li>Deployment Size: Small</li> <li>Environment Variable: Add a new one that's a Config Map -&gt; Key/value and enter<ul> <li>Key: <code>MLFLOW_ROUTE</code></li> <li>Value: <code>http://&lt;route-to-mlflow&gt;:&lt;port&gt;</code>, replacing <code>&lt;route-to-mlflow&gt;</code> and <code>&lt;port&gt;</code> with the route and port that we found in step one.  In my case it is <code>http://mlflow-server.mlflow.svc.cluster.local:8080</code>.</li> </ul> </li> <li>Cluster Storage: Create new persistent storage - I call it \"Credit Fraud Storage\" and set the size to 20GB.</li> </ul> <p></p> <p>Press Create Workbench and wait for it to start - status should say \"Running\" and you should be able to press the Open link.</p> <p></p> <p>Open the workbench and login if needed.</p>"},{"location":"demos/credit-card-fraud-detection-mlflow/credit-card-fraud/#3-train-the-model","title":"3: Train the model","text":"<p>When inside the workbench (Jupyter), we are going to clone a GitHub repository which contains everything we need to train (and run) our model. You can clone the GitHub repository by pressing the GitHub button in the left side menu (see image), then select \"Clone a Repository\" and enter this GitHub URL: https://github.com/red-hat-data-services/credit-fraud-detection-demo</p> <p></p> <p>Open up the folder that was added (credit-fraud-detection-demo). It contains:</p> <ul> <li>Data for training and evaluating the model.</li> <li>A notebook (model.ipynb) inside the <code>model</code> folder with a Deep Neural Network model we will train.</li> <li>An application (model_application.py) inside the <code>application</code> folder that will fetch the trained model from MLFlow and run a prediction on it whenever it gets any user input.</li> </ul> <p>The <code>model.ipynb</code> is what we are going to use for building and training the model, so open that up and take a look inside, there is documentation outlining what each cell does. What is particularly interesting for this demo are the last two cells.</p> <p>The second to last cell contains the code for setting up MLFlow tracking:</p> <pre><code>mlflow.set_tracking_uri(MLFLOW_ROUTE)\nmlflow.set_experiment(\"DNN-credit-card-fraud\")\nmlflow.tensorflow.autolog(registered_model_name=\"DNN-credit-card-fraud\")\n</code></pre> <p><code>mlflow.set_tracking_uri(MLFLOW_ROUTE)</code> just points to where we should send our MLFlow data. <code>mlflow.set_experiment(\"DNN-credit-card-fraud\")</code> tells MLFlow that we want to create an experiment, and what we are going to call it. In this case I call it \"DNN-credit-card-fraud\" as we are building a Deep Neural Network. <code>mlflow.tensorflow.autolog(registered_model_name=\"DNN-credit-card-fraud\")</code> enables autologging of a bunch of variables (such as accuracy, loss, etc) so we don't manually have to track them. It also automatically uploads the model to MLFlow after the training completes. Here we name the model the same as the experiment.</p> <p>Then in the last cell we have our training code:</p> <pre><code>with mlflow.start_run():\n    epochs = 2\n    history = model.fit(X_train, y_train, epochs=epochs, \\\n                        validation_data=(scaler.transform(X_val),y_val), \\\n                        verbose = True, class_weight = class_weights)\n\n    y_pred_temp = model.predict(scaler.transform(X_test))\n\n    threshold = 0.995\n\n    y_pred = np.where(y_pred_temp &gt; threshold, 1,0)\n    c_matrix = confusion_matrix(y_test,y_pred)\n    ax = sns.heatmap(c_matrix, annot=True, cbar=False, cmap='Blues')\n    ax.set_xlabel(\"Prediction\")\n    ax.set_ylabel(\"Actual\")\n    ax.set_title('Confusion Matrix')\n    plt.show()\n\n    t_n, f_p, f_n, t_p = c_matrix.ravel()\n    mlflow.log_metric(\"tn\", t_n)\n    mlflow.log_metric(\"fp\", f_p)\n    mlflow.log_metric(\"fn\", f_n)\n    mlflow.log_metric(\"tp\", t_p)\n\n    model_proto,_ = tf2onnx.convert.from_keras(model)\n    mlflow.onnx.log_model(model_proto, \"models\")\n</code></pre> <p><code>with mlflow.start_run():</code> is used to tell MLFlow that we are starting a run, and we wrap our training code with it to define exactly what code belongs to the \"run\". Most of the rest of the code in this cell is normal model training and evaluation code, but at the bottom we can see how we send some custom metrics to MLFlow through <code>mlflow.log_metric</code> and then convert the model to ONNX. This is because ONNX is one of the standard formats for RHOAI Model Serving which we will use later.</p> <p>Now run all the cells in the notebook from top to bottom, either by clicking Shift-Enter on every cell, or by going to Run-&gt;Run All Cells in the very top menu. If everything is set up correctly it will train the model and push both the run and the model to MLFlow. The run is a record with metrics of how the run went, while the model is the actual tensorflow and ONNX model which we later will use for inference. You may see some warnings in the last cell related to MLFlow, as long as you see a final progressbar for the model being pushed to MLFlow you are fine: </p>"},{"location":"demos/credit-card-fraud-detection-mlflow/credit-card-fraud/#4-view-the-model-in-mlflow","title":"4: View the model in MLFlow","text":"<p>Let's take a look at how it looks inside MLFlow now that we have trained the model. If you opened the MLFlow UI in a new tab in step 1.1, then just swap over to that tab, otherwise follow these steps:</p> <ul> <li>Go to the OpenShift Console</li> <li>Make sure you are in Developer view in the left menu</li> <li>Go to Topology in the left menu</li> <li>At the top left, change your project to \"mlflow\" (or whatever you called it when installing the MLFlow operator in pre-requisites)</li> <li>Press the \"Open URL\" icon in the top right of the MLFlow circle in the topology map</li> </ul> <p></p> <p>When inside the MLFlow interface you should see your new experiment in the left menu. Click on it to see all the runs under that experiment name, there should only be a single run from the model we just trained. You can now click on the row in the Created column to get more information about the run and how to use the model from MLFlow.</p> <p></p> <p>We will need the Full Path of the model in the next section when we are going to serve it, so keep this open.</p> <p></p>"},{"location":"demos/credit-card-fraud-detection-mlflow/credit-card-fraud/#5-serve-the-model","title":"5: Serve the model","text":"<p>Note</p> <p>You can either serve the model using RHOAI Model Serving or use the model straight from MLFlow. We will here show how you serve it with RHOAI Model Serving as that scales better for large applications and load. At the bottom of this section we'll go through how it would look like to use MLFlow instead.</p> <p>To start, go to your RHOAI Project and click \"Add data connection\". This data connection connects us to a storage we can load our models from.</p> <p></p> <p>Here we need to fill out a few details. These are all assuming that you set up MLFlow according to this guide and have it connected to ODF. If that's not the case then enter the relevant details for your use case.</p> <ul> <li>Name: mlflow-connection</li> <li>AWS_ACCESS_KEY_ID: Run <code>oc get secrets mlflow-server -n mlflow -o json | jq -r '.data.AWS_ACCESS_KEY_ID|@base64d'</code> in your command prompt, in my case it's <code>nB0z01i0PwD9PMSISQ2W</code></li> <li>AWS_SECRET_ACCESS_KEY: Run <code>oc get secrets mlflow-server -n mlflow -o json | jq -r '.data.AWS_SECRET_ACCESS_KEY|@base64d'</code> in your command prompt, in my case it's <code>FLgEJmGQm5CdRQRnXc8jVFcc+QDpM1lcrGpiPBzI</code>.</li> </ul> <p>Note</p> <p>In my case the cluster and storage has already been shut down, don't share this in normal cases.</p> <ul> <li>AWS_S3_ENDPOINT: Run <code>oc get configmap mlflow-server -n mlflow -o yaml | grep BUCKET_HOS</code> in your command prompt, in my case it's <code>http://s3.openshift-storage.svc</code></li> <li>AWS_DEFAULT_REGION: Where the cluster is being ran</li> <li>AWS_S3_BUCKET: Run <code>oc get obc -n mlflow -o yaml | grep bucketName</code> in your command prompt, in my case it's <code>mlflow-server-576a6525-cc5b-46cb-95f3-62c3986846df</code></li> </ul> <p>Then press \"Add data connection\". Here's an example of how it can look like: </p> <p>Then we will configure a model server, which will serve our models.</p> <p></p> <p>Just check the 'Make deployed available via an external route' checkbox and then press \"Configure\" at the bottom.</p> <p>Finally, we will deploy the model, to do that, press the \"Deploy model\" button which is in the same place that \"Configure Model\" was before. We need to fill out a few settings here:</p> <ul> <li>Name: credit card fraud</li> <li>Model framework: onnx-1 - Since we saved the model as ONNX in the model training section</li> <li>Model location:<ul> <li>Name: <code>mlflow-connection</code></li> <li>Folder path: This is the full path we can see in the MLFlow interface from the end of the previous section. In my case it's <code>1/b86481027f9b4b568c9efa3adc01929f/artifacts/models/</code>. Beware that we only need the last part, which looks something like: <code>1/..../artifacts/models/</code> </li> </ul> </li> </ul> <p></p> <p>Press Deploy and wait for it to complete. It will show a green checkmark when done. You can see the status here:</p> <p></p> <p>Click on \"Internal Service\" in the same row to see the endpoints, we will need those when we deploy the model application.</p> <p>[Optional] MLFlow Serving:</p> <p>This section is optional</p> <p>This section explains how to use MLFlow Serving instead of RHOAI Model Serving. We recommend using RHOAI Model Serving as it scales better. However, if you quickly want to get a model up and running for testing, this would be an easy way.</p> <p>To use MLFlow serving, simply deploy an application which loads the model straight from MLFlow. You can find the model application code for using MLFlow serving in the \"application_mlflow_serving\" folder in the GitHub repository you cloned in step 3.</p> <p>If you look inside <code>model_application_mlflow_serve.py</code> you are going to see a few particularly important lines of code:</p> <pre><code># Get a few environment variables. These are so we can:\n# - get data from MLFlow\n# - Set server name and port for Gradio\nMLFLOW_ROUTE = os.getenv(\"MLFLOW_ROUTE\")\n...\n\n# Connect to MLFlow using the route.\nmlflow.set_tracking_uri(MLFLOW_ROUTE)\n\n# Specify what model and version we want to load, and then load it.\nmodel_name = \"DNN-credit-card-fraud\"\nmodel_version = 1\nmodel = mlflow.pyfunc.load_model(\n    model_uri=f\"models:/{model_name}/{model_version}\"\n)\n</code></pre> <p>Here is where we set up everything that's needed for loading the model from MLFlow. The environment variable MLFLOW_ROUTE is set in the Dockerfile. You can also see that we specifically load version 1 of the model called \"DNN-credit-card-fraud\" from MLFlow. This makes sense since we only ran the model once, but is easy to change if any other version or model should go into production</p> <p>Follow the steps of the next section to see how to deploy an application, but when given the choice for \"Context dir\" and \"Environment variables (runtime only)\", use these settings instead:</p> <ul> <li>Context dir: \"/model_application_mlflow_serve\"</li> <li>Environment variables (runtime only) fields:<ul> <li>Name: <code>MLFLOW_ROUTE</code></li> <li>Value: The MLFlow route from step one (<code>http://mlflow-server.mlflow.svc.cluster.local:8080</code> for example)</li> </ul> </li> </ul>"},{"location":"demos/credit-card-fraud-detection-mlflow/credit-card-fraud/#6-deploy-the-model-application","title":"6: Deploy the model application","text":"<p>The model application is a visual interface for interacting with the model. You can use it to send data to the model and get a prediction of whether a transaction is fraudulent or not. You can find the model application code in the \"application\" folder in the GitHub repository you cloned in step 3.</p> <p></p> <p>If you look inside it <code>model_application.py</code>, you will see two particularly important lines of code:</p> <pre><code># Get a few environment variables. These are so we:\n# - Know what endpoint we should request\n# - Set server name and port for Gradio\nURL = os.getenv(\"INFERENCE_ENDPOINT\") &lt;----------\n...\n\n    response = requests.post(URL, json=payload, headers=headers)  &lt;----------\n</code></pre> <p>This is what we use to send a request to our RHOAI Model Server with some data we want it to run a prediction on.</p> <p>We are going to deploy the application with OpenShift by pointing to the GitHub repository. It will pull down the folder, automatically build a container image based on the Dockerfile, and publish it.</p> <p>To do this, go to the OpenShift Console and make sure you are in Developer view and have selected the credit-card-fraud project. Then press \"+Add\" in the left menu and select Import from Git.</p> <p></p> <p>In the \"Git Repo URL\" enter: https://github.com/red-hat-data-services/credit-fraud-detection-demo (this is the same repository we pulled into RHOAI earlier). Then press \"Show advanced Git options\" and set \"Context dir\" to \"/application\". Finally, at the very bottom, click the blue \"Deployment\" link:</p> <p></p> <p>Set these values in the Environment variables (runtime only) fields:</p> <ul> <li>Name: <code>INFERENCE_ENDPOINT</code></li> <li>Value: In the RHOAI projects interface (from the previous section), copy the \"restURL\" and add <code>/v2/models/credit-card-fraud/infer</code> to the end if it's not already there. For example: <code>http://modelmesh-serving.credit-card-fraud:8008/v2/models/credit-card-fraud/infer</code> </li> </ul> <p>Your full settings page should look something like this:</p> <p></p> <p>Press Create to start deploying the application.</p> <p>You should now see three objects in your topology map, one for the Workbench we created earlier, one for the model serving, and one for the application we just added. When the circle of your deployment turns dark blue it means that it has finished deploying.</p> <p>If you want more details on how the deployment is going, you can press the circle and look at Resources in the right menu that opens up. There you can see how the build is going and what's happening to the pod. The application will be ready when the build is complete and the pod is \"Running\".</p> <p>When the application has been deployed you can press the \"Open URL\" button to open up the interface in a new tab.</p> <p></p> <p>Congratulations, you now have an application running your AI model!</p> <p>Try entering a few values and see if it predicts it as a credit fraud or not. You can select one of the examples at the bottom of the application page.</p> <p></p>"},{"location":"demos/financial-fraud-detection/financial-fraud-detection/","title":"Financial Fraud Detection","text":"<p>Info</p> <p>The full source and instructions for this demo are available on this repo</p> <p>This demo shows how to use OpenShift AI to train and test a relatively simplistic fraud detection model. In exploring this content, you will become familiar with the OpenShift AI offering and common workflows to use with it.</p>"},{"location":"demos/llama2-finetune/llama2-finetune/","title":"Fine-Tune Llama 2 Models with Ray and DeepSpeed","text":"<p>In this repo, you will find an example demonstrating how to fine-tune LLMs with Ray on OpenShift AI, using HF Transformers, Accelerate, PEFT (LoRA), and DeepSpeed, for Llama 2 models.</p> <p>It adapts the Fine-tuning Llama-2 series models with Deepspeed, Accelerate, and Ray Train TorchTrainer<sup>1</sup> example from the Ray project, so it runs using the Distributed Workloads stack, on OpenShift AI.</p> <p>Overview of the Ray Dashboard during the fine-tuning: </p> <p>Example of a visualization comparing experiments with different combinations of context length and batch size: </p> <ol> <li> <p>https://github.com/ray-project/ray/tree/master/doc/source/templates/04_finetuning_llms_with_deepspeed \u21a9</p> </li> </ol>"},{"location":"demos/llama3-finetune/llama3-finetune-kfto/","title":"Fine-Tune Llama Models with Kubeflow Training Operator","text":"<p>In this repo, you will find an example showing how to fine-tune Large Language Models with the Kubeflow Training Operator on OpenShift AI. This example uses tools like HuggingFace SFTTrainer, PEFT (LoRA) and PyTorch FSDP, for Llama 3.1 and 3.3 models. This approach is built for Kubernetes and provides a simple way to train large models across multiple GPUs (Nvidia &amp; AMD)</p> <p>Openshift AI workbench setup with all required components and shared storage: </p> <p>TensorBoard dashboard showing successful model finetuning using NVIDIA GPUs: </p> <p>TensorBoard dashboard showing successful model finetuning on AMD GPUs: </p> <ol> <li> <p>[https://github.com/opendatahub-io/distributed-workloads/tree/main/examples/kfto-sft-llm]\u00a0\u21a9</p> </li> </ol>"},{"location":"demos/llm-chat-doc/llm-chat-doc/","title":"Chat with your Documentation","text":"<p>If you want to learn more about LLMs and how to serve them, please read the LLM Serving documentation first.</p>"},{"location":"demos/llm-chat-doc/llm-chat-doc/#rag-chatbot-full-walkthrough","title":"RAG Chatbot Full Walkthrough","text":"<p>Although the available code is normally pretty well documented, especially the notebooks, giving a full overview will surely help you understand how all of the different elements fit together.</p> <p>For this walkthrough we will be using this application, which is a RAG-based Chatbot that will use a Milvus vector store, vLLM for LLM serving, Langchain as the \"glue\" between those components, and Gradio as the UI engine.</p>"},{"location":"demos/llm-chat-doc/llm-chat-doc/#requirements","title":"Requirements","text":"<ul> <li>An OpenShift cluster with RHOAI or ODH deployed.</li> <li>A node with a GPU card. For the model we will use, 24GB memory on the GPU (VRAM) is necessary. If you have less than that you can either use quantization when loading the model, use an already quantized model (results may vary as they are not all compatible with the model server), or choose another compatible smaller model.</li> </ul>"},{"location":"demos/llm-chat-doc/llm-chat-doc/#model-serving","title":"Model Serving","text":"<p>Deploy vLLM Model Serving instance in the OpenAI compatible API mode, either:</p> <ul> <li>as a custom server runtime in ODH/RHOAI.</li> <li>as a standalone server in OpenShift.</li> </ul> <p>In both cases, make sure you deploy the model <code>mistralai/Mistral-7B-Instruct-v0.2</code>.</p>"},{"location":"demos/llm-chat-doc/llm-chat-doc/#vector-store","title":"Vector Store","text":""},{"location":"demos/llm-chat-doc/llm-chat-doc/#milvus-deployment","title":"Milvus deployment","text":"<p>For our RAG we will need a Vector Database to store the Embeddings of the different documents. In this example we are using Milvus.</p> <p>Deployment instructions specific to OpenShift are available here.</p> <p>After you follow those instructions you should have a Milvus instance ready to be populated with documents.</p>"},{"location":"demos/llm-chat-doc/llm-chat-doc/#document-ingestion","title":"Document ingestion","text":"<p>In this notebook you will find detailed instructions on how to ingest different types of documents: PDFs first, then Web pages.</p> <p>The examples are based on RHOAI documentation, but of course we encourage you to use your own documentation. After all that's the purpose of all of this!</p> <p>This other notebook will allow you to execute simple queries against your Vector Store to make sure it works alright.</p> <p>Note</p> <p>Those notebooks are using the NomicAI Embeddings to create and query the collection. If you want to use the default embeddings from Langchain, other notebooks are available. They have the same name, just without the <code>-nomic</code> at the end.</p>"},{"location":"demos/llm-chat-doc/llm-chat-doc/#testing","title":"Testing","text":"<p>Now let's put all of this together!</p> <p>This notebook will be used to create a RAG solution leveraging the LLM and the Vector Store we just populated. Don't forget to enter the relevant information about your Model Server (the Inference URL and model name), and about your Vector store (connection information and collection name) on the third cell.</p> <p>You can also adjust other parameters as you see fit.</p> <p></p> <ul> <li>It will first initialize a connection to the vector database (embeddings are necessary for the Retriever to \"understand\" what is stored in the database):</li> </ul> <pre><code>model_kwargs = {'trust_remote_code': True}\nembeddings = HuggingFaceEmbeddings(\n    model_name=\"nomic-ai/nomic-embed-text-v1\",\n    model_kwargs=model_kwargs,\n    show_progress=False\n)\n</code></pre> <ul> <li>A prompt template is then defined. You can see that we will give it specific instructions on how the model must answer. This is necessary if you want to keep it focused on its task and not say anything that may not be appropriate (on top of getting you fired!). The format of this prompt is originally the one used for Llama2, but Mistral uses the same one. You may have to adapt this format if you use another model.</li> </ul> <pre><code>template=\"\"\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\nYou are a helpful, respectful and honest assistant named HatBot answering questions.\nYou will be given a question you need to answer, and a context to provide you with information. You must answer the question based as much as possible on this context.\nAlways answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n&lt;&lt;/SYS&gt;&gt;\n\nContext: \n{context}\n\nQuestion: {question} [/INST]\n\"\"\"\n</code></pre> <ul> <li>Now we will define the LLM connection itself. As you can see there are many parameters you can define that will modify how the model will answer. Details on those parameters are available here.</li> </ul> <pre><code>llm =  VLLMOpenAI(\n    openai_api_key=\"EMPTY\",\n    openai_api_base=INFERENCE_SERVER_URL,\n    model_name=MODEL_NAME,\n    max_tokens=MAX_TOKENS,\n    top_p=TOP_P,\n    temperature=TEMPERATURE,\n    presence_penalty=PRESENCE_PENALTY,\n    streaming=True,\n    verbose=False,\n    callbacks=[StreamingStdOutCallbackHandler()]\n)\n</code></pre> <ul> <li>And finally we can tie it all together with a specific chain, RetrievalQA:</li> </ul> <pre><code>qa_chain = RetrievalQA.from_chain_type(\n        llm,\n        retriever=store.as_retriever(\n            search_type=\"similarity\",\n            search_kwargs={\"k\": 4}\n            ),\n        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n        return_source_documents=True\n        )\n</code></pre> <ul> <li>That's it! We can now use this chain to send queries. The retriever will look for relevant documents in the Vector Store, their content will be injected automatically in the prompt, and the LLM will try to create a valid answer based on its own knowledge and this content:</li> </ul> <pre><code>question = \"How can I create a Data Science Project?\"\nresult = qa_chain.invoke({\"query\": question})\n</code></pre> <ul> <li>The last cell in the notebook will simply filter for duplicates in the sources that were returned in the <code>result</code>, and display them:</li> </ul> <pre><code>def remove_duplicates(input_list):\n    unique_list = []\n    for item in input_list:\n        if item.metadata['source'] not in unique_list:\n            unique_list.append(item.metadata['source'])\n    return unique_list\n\nresults = remove_duplicates(result['source_documents'])\n\nfor s in results:\n    print(s)\n</code></pre>"},{"location":"demos/llm-chat-doc/llm-chat-doc/#application","title":"Application","text":"<p>Notebooks are great and everything, but it's not what you want to show to your users. I hope...</p> <p>So instead, here is a simple UI that implements mostly the same code we used in the notebooks.</p> <p></p> <p>The deployment is already explained in the repo and pretty straightforward as the application will only \"consume\" the same Vector Store and LLM Serving we have used in the notebooks. However I will point out some specificities:</p> <ul> <li>This implementation allows you to have different collections in Milvus you can query from. This is fully configurable, you can create as many collections as you want and add them to the application.</li> <li>The code is more complicated than the notebooks as it allows for multiple users to use the application simultaneously. They can all use a different collection, ask questions at the same time, they stay fully isolated. The limitation is the memory you have.</li> <li>Most (if not all) parameters are configurable. They are all described in the README file.</li> </ul> <p>Some info on the code itself (<code>app.py</code>):</p> <ul> <li><code>load_dotenv</code>, along with the <code>env.example</code> file (once renamed <code>.env</code>) will allow you to develop locally.</li> <li>As normally your Milvus instance won't be exposed externally to OpenShift, if you want to develop locally you may want to open a tunnel to it with <code>oc port-forward Service/milvus-service 19530:19530</code> (replace with the name of the Milvus Service along with the ports if you change them). You can use the same technique for the LLM endpoint if you have not exposed it as a route.</li> <li>The class <code>QueueCallback</code> was necessary because the <code>vLLMOpenAI</code> library used to query the model does not return an iterator in the format Langchain expects it (at the time of this writing). Instead, this implementation of the Callback functions for the LLM puts the new tokens in a Queue (L43) that is then retrieved from continuously (L65), with the content being yielded for display. This is a little bit convoluted, but the whole stack is still in full development, so sometimes you have to be creative...</li> <li>The default Milvus Retriever (same for almost all vector databases in Langchain) does not allow to filter on the score. This means that whatever your query, some documents will always be fetched and passed into the context. This is an unwanted behavior if the query has no relation to the knowledge base you are using. So I created a custom Retriever Class, in the file <code>milvus_retriever_with_score_threshold.py</code> that allows to filter the documents according to score. NOTE: this a similarity search with a cosine score, so the lesser, the better. The threshold calculation is \"no more than...\".</li> <li>Gradio configuration is pretty straightforward trough the ChatInterface component, only hiding some buttons, adding an avatar image for the bot,... The only notable thing is the use of a State variable for the selected collection so that a switch from one collection to the other is not applied to all users (this is an early mistake I made \ud83d\ude0a) .</li> </ul> <p>Here is what you RAG-based Chatbot should look like:</p>"},{"location":"demos/modelops-benchmarking/modelops-benchmarking/","title":"Secure Model Ingestion and Evaluation with OpenShift AI","text":""},{"location":"demos/modelops-benchmarking/modelops-benchmarking/#the-llm-selection-problem","title":"The LLM Selection Problem","text":"<p>As organizations rush to adopt AI they quickly find that LLMs are not created equal and model selection is not an easy task. Allowing developers to download and deploy any model they want is not sustainable. This creates massive security vulnerabilities, leads to resource waste, and provides no clear way to compare model capabilities for a given business problem.</p>"},{"location":"demos/modelops-benchmarking/modelops-benchmarking/#the-solution-a-two-part-modelops-strategy","title":"The Solution: A Two-Part ModelOps Strategy","text":"<p>A successful enterprise AI strategy requires a formal process for managing the LLM lifecycle. This involves two key groups: a central ModelOps team and the AI Engineers building applications.</p>"},{"location":"demos/modelops-benchmarking/modelops-benchmarking/#part-1-the-modelops-team","title":"Part 1: The ModelOps Team","text":"<p>The ModelOps team acts as the gatekeeper. They are responsible for:</p> <ul> <li> <p>Secure Ingestion: Creating an OpenShift pipeline to safely bring new models inside the organization's network.</p> </li> <li> <p>Baseline Validation: Before a model is made available for internal use, baseline benchmarks with GuideLLM and lm-eval harness should be run and published.</p> </li> <li> <p>Infrastructure Performance (GuideLLM): How fast does this model run on our hardware?</p> </li> <li> <p>General Accuracy (lm-eval): How does it score on standard academic evaluation datasets?</p> </li> </ul> <p>This process creates a central registry (OpenShift AI model registry) of approved models that gives teams a safe, documented starting point.</p> <p></p>"},{"location":"demos/modelops-benchmarking/modelops-benchmarking/#part-2-the-ai-engineer","title":"Part 2: The AI Engineer","text":"<p>AI Engineers can now browse the internal registry and use the baseline benchmarks to create a shortlist. However, a general-purpose benchmark doesn't guarantee performance for a specific use case.</p> <p>Therefore, the AI Engineer's workflow becomes:</p> <ul> <li> <p>Discovery: Find a promising model in the OpenShift AI model registry based on the ModelOps team's general benchmarks.</p> </li> <li> <p>Specific Evaluation: Run that model through an OpenShift AI pipeline using their own custom, use-case-specific data.</p> </li> </ul> <p>This final step is what provides the definitive answer, allowing the AI Engineer to confidently determine if a model's performance and accuracy are a true fit for their unique application.</p> <p></p>"},{"location":"demos/modelops-benchmarking/modelops-benchmarking/#tutorials","title":"Tutorials","text":"<ol> <li> <p>ModelOps Tutorial  (est. 1 hour)</p> <ul> <li>For the ModelOps role that covers part 1 of the solution presented above.</li> </ul> </li> <li> <p>AI Engineer Tutorial (est. 30 minutes)</p> <ul> <li>For the AI Engineer role that covers part 2 of the solution presented above.</li> </ul> <p>NOTE: The ModelOps tutorial must be completed before you can start the AI Engineer tutorial.</p> </li> </ol>"},{"location":"demos/modelops-benchmarking/modelops-benchmarking/#pre-reqs","title":"Pre-reqs","text":"<ul> <li>OpenShift is installed</li> <li>OpenShift AI is installed</li> <li>OpenShift AI is configured with a GPU</li> <li>This tutorial was test with OpenShift AI 2.25 and OpenShift 4.19</li> </ul>"},{"location":"demos/modelops-benchmarking/modelops-benchmarking/#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>Implement a Two-Part ModelOps Strategy: Clearly distinguish between the responsibilities of the ModelOps or IT Operations team and a consuming AI Engineer or Developer team.</li> <li>Understand how a central, trusted model registry acts as the hand-off between operations teams and application teams.</li> <li>Configure OpenShift AI model registry and AI pipelines. </li> <li>In the ModelOps tutorial, deploy and run a GuideLLM and lm-eval harness OpenShift pipeline.</li> <li>In the AI Engineer tutorial, deploy and run a GuideLLM and lm-eval harness OpenShift AI Elyra pipeline that is compiled into a Kubeflow pipeline. </li> <li>See how the benchmark results are stored in the OpenShift AI model registry to easily compare results based on default and custom datasets.   </li> </ul>"},{"location":"demos/modelops-benchmarking/modelops-benchmarking/#personas","title":"Personas","text":"<p>The terms ModelOps and AI Engineer are used throughout this tutorial. Since these might not be well defined or dedicated roles in your organization we'll offer the following role mapping for this tutorial:</p> <ul> <li>ModelOps - In your organization this could be your IT operations team or DevOps team.</li> <li>AI Engineer - In your organization this could be an application developer or a data scientist.</li> </ul> <p>Special Thanks</p> <p>Parts of this tutorial builds on or reuses some of the awesome work that Taylor Smith and Roberto Carratal\u00e1 have created. </p>"},{"location":"demos/podman-ai-lab-to-rhoai/podman-ai-lab-to-rhoai/","title":"From Podman AI Lab to OpenShift AI","text":"<p>Overview</p> <p>A common statement from many people today is \"I want the ability to chat with my documents\". They might be aware that Large Language Models (LLMs) provide them the ability to do that, but the implementation details are unknown as well as the possible risks. These unknowns make it difficult to understand where to start. </p> <p>Thankfully, there is a path forward with the newly released Podman AI Lab extension. Podman AI Lab allows you to pull down models and test them out locally to see how they perform and which one(s) will work best for your use case(s). The chatbot recipe within Podman AI Lab makes integrating LLMs with applications as easy as the click of a button.</p> <p>Podman AI Lab is an excellent place to evaluate and test models, but you'll eventually want to see how this will actually be deployed in your enterprise. For that, we can use OpenShift and OpenShift AI along with the Elasticsearch vector database to create a Retrieval Augmented Generation (RAG) chatbot.</p> <p>This article will walk you through how to go from a chatbot recipe in the Podman AI Lab extension to a RAG chatbot deployed on OpenShift and OpenShift AI. </p>"},{"location":"demos/podman-ai-lab-to-rhoai/podman-ai-lab-to-rhoai/#high-level-architecture","title":"High Level Architecture","text":"<ol> <li> <p>An LLM is downloaded through Podman AI Lab.</p> </li> <li> <p>A chatbot recipe is started in Podman AI Lab with the downloaded model.</p> </li> <li> <p>The chatbot recipe code from Podman AI Lab is updated in VS Code with LangChain to connect to the Elasticsearch vector database and OpenShift AI model serving inference endpoint.</p> </li> <li> <p>An ingestion notebook is run in OpenShift AI to add data to the Elasticsearch vector database. </p> </li> <li> <p>The LLM we downloaded from Podman AI Lab is deployed to OpenShift AI on a custom serving runtime.</p> </li> <li> <p>The updated chatbot with LangChain is built as a container and deployed to OpenShift.</p> </li> </ol>"},{"location":"demos/podman-ai-lab-to-rhoai/podman-ai-lab-to-rhoai/#requirements","title":"Requirements","text":"<p>It is expected that you have admin access to an OpenShift 4.12+ cluster. The following code was tested with an OpenShift 4.18 cluster and OpenShift AI 2.16.</p>"},{"location":"demos/podman-ai-lab-to-rhoai/podman-ai-lab-to-rhoai/#podman-ai-lab","title":"Podman AI Lab","text":""},{"location":"demos/podman-ai-lab-to-rhoai/podman-ai-lab-to-rhoai/#install-podman-desktop-and-podman-ai-lab-extension","title":"Install Podman Desktop and Podman AI Lab extension","text":"<p>Follow the installation instructions for Podman Desktop and the Podman AI Lab extension in the below Red Hat Developer article. The article also gives a great overview of the features in Podman AI Lab.  Podman AI Lab - Getting Started</p>"},{"location":"demos/podman-ai-lab-to-rhoai/podman-ai-lab-to-rhoai/#download-the-model","title":"Download the model","text":"<p>We will be downloading and using the TheBloke/Mistral-7B-Instruct-v0.2-GGUF. This model is quantised (smaller) version of the full Mistral-7B-Instruct-v0.2. The smaller model will allow us to run inferencing on CPUs if GPUs are not an option.</p> <ol> <li> <p>Go to the AI Lab extension and select Catalog under Models. </p> </li> <li> <p>If you haven't already, download the TheBloke/Mistral-7B-Instruct-v0.2-GGUF model.  </p> </li> <li> <p>The model is around 4GB so it might take some time.  </p> </li> <li> <p>Podman AI Lab allows you to get started quickly with downloaded models through Services, Playgrounds, and the Recipes Catalog. </p> <ol> <li> <p>Services -   The Services section allows you to create a model service endpoint for models you've downloaded. Client code is provided (cURL by default) in multiple formats to get you started quickly with sending in requests to the model service endpoint.    </p> </li> <li> <p>Playgrounds -   The Playgrounds area allows you to define system prompts and experiment with different settings like temperature, max tokens, and top-p.   </p> </li> <li> <p>Recipes Catalog -   The Recipes Catalog contains demo applications for Natural Language Processing (NLP), Computer Vision, and Audio. We'll be using the ChatBot recipe demo code in this example.</p> </li> </ol> </li> <li> <p>Create the Chatbot - Make sure to select TheBloke/Mistral-7B-Instruct-v0.2-GGUF as your model and then click Start AI App button. </p> </li> <li> <p>After the chatbot has started open it up to test it out. </p> </li> <li> <p>At the bottom of the AI App Details section you'll see a Open in VSCode button. Clicking on that will open all of the code that is running your chatbot. Later we'll modify that code to connect langchain, TheBloke/Mistral-7B-Instruct-v0.2-GGUF model, and the Elasticsearch Vector Database.  </p> </li> </ol>"},{"location":"demos/podman-ai-lab-to-rhoai/podman-ai-lab-to-rhoai/#deploying-openshift-ai","title":"Deploying OpenShift AI","text":"<p>Optional: If you already have an OpenShift AI instance with a Data Science Cluster you can skip this section.</p> <p>Follow the product documentation to install OpenShift AI.</p> <p>OR</p> OpenShift AI automated install <ol> <li> <p>Clone podman-ai-lab-to-rhoai</p> </li> <li> <p>Login to your OpenShift cluster in a terminal with the API token. You can get your API token from the OpenShift web console.  </p> <p> <pre><code>oc login --token=&lt;YOUR_OPENSHIFT_API_TOKEN&gt; --server=https://&lt;YOUR_OPENSHIFT_API_URL&gt;:6443\n</code></pre></p> </li> <li> <p>We'll first deploy the OpenShift AI operator.      <pre><code>oc apply -k ./components/openshift-ai/operator/overlays/fast\n</code></pre></p> </li> <li> <p>Now we'll create a Data Science Cluster. Make sure the operator is fully deployed before creating the Data Science Cluster.     <pre><code>watch oc get pods -n redhat-ods-operator\n</code></pre></p> <p></p> <p>Once the pod has a Running status and is ready you can run the below command.  <pre><code>oc apply -k ./components/openshift-ai/instance/overlays/fast\n</code></pre></p> </li> </ol>"},{"location":"demos/podman-ai-lab-to-rhoai/podman-ai-lab-to-rhoai/#deploy-elasticsearch-vector-db","title":"Deploy Elasticsearch Vector DB","text":"<p>Optional: If you already have an Elasticsearch instance you can skip this section.</p> <p>Follow the product documentation to install Elasticsearch </p> <p>OR </p> Elasticsearch automated install <ol> <li> <p>Clone podman-ai-lab-to-rhoai</p> </li> <li> <p>We'll now deploy the Elasticsearch operator. This will be our vector database.</p> <pre><code>oc apply -k ./components/elasticsearch/base/\n</code></pre> </li> <li> <p>Now we can create an Elasticsearch cluster instance. Make sure the Elasticsearch operator pod is in a running state and ready.     <pre><code>watch oc get pods -n elastic-vectordb\n</code></pre></p> <p></p> <pre><code>oc apply -f ./components/elasticsearch/cluster/instance.yaml\n</code></pre> </li> </ol>"},{"location":"demos/podman-ai-lab-to-rhoai/podman-ai-lab-to-rhoai/#ingest-data-into-the-elasticsearch-vector-database","title":"Ingest data into the Elasticsearch Vector Database","text":"<p>Now that the Elasticsearch operator has been deployed and an instance created, we can ingest some data and query the vector database.</p> <ol> <li> <p>Go to your OpenShift AI Dashboard. You can get the URL either from the below oc command or the OpenShift web console.     <pre><code>oc get routes -n redhat-ods-applications\n</code></pre>     OR</p> <p>Select the redhat-ods-applications project, the Networking -&gt; Routes, and then open the URL under Location.</p> <p></p> </li> <li> <p>Login to the dashboard and select Data Science Projects and click the Create Data Science Project button.</p> <p></p> </li> <li> <p>Name the project podman-ai-lab-rag-project and click the Create button.</p> <p></p> </li> <li> <p>We'll now create a workbench where we can upload a Jupyter notebook to ingest data into the Elasticsearch vector DB. We'll then test it out by querying for some data. Click the Create a workbench button.</p> <p></p> </li> <li> <p>Name the workbench elastic-vectordb-workbench, select a Standard Data Science notebook, and select a Medium size.</p> <p></p> <p>You'll also want to set two environment variables so we can connect to Elasticsearch from the notebook. </p> <ul> <li> <p>CONNECTION_STRING - Copy the CLUSTER-IP and PORT from this oc command</p> <pre><code>oc get service elasticsearch-sample-es-http -n elastic-vectordb\n</code></pre> <p>Add the CONNECTION_STRING key/value as a ConfigMap environment variable.</p> </li> <li> <p>PASSWORD - Create a secret environment variable with the Elasticsearch secret value.     <pre><code>oc get secret elasticsearch-sample-es-elastic-user -n elastic-vectordb -o jsonpath=\"{.data['elastic']}\" | base64 -d &gt; elastic_pass.txt\n</code></pre></p> <p>Add the PASSWORD key/value as a Secret environment variable. The password is in the elastic_pass.txt file that was created by the above oc command.</p> </li> </ul> <p></p> <p>NOTE: You can delete the elastic_pass.txt file that you got the password from after you add it to the environment variable.</p> <p>Click on the Create Workbench button. Your workbench should start in a few minutes.</p> </li> <li> <p>Open your workbench after it has started and login.</p> <p>Note: If you have insufficient resources to start a medium container size then stop the workbench and change the workbench to start as a small container size.</p> </li> <li> <p>Upload or import the ./notebooks/Langchain-ElasticSearchVector-Ingest.ipynb notebook to your workbench.</p> <p></p> </li> <li> <p>Run the first 3 cells (Shift + Enter in each cell to run). Make sure you can connect to your Elasticsearch cluster.</p> <p></p> </li> <li> <p>Continue to run through each cell while reading through what is occurring in each one. The Create the index and ingest the documents cell is where all of the websites and pdfs are stored with embeddings into the Elasticsearch vector database.</p> <p>This cell will take a while to run. Good time for a coffee break.</p> <p></p> </li> <li> <p>After all of the data is stored into our vector database we can directly query it. Run the last 2 cells to make sure the data was stored successfully and we get back the results we expect.</p> <p></p> </li> </ol>"},{"location":"demos/podman-ai-lab-to-rhoai/podman-ai-lab-to-rhoai/#deploy-s3-storage-minio","title":"Deploy s3 Storage (Minio)","text":"<p>Optional: If you already have s3 compatible storage you can skip to step 2 to create the bucket.</p> <p>OpenShift AI model serving has a dependency on s3 storage. We'll deploy Minio for this tutorial, but any s3 compatible storage should work. For an enterprise s3 storage solution consider OpenShift Data Foundation.</p> <p>Follow the Minio Installation if you don't have s3 compatible storage.</p> <ol> <li> <p>Login to the Minio UI. You can find the route in either the web console or from the oc cli in your terminal. Login with minio/minio123. Minio contains 2 routes, an API route and UI route. Make sure you use the UI route.</p> <p></p> </li> <li> <p>Create a bucket named models and click the Create Bucket button.</p> <p></p> </li> <li> <p>Go to Object Browser, select the models bucket you just created, and click the Create new path button. Name the folder path mistral7b and click the Create button.</p> <p></p> </li> <li> <p>Upload the Mistral7b model to the folder path you just created. You can find out where the model was downloaded if you go back to Podman AI Lab and click the Open Model Folder icon.</p> <p></p> <p>In Minio click on the Upload File button and select the model file under the hf.TheBloke.mistral-7b-instruct-v0.2.Q4_K_M directory.</p> <p></p> </li> <li> <p>If the model is uploaded successfully you should see the below screen.</p> <p></p> </li> </ol>"},{"location":"demos/podman-ai-lab-to-rhoai/podman-ai-lab-to-rhoai/#create-custom-model-serving-runtime","title":"Create Custom Model Serving Runtime","text":"<p>Follow the product documentation to install the single-model serving platform.</p> <p>OR </p> Single-model serving platform automated install <ol> <li> <p>Clone podman-ai-lab-to-rhoai</p> </li> <li> <p>We first need to enable the single serving runtime before we can add our custom serving runtime.</p> <ol> <li>Run the following oc command to deploy Service Mesh.     <pre><code>oc apply -k ./components/openshift-servicemesh/operator/overlays/stable\n</code></pre></li> <li> <p>Run the following oc command to deploy Serverless.     <pre><code>oc apply -k ./components/openshift-serverless/operator/overlays/stable\n</code></pre></p> </li> <li> <p>Wait until the Service Mesh and Serverless operators have installed successfully.      <pre><code>watch oc get csv -n openshift-operators\n</code></pre></p> <p></p> </li> <li> <p>We'll be using the single stack serving in OpenShift AI so we'll want use a trusted certificate instead of a self signed one. This will allow our chatbot to access the model inference endpoint.</p> <p>Run the below oc commands</p> <ol> <li> <p>Get the name of the ingress cert we will need to copy. Select a secret that has cert in the name.     <pre><code>oc get secrets -n openshift-ingress | grep cert\n</code></pre></p> </li> <li> <p>Copy the full name of the secret you chose and replace the name in the below oc command. Make sure you're in the top level directory of this project and run the below command.     <pre><code>oc extract secret/&lt;CERT_SECRET_FROM_ABOVE&gt; -n openshift-ingress --to=ingress-certs --confirm\n</code></pre></p> <p>You should now have a ingress-certs directory with a tls.crt and tls.key file.</p> <p></p> </li> <li> <p>We'll now update the secret that will be used in our OpenShift AI data science cluster.</p> <pre><code>cd ingress-certs\n\noc create secret generic knative-serving-cert -n istio-system --from-file=. --dry-run=client -o yaml | oc apply -f -\n\ncd ..\n</code></pre> <p>NOTE: You can delete the ingress-certs folder after you have created the knative-serving-cert secret.</p> </li> <li> <p>To avoid possible SSL errors with the connection to the Minio bucket when creating your model server in the following steps, add a custom CA bundle to default-dsci:</p> <pre><code>oc get secret -n openshift-ingress-operator router-ca -o jsonpath='{.data.tls\\.crt}' | base64 -d &gt; openshift-ca-bundle.pem\n\noc get configmap -n openshift-config openshift-service-ca.crt -o jsonpath='{.data.service-ca\\.crt}' &gt;&gt; openshift-ca-bundle.pem\n\nCA_BUNDLE_FILE=./openshift-ca-bundle.pem\n\noc patch dscinitialization default-dsci --type='json' -p='[{\"op\":\"replace\",\"path\":\"/spec/trustedCABundle/customCABundle\",\"value\":\"'\"$(awk '{printf \"%s\\\\n\", $0}' $CA_BUNDLE_FILE)\"'\"}]'\n</code></pre> <p>NOTE: You can delete the openshift-ca-bundle.pem file after you have patched your dscinitialization, or you can add it to your trusted CA sources if it's necessary.</p> </li> </ol> </li> <li> <p>Run the following oc commands to enable the Single Model Serving runtime for OpenShift AI.      <pre><code>oc apply -k ./components/model-server/components-serving\n</code></pre></p> </li> <li> <p>It will take around 5 to 10 minutes for the changes to be applied. Single-model serving should be ready when Service Mesh and Serverless have the below instances created. Open the OpenShift web console and go to Operators -&gt; Installed Operators.</p> <p></p> <p></p> </li> </ol> </li> <li> <p>Go to the OpenShift AI dashboard and expand Settings and select Serving Runtimes. You should now see that Single-model serving enabled at the top of the page.</p> <p>NOTE: You might need to refresh the page and it could take a few minutes for the changes to be applied.</p> <p></p> </li> </ol> <p>NOTE: Make sure your single-model serving platform is using a trusted certificate. If it is not or you're unsure see section D in the Single-model serving platform automated install above.</p>"},{"location":"demos/podman-ai-lab-to-rhoai/podman-ai-lab-to-rhoai/#add-a-custom-serving-runtime","title":"Add a Custom Serving Runtime","text":"<p>We'll now add a custom serving runtime so we can deploy the GGUF version of model. </p> <p>NOTE: We will continue to use the GGUF version of the model to be able to deploy this model without the need for a hardware accelerator (e.g. GPU). OpenShift AI contains a scalable model serving platform to accommodate deploying multiple full sized LLMs.</p> <ol> <li> <p>Click on the Add serving runtime button. </p> <p></p> </li> <li> <p>Select Single-model serving platform for the runtime and select REST for the API protocol. Upload the ./components/custom-model-serving-runtime/llamacpp-runtime-custom.yaml file as the serving runtime. Click the Create button.</p> <p></p> <p>NOTE 1: I've included a pre-built image that is public. You can build your own image with the Containerfile under ./components/ucstom-model-serving-runtime if you would rather pull from your own repository.</p> <p>NOTE 2: If you intend to work with the Granite model using the GraniteForCausalLM architecture instead of the LlamaForCausalLM one, then use the quay.io/alexonoliveira/llamacpp_python:latest image, since this image is built with the latest version of the llama-cpp-python package, which supports the GraniteForCausalLM architecture.</p> </li> <li> <p>If the serving runtime was added was succesfully you should now see it in the list of serving runtimes available. </p> <p></p> </li> </ol>"},{"location":"demos/podman-ai-lab-to-rhoai/podman-ai-lab-to-rhoai/#deploy-model","title":"Deploy Model","text":"<ol> <li> <p>Go to your podman-ai-lab-rag-project and select Models. You should see two model serving type options. Click on the Deploy model under the Single-model serving platform. </p> <p></p> </li> <li> <p>Fill in the following values and click the Deploy button at the bottom of the form.</p> <ul> <li>Model name = mistral7b</li> <li>Serving runtime = LlamaCPP</li> <li>Model framework = any</li> <li>Model server size = Medium</li> <li>Select New data connection</li> <li>Name = models</li> <li>Access key = minio</li> <li>Secret key = minio123</li> <li>Endpoint = Your Minio API URL</li> <li>Region = us-east-1</li> <li>Bucket = models</li> <li>Path = mistral7b</li> </ul> <p></p> <p></p> <p></p> </li> <li> <p>If your model deploys successfully you should see the following page.</p> <p></p> </li> <li> <p>Test your model to make sure you can send in a request and get a response. You can use the client code that is provided by the model service in Podman AI Lab. </p> <p>Make sure to update the URL in the cURL command to the Inference endpoint on OpenShift AI.</p> <pre><code>curl --location 'https://YOUR-OPENSHIFT-AI-INFERENCE-ENDPOINT/v1/chat/completions' --header 'Content-Type: application/json' --data '{\n  \"messages\": [\n    {\n      \"content\": \"You are a helpful assistant.\",\n      \"role\": \"system\"\n    },\n    {\n      \"content\": \"How large is the capital of France?\",\n      \"role\": \"user\"\n    }\n  ]\n}'\n</code></pre> <p>Your response should be similar to the following</p> <pre><code>{\"id\":\"chatcmpl-c76974b1-4709-41a5-87cf-1951e10886fe\",\"object\":\"chat.completion\",\"created\":1717616440,\"model\":\"/mnt/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\"choices\":[{\"index\":0,\n\"message\":{\"content\":\" The size of a city's area, including its metropolitan area, can vary greatly, and when referring to the \n\\\"capital\\\" of a country like France, people usually mean the city itself rather than its total metropolitan area. Paris, the capital \ncity of France, covers an urban area of approximately 105 square \nkilometers (40.5 square miles) within its administrative limits.\n\\n\\nHowever, if you are asking about the total area of the Paris \nMetropolitana region, which includes suburban areas and their \ncombined population, it is much larger at around 13,022 square \nkilometers (5,028 square miles). This encompasses more than just the city of Paris.\",\n\"role\":\"assistant\"},\"logprobs\":null,\"finish_reason\":\"stop\"}],\"usage\":{\"prompt_tokens\":32,\"completion\n</code></pre> <p>If you face any SSL errors when running the previous command, try to add the max_tokens limit to 100, which will get you safely under the 60s timeout limit of the Knative queue-proxy service (PS. The use of jq is optional):</p> <pre><code>  curl -k --location 'https://YOUR-OPENSHIFT-AI-INFERENCE-ENDPOINT/v1/chat/completions' --header 'Content-Type: application/json' --data '{\n    \"messages\": [\n      {\n        \"content\": \"You are a helpful assistant.\",\n        \"role\": \"system\"\n      },\n      {\n        \"content\": \"How large is the capital of France?\",\n        \"role\": \"user\"\n      }\n    ],\n    \"max_tokens\": 100\n  }' | jq .\n    % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                             Dload  Upload   Total   Spent    Left  Speed\n  100  1011  100   793  100   218     14      3  0:01:12  0:00:55  0:00:17   174\n  {\n    \"id\": \"chatcmpl-687c22c8-d0ba-4ea4-a012-d4b64069d7a2\",\n    \"object\": \"chat.completion\",\n    \"created\": 1727727459,\n    \"model\": \"/mnt/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n    \"choices\": [\n      {\n        \"index\": 0,\n        \"message\": {\n          \"content\": \" The size of a city's area, including its urban and rural parts, is typically measured in square kilometers or square miles. However, when referring to the size of a city's capital, people usually mean the size of its urban core or central business district rather than the entire metropolitan area. In this context, Paris, the capital city of France, has an urban area of approximately 105 square kilometers (40.5 square miles). However, if you meant\",\n          \"role\": \"assistant\"\n        },\n        \"logprobs\": null,\n        \"finish_reason\": \"length\"\n      }\n    ],\n    \"usage\": {\n      \"prompt_tokens\": 32,\n      \"completion_tokens\": 100,\n      \"total_tokens\": 132\n    }\n  }\n</code></pre> </li> </ol>"},{"location":"demos/podman-ai-lab-to-rhoai/podman-ai-lab-to-rhoai/#update-the-chat-recipe-application","title":"Update the Chat Recipe Application","text":"<p>We'll now update the chat recipe application that we created from Podman AI Lab to use Langchain to connect the model we just deployed on OpenShift AI and the Elasticsearch vector database.</p> <ol> <li> <p>We'll start from the default chatbot recipe code accessible from Podman AI Lab. </p> <p></p> <p>In Podman AI Lab, after clicking the  button you should see the following.</p> <p></p> <p>The only code we'll need to modify is under the app directory.</p> </li> <li> <p>Open the ./components/app/chatbot_ui.py file. </p> <ul> <li>We'll first get some environment variables.</li> </ul> <p></p> <p>Then we'll add in the Langchain code to give us our RAG functionality. Note the items highlighted in red. Specifically where the model_service or your OpenShift AI inference endpoint URL and the Elasticsearch setup. Finally, take note of how both of these are passed to Langchain (chain).</p> <p></p> <ul> <li>The last updates to the code are just to format the response so that the relevant documents will be included. Extra packages were also added to the ./components/app/requirements.txt file.</li> </ul> </li> <li> <p>You can build the Containerfile and push it to your own repository or you can use the one at quay.io/jhurlocker/elastic-vectordb-chat.</p> </li> <li> <p>If the chatbot app has SSL failures or timeouts similar to those mentioned in item 4 of the previous subtitle, add the max_tokens parameter to the chatbot_ui.py code in the ChatOpenAI connection part. Or if you want a built image with this parameter, you can use quay.io/alexonoliveira/elastic-vectordb-chat:latest.</p> <p></p> </li> <li> <p>Update the ./components/app/deployment.yaml file with your values for the MODEL_ENDPOINT, ELASTIC_URL, and ELASTIC_PASS environment variables.</p> <p></p> <p>NOTE: Make sure you include 'https://' and the port ':9200' in the ELASTIC_URL environment variable</p> </li> <li> <p>Create the project</p> <pre><code>oc new-project elastic-vectordb-chat\n</code></pre> </li> <li> <p>Apply the deployment.yaml you just updated to deploy the chatbot application.</p> <pre><code>oc apply -f ./components/app/deployment.yaml\n</code></pre> </li> <li> <p>Get the route to the chatbot application</p> <pre><code>oc get route -n elastic-vectordb-chat\n</code></pre> <p>Open the application in your browser</p> <p></p> </li> <li> <p>Type in a message and press Enter. It might take awhile to respond if the model is deployed on a CPU.</p> <p></p> </li> <li> <p>In the OpenShift web console you can check the model server logs under the podman-ai-lab-rag-project -&gt; Workloads -&gt; Pods (mistral7b-*) -&gt; Logs. Note the log statements when a message is sent to the model inference endpoint.</p> <p></p> </li> <li> <p>Congratulations! You've successfully taken a model and application from Podman AI Lab and created a RAG chatbot deployed on OpenShift and OpenShift AI.</p> </li> </ol> <p>Special thanks to the maintainers of the below repositories.</p> <ul> <li> <p>LLM On OpenShift The notebook to ingest data into Elasticsearch and the Langchain code added to the chatbot app.</p> </li> <li> <p>AI Accelerator The code used to deploy the various components on OpenShift and OpenShift AI.</p> </li> </ul>"},{"location":"demos/retail-object-detection/retail-object-detection/","title":"Object Detection in Retail","text":"<p>Info</p> <p>The full source and instructions for this demo are available in this repo</p> <p>In this demo, you can see how to build an intelligent application that gives a customer the ability to find merchandise discounts, for shirts, as they browse clothing in a department store.</p> <p>You can download the related presentation.</p> <p></p> <p></p> <p></p>"},{"location":"demos/smart-city/smart-city/","title":"Smart City, an Edge-to-Core Data Story","text":"<p>Info</p> <p>The full source and instructions for this demo are available in this repo</p> <p>In this demo, we show how to implement this scenario:</p> <ul> <li>Using a trained ML model, licence plates are recognized at toll location.</li> <li>Data (plate number, location, timestamp) is send from toll locations (edge) to the core using Kafka mirroring to handle communication issues and recovery.</li> <li>Incoming data is screened real-time to trigger alerts for wanted vehicles (Amber Alert).</li> <li>Data is aggregated and stored into object storage.</li> <li>A central database contains other information coming from licence registry system: car model, color,\u2026\u200b</li> <li>Data analysis leveraging Presto and Superset is done against stored data.</li> </ul> <p>This demo is showcased in this video.</p> <p></p> <p></p>"},{"location":"demos/stable-diffusion/stable-diffusion/","title":"Text to Image using Stable Diffusion with DreamBooth","text":"<p>Stable Diffusion is a generative model that creates high-quality images by gradually denoising from random noise. DreamBooth fine-tuning customizes this model by training it on specific examples, allowing it to generate personalized images based on unique tokens and descriptive prompts.</p> <p></p> <p>Credit: DreamBooth</p> <p>See the original DreamBooth project homepage for more details on what this fine-tuning method achieves.</p>"},{"location":"demos/stable-diffusion/stable-diffusion/#requirements","title":"Requirements","text":"<ul> <li>An OpenShift cluster with RHOAI or ODH deployed.</li> <li>CSI driver capable of providing RWX volumes.</li> <li>A node with a GPU card. The example has been tested with a AWS p3.8xlarge node.</li> </ul> <p>Info</p> <p>The full source and instructions for this demo are available on this repo and is based on this sample code.</p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/","title":"Telecom Customer Churn using Airflow and Red Hat OpenShift AI","text":"<p>Info</p> <p>The full source and instructions for this demo are available in this repo</p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#demo-description","title":"Demo description","text":"<p>The goal of this demo is to demonstrate how Red Hat OpenShift AI (RHOAI) and Airflow can be used together to build an easy-to-manage pipeline. To do that, we will show how to build and deploy an airflow pipeline, mainly with Elyra but also some tips if you want to build it manually. In the end, you will have a pipeline that:</p> <ul> <li>Loads some data</li> <li>Trains two different models</li> <li>Evaluates which model is best</li> <li>Saves that model to S3</li> </ul> <p>Hint</p> <p>You can expand on this demo by loading the pushed model into MLFlow, or automatically deploying it into some application, like in the Credit Card Fraud Demo</p> <p>The models we build are used to predict customer churn for a Telecom company using structured data. The data contains fields such as: If they are a senior citizen, if they are a partner, their tenure, etc.</p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#deploying-the-demo","title":"Deploying the demo","text":""},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Fork this git repository into a GitHub or GitLab repo (the demo shows steps for GitHub, but either works): https://github.com/red-hat-data-services/telecom-customer-churn-airflow</li> <li>Have Airflow running in a cluster and point Airflow to the cloned git repository.</li> <li>Have access to some S3 storage (this guide uses ODF with a bucket created in the namespace \"airflow\").</li> <li>Have Red Hat OpenShift AI (RHOAI) running in a cluster. Make sure you have admin access in RHOAI, or know someone who does.</li> </ul> <p>Note</p> <p>Note: You can use Open Data Hub instead of RHOAI, but some instructions and screenshots may not apply</p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#1-open-up-airflow","title":"1: Open up Airflow","text":"<p>You find the route to the Airflow console through this command: <code>oc get route -n airflow</code></p> <p></p> <p>Enter it in the browser and you will see something like this:</p> <p></p> <p>Keep that open in a tab as we will come back to Airflow later on.</p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#2-add-elyra-as-a-custom-notebook-image","title":"2: Add Elyra as a Custom Notebook Image","text":"<p>It's possible to build pipelines by creating an Airflow DAG script in python. Another, arguably simpler, method is to use Elyra to visually build out the pipeline and then submit it to Airflow. Most of this demo is going to be revolving around using Elyra together with Airflow, but at the very end, there will be a bonus section for how to use Airflow independently.</p> <p>To get access to Elyra, we will simply import it as a custom notebook image. Start by opening up RHOAI by clicking on the 9-square symbol in the top menu and choosing \"Red Hat OpenShift AI\".</p> <p></p> <p>Then go to Settings -&gt; Notebook Images and press \"Import new image\". If you can't see Settings then you are lacking sufficient access. Ask your admin to add this image instead.</p> <p></p> <p>Under Repository enter: <code>quay.io/eformat/elyra-base:0.2.1</code> and then name it something like <code>Elyra</code>.</p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#3-create-a-rhoai-workbench","title":"3: Create a RHOAI workbench","text":"<p>A workbench in RHOAI lets us spin up and down notebooks as needed and bundle them under Projects, which is a great way to get easy access to compute resources and keep track of your work. Start by creating a new Data Science project (see image). I'm calling my project 'Telecom Customer Churn', feel free to call yours something different but be aware that some things further down in the demo may change.</p> <p></p> <p>After the project has been created, create a workbench where we can run Jupyter. There are a few important settings here that we need to set:</p> <ul> <li>Name: Customer Churn</li> <li>Notebook Image: Elyra</li> <li>Deployment Size: Small</li> <li>Environment Variables: Secret -&gt; AWS with your AWS details</li> </ul> <p></p> <p>Press Create Workbench and wait for it to start - status should say \"Running\" and you should be able to press the Open link.</p> <p></p> <p>Open the workbench and login if needed.</p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#4-load-a-git-repository","title":"4: Load a Git repository","text":"<p>When inside the workbench (Jupyter), we are going to clone a GitHub repository that contains everything we need to build our DAG. You can clone the GitHub repository by pressing the GitHub button in the left side menu (see image), then select \"Clone a Repository\" and enter your GitHub URL (Your forked version of this: https://github.com/red-hat-data-services/telecom-customer-churn-airflow)</p> <p></p> <p>The notebooks we will use are inside the <code>include/notebooks</code> folder, there should be 5 in total, 4 for building the pipeline and 1 for verifying that everything worked. They all run standard Python code, which is the beauty of Airflow combined with Elyra. There is no need to worry about additional syntax.</p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#5-configure-elyra-to-work-with-airflow","title":"5: Configure Elyra to work with Airflow","text":"<p>Before we can build and run any DAGs through Elyra, we first need to configure Elyra to talk with our Airflow instance. There will be two ways to configure this, either visually or through the terminal. Chose one for each section. If you want to do it through the terminal, then open the terminal like this: </p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#51-create-a-runtime-image","title":"5.1 Create a Runtime Image","text":"<p>We will start by configuring a Runtime Image, this is the image we will use to run each node in our pipeline. Open Runtime Images on the left-hand side of the screen.  </p> <p></p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#511-create-the-runtime-image-visually","title":"5.1.1 Create the Runtime Image visually","text":"<p>Press the plus icon next to the Runtime Images title to start creating a new Runtime Image. There are only three fields we need to worry about here:</p> <ul> <li>Display name: <code>airflow-runner</code></li> <li>Image Name: <code>quay.io/eformat/airflow-runner:2.5.1</code></li> <li>Image Pull Policy: Always</li> </ul> <p></p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#512-create-the-runtime-image-via-the-terminal","title":"5.1.2 Create the Runtime Image via the terminal","text":"<p>Execute this in the terminal:</p> <pre><code>mkdir -p ~/.local/share/jupyter/metadata/runtime-images/\ncat &lt;&lt; EOF &gt; ~/.local/share/jupyter/metadata/runtime-images/airflow-runner.json\n{\n  \"display_name\": \"airflow-runner\",\n  \"metadata\": {\n    \"tags\": [],\n    \"display_name\": \"airflow-runner\",\n    \"image_name\": \"quay.io/eformat/airflow-runner:2.5.1\",\n    \"pull_policy\": \"Always\"\n  },\n  \"schema_name\": \"runtime-image\"\n}\nEOF\n</code></pre> <p>Refresh and you should see <code>airflow-runner</code> appear in the Runtime Images.</p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#52-create-a-runtime","title":"5.2 Create a Runtime","text":"<p>Now we just need a Runtime configuration, which is what Elyra will use to save the DAG (in our Git repo), connect to Airflow and run the pipeline. Just like with the Runtime image, we can configure this visually or via the terminal.</p> <p>Open Runtimes on the left-hand side of the screen.  </p> <p></p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#521-configure-the-runtime-visually","title":"5.2.1 Configure the Runtime visually","text":"<p>Press the plus icon next to the title, select \"New Apache Airflow runtime configuration\" and enter these fields:</p> <p></p> <p>General settings:</p> <ul> <li>Display Name: <code>airflow</code></li> </ul> <p>Airflow settings:</p> <ul> <li>Apache Airflow UI Endpoint: run <code>oc get route -n airflow</code> to get the route</li> <li>Apache Airflow User Namespace: <code>airflow</code></li> </ul> <p>Github/GitLabs settings:</p> <ul> <li>Git type: GITHUB or GITLAB, depending on where you stored the repository</li> <li>GitHub or GitLab server API Endpoint: <code>https://api.github.com</code> or your GitLab endpoint</li> <li>GitHub or GitLab DAG Repository: Your repository (<code>red-hat-data-services/telecom-customer-churn-airflow</code> in my case)</li> <li>GitHub or GitLab DAG Repository Branch: Your branch (<code>main</code> in my case)</li> <li>Personal Access Token: A personal access token for pushing to the repository</li> </ul> <p>Cloud Object Storage settings: These completely depend on where and how you set up your S3 storage. If you created a bucket from ODF then it will look similar to this:</p> <ul> <li>Cloud Object Storage Endpoint: <code>http://s3.openshift-storage.svc</code></li> <li>Cloud Object Storage Bucket Name: The name of your bucket (<code>airflow-storage-729b10d1-f44d-451d-badb-fbd140418763</code> in my case)</li> <li>Cloud Object Storage Authentication Type: KUBERNETES_SECRET</li> <li>Cloud Object Storage Credentials Secret: The name of your secret containing the access and secret key is (in my case it was <code>airflow-storage</code>, which is the name I gave the Object Bucket Claim)</li> <li>Cloud Object Storage Username: your AWS_ACCESS_KEY_ID</li> <li>Cloud Object Storage Password: your AWS_SECRET_ACCESS_KEY</li> </ul>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#522-configure-the-runtime-via-the-terminal","title":"5.2.2 Configure the Runtime via the terminal","text":"<p>If you prefer doing this through the terminal, then execute this in the terminal and replace any variables with their values (see the visual section for hints):</p> <pre><code>mkdir -p ~/.local/share/jupyter/metadata/runtimes\ncat &lt;&lt; EOF &gt;  ~/.local/share/jupyter/metadata/runtimes/airflow.json\n{\n  \"display_name\": \"airflow\",\n  \"metadata\": {\n     \"tags\": [],\n     \"display_name\": \"airflow\",\n     \"user_namespace\": \"airflow\",\n     \"git_type\": \"GITHUB\",\n     \"github_api_endpoint\": \"https://${GIT_SERVER}\",\n     \"api_endpoint\": \"${AIRFLOW_ROUTE}\",\n     \"github_repo\": \"${GIT_REPO}\",\n     \"github_branch\": \"main\",\n     \"github_repo_token\": \"${GIT_TOKEN}\",\n     \"cos_auth_type\": \"KUBERNETES_SECRET\",\n     \"cos_endpoint\": \"${STORAGE_ENDPOINT}\",\n     \"cos_bucket\": \"${STORAGE_BUCKET}\",\n     \"cos_secret\": \"airflow-storage\" - the name of your secret,\n     \"cos_username\": \"${AWS_ACCESS_KEY_ID}\",\n     \"cos_password\": \"${AWS_SECRET_ACCESS_KEY}\",\n     \"runtime_type\": \"APACHE_AIRFLOW\"\n  },\n  \"schema_name\": \"airflow\"\n}\nEOF\n</code></pre> <p>Refresh and you should see <code>airflow</code> appear in the Runtimes.</p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#6-create-a-dag-with-elyra","title":"6. Create a DAG with Elyra","text":"<p>Now that we have a runtime and runtime image defined, we can build and run the pipeline. You can also find this pipeline in <code>/dags/train_and_compare_models.pipeline</code> if you prefer to just open an existing one.  </p> <p>To start creating a new pipeline, open up the launcher (click on the plus next to a notebook tab if you don't have it open), and press the \"Apache Airflow Pipeline Editor\". </p> <p>Now drag the Notebooks in the correct order and connect them up with each other. You can find the Notebooks in <code>/included/notebooks</code> and the correct order is: process_data -&gt; model_gradient_boost &amp; model_randomforest -&gt; compare_and_push. These are their functions:</p> <ul> <li>process_data.ipynb: Downloads data from GitHub that we will use to train the models. Then processes it, splits it into training and testing partitions and finally pushes it to S3.</li> <li>model_gradient_boost.ipynb: Fetches the processed data from S3 and uses it to train the model and evaluate it to get a test accuracy. Then pushes the model and the accompanying accuracy to S3.</li> <li>model_randomforest.ipynb: Fetches the processed data from S3 and uses it to train the model and evaluate it to get a test accuracy. Then pushes the model and the accompanying accuracy to S3.</li> <li>compare_and_push.ipynb: Downloads the models and their accuracies from S3, does a simple compare on which performs better, and pushes that model under the name \"best_model\" to S3.</li> </ul> <p></p> <p>After the notebooks are added, we need to go through each of them and change their Runtime Images to <code>airflow-runner</code> that we created earlier. </p> <p>We also need to set some environment variables so that the airflow nodes get access to the bucket name and endpoint when running, without hard-coding it in the notebooks. These details are already added to the Airflow Runtime we set up before, but when running it only passes along the Kubernetes secret which contains AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.  </p> <p>Add these two environment variables (both should be the same as you entered in section 5.2):</p> <ul> <li>Endpoint:<ul> <li>Name: AWS_S3_ENDPOINT</li> <li>Value: <code>http://s3.openshift-storage.svc</code> (or similar endpoint address)</li> </ul> </li> <li>Bucket name:<ul> <li>Name: AWS_S3_BUCKET</li> <li>Value: The name of your bucket (<code>airflow-storage-729b10d1-f44d-451d-badb-fbd140418763</code> in my case)</li> </ul> </li> </ul> <p></p> <p>Press Run to start the pipeline: </p> <p>You can now go to the Airflow UI to see the progress. If you have closed the tab then refer to section 1.</p> <p>In Airflow you will see a dag called <code>train_and_compare_models</code> with some numbers behind it. Click on it and go open the Graph tab. </p> <p></p> <p>If all are dark green that means that the run has completed successfully.</p> <p>We can now also confirm that the trained model was saved in our bucket by going back to the RHOAI notebook and running the notebook <code>test_airflow_success.ipynb</code>. If all went well it should print the model, its type and its accuracy. </p> <p>And that's how you can use Airflow together with RHOAI to create a pipeline!</p>"},{"location":"demos/telecom-customer-churn-airflow/telecom-customer-churn-airflow/#bonus-section-use-an-airflow-dag-file","title":"Bonus section: Use an Airflow DAG file","text":"<p>Instead of building a pipeline through notebooks in Elyra, we can of course build and use an Airflow DAG. You can develop individual methods (data processing, mode training, etc) in RHOAI notebooks and then pull them all together in a DAG python file. This is a more segmented way for a Data Scientist to work than with Elyra, but still very possible within OpenShift and provides some more flexibility.  </p> <p>I have created a simple <code>test_dag.py</code> just to show what it can look like. You can find it in the <code>/dags</code> folder. Then it's up to you what operators you want to run, which secrets you want to load, etc. For inspiration, you can open up the automatically created Elyra DAG we just ran. To do that, go into the DAG and press Code:</p> <p></p> <p>Some notes if you wish to manually build a similar DAG:</p> <ul> <li>Make sure to add the environment variables</li> <li>Don't hardcode secrets into the DAG, but rather reference a Kubernetes secret. For example:</li> </ul> <pre><code>secrets=[\n        Secret(\"env\", \"AWS_ACCESS_KEY_ID\", \"airflow-storage\", \"AWS_ACCESS_KEY_ID\"),\n        Secret(\n            \"env\", \"AWS_SECRET_ACCESS_KEY\", \"airflow-storage\", \"AWS_SECRET_ACCESS_KEY\"\n        ),\n    ]\n</code></pre> <ul> <li>The image that is being used for the KubernetesPodOperator is <code>quay.io/eformat/airflow-runner:2.5.1</code></li> <li>If you want to run notebooks manually, look at the Papermill Operator</li> </ul>"},{"location":"demos/water-pump-failure-prediction/water-pump-failure-prediction/","title":"Water Pump Failure Prediction","text":"<p>Info</p> <p>The full source for this demo is available in this repo. Look in the <code>workshop</code> folder for the full instructions.</p> <p>This demo shows how to do detection of anomalies in sensor data. This web app allows you to broadcast various sources of data in real time.</p> <p></p>"},{"location":"demos/xray-pipeline/xray-pipeline/","title":"XRay Analysis Automated Pipeline","text":"<p>Info</p> <p>The full source and instructions for this demo are available in this repo</p> <p>In this demo, we implement an automated data pipeline for chest Xray analysis:</p> <ul> <li>Ingest chest Xrays into an object store based on Ceph.</li> <li>The Object store sends notifications to a Kafka topic.</li> <li>A KNative Eventing Listener to the topic triggers a KNative Serving function.</li> <li>An ML-trained model running in a container makes a risk of Pneumonia assessment for incoming images.</li> <li>A Grafana dashboard displays the pipeline in real time, along with images incoming, processed and anonymized, as well as full metrics.</li> </ul> <p>This pipeline is showcased in this video (slides are also here).</p> <p></p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/","title":"Model Training and Serving - YOLOv5","text":"<p>Info</p> <p>The full source and instructions for this demo are available in these repos:</p> <ul> <li>Model Training</li> <li>Model Serving</li> </ul> <p>In this tutorial, we're going to see how you can customize YOLOv5, an object detection model, to recognize specific objects in pictures, and how to deploy and use this model.</p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#yolo-and-yolov5","title":"YOLO and YOLOv5","text":"<p>YOLO (You Only Look Once) is a popular object detection and image segmentation model developed by Joseph Redmon and Ali Farhadi at the University of Washington. The first version of YOLO was released in 2015 and quickly gained popularity due to its high speed and accuracy.</p> <p>YOLOv2 was released in 2016 and improved upon the original model by incorporating batch normalization, anchor boxes, and dimension clusters. YOLOv3 was released in 2018 and further improved the model's performance by using a more efficient backbone network, adding a feature pyramid, and making use of focal loss.</p> <p>In 2020, YOLOv4 was released which introduced a number of innovations such as the use of Mosaic data augmentation, a new anchor-free detection head, and a new loss function.</p> <p>In 2021, Ultralytics released YOLOv5, which further improved the model's performance and added new features such as support for panoptic segmentation and object tracking.</p> <p>YOLO has been widely used in a variety of applications, including autonomous vehicles, security and surveillance, and medical imaging. It has also been used to win several competitions, such as the COCO Object Detection Challenge and the DOTA Object Detection Challenge.</p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#model-training","title":"Model training","text":"<p>YOLOv5 has already been trained to recognize some objects. Here we are going to use a technique called Transfer Learning to adjust YOLOv5 to recognize a custom set of images.</p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#transfer-learning","title":"Transfer Learning","text":"<p>Transfer learning is a machine learning technique in which a model trained on one task is repurposed or adapted to another related task. Instead of training a new model from scratch, transfer learning allows the use of a pre-trained model as a starting point, which can significantly reduce the amount of data and computing resources needed for training.</p> <p>The idea behind transfer learning is that the knowledge gained by a model while solving one task can be applied to a new task, provided that the two tasks are similar in some way. By leveraging pre-trained models, transfer learning has become a powerful tool for solving a wide range of problems in various domains, including natural language processing, computer vision, and speech recognition.</p> <p>Ultralytics have fully integrated the transfer learning process in YOLOv5, making it easy for us to do. Let's go!</p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#environment-and-prerequisites","title":"Environment and prerequisites","text":"<ul> <li>This training should be done in a Data Science Project to be able to modify the Workbench configuration (see below the /dev/shm issue).</li> <li>YOLOv5 is using PyTorch, so in RHOAI it's better to start with a notebook image already including this library, rather than having to install it afterwards.</li> <li>PyTorch is internally using shared memory (/dev/shm) to exchange data between its internal worker processes. However, default container engine configurations limit this memory to the bare minimum, which can make the process exhaust this memory and crash. The solution is to manually increase this memory by mounting a specific volume with enough space at this emplacement. This problem will be fixed in an upcoming version. Meanwhile you can use this procedure.</li> <li>Finally, a GPU is strongly recommended for this type of training.</li> </ul>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#data-preparation","title":"Data Preparation","text":"<p>To train the model we will of course need some data. In this case a sufficient number of images for the various classes we want to recognize, along with their labels and the definitions of the bounding boxes for the object we want to detect.</p> <p>In this example we will use images from Google's Open Images. We will work with 3 classes: Bicycle, Car and Traffic sign.</p> <p>We have selected only a few classes in this example to speed up the process, but of course feel free to adapt and choose the ones you want.</p> <p>For this first step:</p> <ul> <li>If not already done, create your Data Science Project,</li> <li>Create a Workbench of type PyTorch, with at least 8Gi of memory, 1 GPU and 20GB of storage.</li> <li>Apply this procedure to increase shared memory.</li> <li>Start the workbench.</li> <li>Clone the repository https://github.com/rh-aiservices-bu/yolov5-transfer-learning, open the notebook 01-data_preparation.ipynb and follow the instructions.</li> </ul> <p>Once you have completed to whole notebook the Dataset is ready for training!</p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#training","title":"Training","text":"<p>In this example, we will do the training with the smallest base model available to save some time. Of course you can change this base model and adapt the various hyperparameters of the training to improve the result.</p> <p>For this second step, from the same workbench environment, open the notebook <code>02-model_training.ipynb</code> and follow the instructions.</p> <p>Warning</p> <p>The amount of memory you have assigned to your Workbench has a great impact on the batch size you will be able to work with, independently of the size of your GPU. For example, a batch size of 128 will barely fit into an 8Gi of memory Pod. The higher the better, until it breaks... Which you will find out soon anyway, after the first 1-2 epochs.</p> <p>Note</p> <p>During the training, you can launch and access Tensorboard by:</p> <ul> <li>Opening a Terminal tab in Jupyter</li> <li>Launch Tensorboard from this terminal with <code>tensorboard --logdir yolov5/runs/train</code></li> <li>Access Tensorboard in your browser using the same Route as your notebook, but replacing the <code>.../lab/...</code> part by <code>.../proxy/6006/</code>. Example: <code>https://yolov5-yolo.apps.cluster-address/notebook/yolo/yolov5/proxy/6006/</code></li> </ul> <p>Once you have completed to whole notebook you have a model that is able to recognize the three different classes on a given image.</p> <p></p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#model-serving","title":"Model Serving","text":"<p>We are going to serve a YOLOv5 model using the ONNX format, a general purpose open format built to represent machine learning models. RHOAI Model Serving includes the OpenVino serving runtime that accepts two formats for models: OpenVino IR, its own format, and ONNX.</p> <p>Note</p> <p>Many files and code we are going to use, especially the ones from the utils and models folders, come directly from the YOLOv5 repository. They includes many utilities and functions needed for image pre-processing and post-processing. We kept only what is needed, rearranged in a way easier to follow within notebooks. YOLOv5 includes many different tools and CLI commands that are worth learning, so don't hesitate to have a look at it directly.</p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#environment-and-prerequisites_1","title":"Environment and prerequisites","text":"<ul> <li>YOLOv5 is using PyTorch, so in RHOAI it's better to start with a notebook image already including this library, rather than having to install it afterwards.</li> <li>Although not necessary as in this example we won't use the model we trained in the previous section, the same environment can totally be reused.</li> </ul>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#converting-a-yolov5-model-to-onnx","title":"Converting a YOLOv5 model to ONNX","text":"<p>YOLOv5 is based on PyTorch. So base YOLOv5 models, or the ones you retrain using this framework, will come in the form of a <code>model.pt</code> file. We will first need to convert it to the ONNX format.</p> <ul> <li>From your workbench, clone the repository https://github.com/rh-aiservices-bu/yolov5-model-serving.</li> <li>Open the notebook <code>01-yolov5_to_onnx.ipynb</code> and follow the instructions.</li> <li>The notebook will guide you through all the steps for the conversion. If you don't want to do it at this time, you can also find in this repo the original YOLOv5 \"nano\" model, <code>yolov5n.pt</code>, and its already converted ONNX version, <code>yolov5n.onnx</code>.</li> </ul> <p>Once converted, you can save/upload your ONNX model to the storage you will use in your Data Connection on RHOAI. At the moment it has to be an S3-Compatible Object Storage, and the model must be in it own folder (not at the root of the bucket).</p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#serving-the-model","title":"Serving the model","text":"<p>Here we can use the standard configuration path for RHOAI Model Serving:</p> <ul> <li>Create a Data Connection to the storage where you saved your model. In this example we don't need to expose an external Route, but of course you can. In this case though, you won't be able to directly see the internal gRPC and REST endpoints in the RHOAI UI, you will have to get them from the Network-&gt;Services panel in the OpenShift Console.</li> <li>Create a Model Server, then deploy the model using the ONNX format.</li> </ul> <p>Note</p> <p>You can find full detailed versions of this procedure in this Learning Path or in the RHOAI documentation.</p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#grpc-connection","title":"gRPC connection","text":"<p>With the gRPC interface of the model server, you have access to different Services. They are described, along with their format, in the <code>grpc_predict_v2.proto</code> file.</p> <p>There are lots of important information in this file: how to query the service, how to format the data,... This is really important as the data format is not something you can \"invent\", and not exactly the same compared as the REST interface (!).</p> <p>This proto file, which is a service description meant to be used with any programming language, has already been converted to usable Python modules defining objects and classes to be used to interact with the service: <code>grpc_predict_v2_pb2.py</code> and <code>grpc_predict_v2_pb2_grpc.py</code>. If you want to learn more about this, the conversion can be done using the protoc tool.</p> <p>You can use the notebook <code>02-grpc.ipynb</code> to connect to the interface and test some of the services. You will see that many \"possible\" services from ModelMesh are unfortunately simply not implemented with the OpenVino backend at the time of this writing. But at least ModelMetadata will give some information on the formats we have to use for inputs and outputs when doing the inference.</p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#consuming-the-model-over-grpc","title":"Consuming the model over gRPC","text":"<p>In the <code>03-remote_inference_grpc.ipynb</code> notebook, you will find a full example on how to query the grpc endpoint to make an inference. It is backed by the file <code>remote_infer_grpc.py</code>, where most of the relevant code is:</p> <ul> <li>Image preprocessing on L35: reads the image and transforms it in a proper numpy array</li> <li>gRPC request content building on L44: transforms the array in the expected input shape (refer to model metadata obtained in the previous notebook), then flatten it as expected by ModelMesh.</li> <li>gRPC calling on L58.</li> <li>Response processing on L73: reshape the response from flat array to expected output shape (refer to model metadata obtained in the previous notebook), run NMS to remove overlapping boxes, draw the boxes from results.</li> </ul> <p>The notebook gives the example for one image, as well as the processing of several ones from the <code>images</code> folder. This allows for a small benchmark on processing/inference time.</p> <p></p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#consuming-the-model-over-rest","title":"Consuming the model over REST","text":"<p>In the <code>04-remote_inference_rest.ipynb</code> notebook, you will find a full example on how to query the gRPC endpoint to make an inference. It is backed by the file <code>remote_infer_rest.py</code>, where most of the relevant code is:</p> <ul> <li>Image preprocessing on L30: reads the image and transforms it in a proper numpy array</li> <li>Payload building on L39: transforms the array in the expected input shape (refer to model metadata obtained in the previous notebook).</li> <li>REST calling on L54.</li> <li>Response processing on L60: reshape the response from flat array to expected output shape (refer to model metadata obtained in the previous notebook), run NMS to remove overlapping boxes, draw the boxes from results.</li> </ul> <p>The notebook gives the example for one image, as well as the processing of several ones from the <code>images</code> folder. This allows for a small benchmark on processing/inference time.</p>"},{"location":"demos/yolov5-training-serving/yolov5-training-serving/#grpc-vs-rest","title":"gRPC vs REST","text":"<p>Here are a few elements to help you choose between the two available interfaces to query your model:</p> <ul> <li>REST is easier to implement: it is a much better known protocol for most people, and involves a little bit less programming. There is no need to create a connection, instantiate objects,... So it's often easier to use.</li> <li>If you want to query the model directly from outside OpenShift, you have to use REST which is the only one exposed. You can expose gRPC too, but it's kind of difficult right now.</li> <li>gRPC is wwwwwaaaayyyyy much faster than REST. With the exact same model serving instance, as showed in the notebooks, inferences are about 30x faster. That is huge when you have score of images to process.</li> </ul>"},{"location":"generative-ai/ai-for-everyone/","title":"Making AI Accessible to Everyone: What We Learned","text":"<p>If you are a Data Scientist, training or fine-tuning models is now easier than it was not so long ago. There are tools, libraries, frameworks, tons of documentation and examples available to help. Of course you still have some long and hard work to do, but you are not alone. Not totally...</p> <p>It's also true for ML Engineers. It's not as hard as it used to be to serve models, with the benefits of modern platforms: scalability, security,... And if you're the wisest, you've automated everything to make it easier.</p> <p>If you are a Developer, well, we're almost there. Most of the times, those models are now available as API endpoints. You may not know exactly what to put for the parameters, but you're learning (fast!). And at least it's your world, you don't have to learn any Python to be able to develop your application! Some patterns, like Models-as-a-Service, make it even easier to discover, register to, and consume the models you need.</p> <p>But what if you are a \"Normal User\"? Who obviously does not know how to train or serve a model, but neither how to use an API Gateway Portal, what an authentication token is, or a context window,... Isn't it frustrating to be able to use AI daily in your personal/consumer world, but not inside your organization?</p> <p>In this Proof of Concept, we will show you how the Parasol company their employees with easy access to Private AI!</p>"},{"location":"generative-ai/ai-for-everyone/#some-background","title":"Some background","text":"<p>Parasol is a fictitious company that has been modernizing its applications and work environment for some time now, trying to take advantage from AI wherever it can.</p> <p>After having implemented OpenShift AI for its data scientists and ML engineers, served and used models in its own applications, Parasol went a step further and implemented a \"models-as-service infrastructure\". The goal was to enable people with \"self-service AI\". Basically, they put the 3Scale API Gateway in front of OpenShift AI, following this architecture:</p> <p></p> <p>This instantly enabled their developers, who are now able to use the self-service portal to get API keys and consume the models. They would also have access to statistics and other API-related tools, while admin keep a constant overview and control on who is consuming what.</p> <p>However, Parasol felt it was not enough. So they went one step further.</p>"},{"location":"generative-ai/ai-for-everyone/#parasol-ai-studio","title":"Parasol AI Studio","text":"<p>At its heart, Parasol AI Studio is a spin on the OpenShift AI dashboard, heavily simplifying it. The goal is to make it as easy as possible for anyone to quickly instantiate and use AI-enabled applications without having to know anything about AI, models, APIs,...</p> <p>Currently available applications (which behind the scene are in fact OpenShift AI custom Workbenches) are:</p> <ul> <li>AnythingLLM</li> <li>Docling Serve UI</li> <li>Code Server with Continue.dev plugin</li> <li>Stable Diffusion XL Studio</li> </ul> <p>All these applications are automatically consuming AI models through the APIs of a fully integrated Models-as-a-Service environment. Accounts are automatically provisioned and tokens generated, without needing the user to know it even happens.</p>"},{"location":"generative-ai/ai-for-everyone/#architectureflow","title":"Architecture/Flow","text":"<p>The flow for creating a new application is pretty simple:</p> <ol> <li>The user selects which application they want to use, giving it a name.</li> <li>Parasol AI Studio backends executes different queries to the Models-as-a-Service (MaaS) API, using the username, the selected model (determined by the application they chose), and the name of the application entered by the user:</li> <li>If the user does not exists in MaaS, it is created.</li> <li>If the application name already exists for this user, the existing token is retrieved.</li> <li>If the application does not exist in MaaS, it is created and the corresponding token is sent back.</li> <li>Now that the Parasol AI Studio backend has all the needed information (model, endpoint, token,...), it creates a new Workbench of the requested type (AnythingLLM, Docling,...), injecting this information as environment variables. Those variables are used to properly configure the Workbench instance at startup (exact process depends on the type).</li> <li>The Application (Workbench) can now consume the model from MaaS, using its token to authenticate itself.</li> </ol> <p>The full source code of the Parasol AI Studio is available here.</p>"},{"location":"generative-ai/ai-for-everyone/#the-studio-in-action","title":"The Studio in Action","text":""},{"location":"generative-ai/ai-for-everyone/#interactive-demo","title":"Interactive Demo","text":"<p>Follow this link to experience a fully interactive demo of the Parasol AI Studio.</p>"},{"location":"generative-ai/ai-for-everyone/#screenshots","title":"Screenshots","text":"<ul> <li>Applications launcher, easy-to-use UI</li> </ul> <ul> <li>Application creation, with minimal information needed</li> </ul>"},{"location":"generative-ai/building-an-image-generation-app/","title":"Building an Image Generation App: What We Learned","text":"<p>At first, building an image generation app might seem as simple as hosting a model with a nice UI. We thought so too\u2014until we realized there were a few more things we needed to consider. Here\u2019s what we learned while creating our internal image generation app.</p>"},{"location":"generative-ai/building-an-image-generation-app/#the-key-components","title":"The key components:","text":"<ul> <li>An image generation model \u2013 We use Stable Diffusion XL (SDXL).</li> <li>Text input guardrails \u2013 We filter prompts using Granite Guardian.</li> <li>Image output guardrails \u2013 We check generated images with Stable Diffusion Safety Checker.</li> <li>A user-friendly interface \u2013 Built with React (code available here).</li> </ul> <p>In the end, our system looks like this: </p> <p>Let\u2019s go through the different parts! \ud83e\udd13 The full code can be found here.</p>"},{"location":"generative-ai/building-an-image-generation-app/#the-image-generation-model","title":"The image generation model \ud83c\udfa8","text":"<p>If you are unfamiliar with how an image generation model works, here is a quick overview. An image generation model takes text as input (often called prompts) and generates images as outputs. For example, if you type \u201ccute dog\u201d you will soon have a very cute dog on your screen, and depending on the model, it may have an extra nose or foot to keep it interesting.</p> <p>This is great for anyone who quickly wants to illustrate something, just make sure to consider licensing implications before using generated images commercially.</p> <p>There are different types of image generation models. Stable Diffusion, the model we use, gradually converts noise into an image over multiple steps. Since it\u2019s a process with multiple steps, we can show how the image gradually starts appearing, giving a sense of progression to the user while they wait.  </p> <p></p> <p>The specific model we use is SDXL (Stable Diffusion XL) 1.0. We chose it because it has a fairly permissive license and open weights. SDXL works in two phases:  </p> <ol> <li>The base model handles the initial denoising.</li> <li>The refiner model improves image quality in the final steps.  </li> </ol> <p>This way we can get even higher quality by having two models specializing in the different areas. By default, we allocate 80% of the steps to the base model and 20% to the refiner model, but users can adjust this with a slider to see how it impacts the final image.  </p> <p>If you want more details, you can check the deployment instructions and code here.</p>"},{"location":"generative-ai/building-an-image-generation-app/#guardrails","title":"Guardrails \ud83d\udc6e","text":"<p>One of the first things we had to consider was that the SDXL model was trained on fairly diverse content. This means that the model also can generate a diverse range of images, which is great in many cases, but as an enterprise there are a few categories of images that we don\u2019t want our users to be able to generate.</p> <p>To guard ourselves, and our users, against unsafe images we added two types of guardrails:  </p> <ul> <li>Text guardrails \u2013 These check input prompts before generation.</li> <li>Image guardrails \u2013 These review the generated images before displaying them.</li> </ul> <p>By looking at both the image and text, we reduce the chances of unsafe images slipping through.</p>"},{"location":"generative-ai/building-an-image-generation-app/#text-guardrails","title":"Text Guardrails \u270d\ufe0f","text":"<p>The text guardrail is a Large Language Model (LLM) that can generate text outputs from text inputs.</p> <p></p> <p>There exist specific Guardrail LLMs that have been trained on analyzing text and generating simple outputs that describe potential risks. The Granite Guardian is one of these models, which have been specifically designed to detect risks from the IBM AI Risk Atlas.</p> <p>For our case, we also wanted to customize the categories that the guardian model detects, to add our own. This is possible with the Granite Guardian by including custom categories in the input prompt sent to Granite Guardian along with the user\u2019s text.</p> <p>You can find more details in this excellent article.</p>"},{"location":"generative-ai/building-an-image-generation-app/#image-guardrails","title":"Image Guardrails \ud83d\uddbc\ufe0f","text":"<p>For filtering generated images, we use a model called Stable Diffusion Safety Checker.  How it works is that it will take an image as an input and create embeddings. These embeddings are just long arrays/lists of numbers, but they are really cool because the model is trained so that embeddings that are close to each other come from similar looking images.</p> <p></p> <p>The model then uses this to see if our image is too close to any unsafe image so we can block it. In the drawing above we can pretend that tractors are unsafe, so any image too close to the tractor dot will be blocked.</p> <p>We can also customize it and add another category by simply adding another image that we are not allowed to be too close to. Note that this works much better if the model has been trained on images from the new category so it easier can identify them, but it still works well enough without the training to experiment with it.</p> <p>However, we noticed an issue: the model wasn\u2019t great at detecting unsafe content while images were still blurry during generation. A simple fix would have been to check only the final image, but by that point, the image is already visible. Instead, we decided to only use the image guardrails after 50% of the steps, where the image is blurry enough that details aren\u2019t clear, but visible enough for the guardrails to not be confused.</p> <p>You can find more details about how this works and was customized here.</p>"},{"location":"generative-ai/building-an-image-generation-app/#ui","title":"UI \ud83e\ude84","text":"<p>To be able to interface with the model, we built a user-friendly UI.</p> <p></p> <p>There are a few settings in this UI:</p> <ul> <li>Prompt - Users enter a text description of what image they want.</li> <li>Size - Dropdown to choose the image size.</li> <li>Guidance Scale - Adjusts how creative vs. strict the model follows the prompts.</li> <li>Number of Inference Steps - Balances speed and image quality.</li> <li>Denoising Limit - Controls when the refiner model takes over from the base model.</li> </ul> <p>Along with calling the text and image guardrails, the UI automatically watermarks final images, making it clear they were generated by our system. </p>"},{"location":"generative-ai/building-an-image-generation-app/#summary","title":"Summary","text":"<p>We can now look back at the full architecture and see how all the pieces fit together</p> <p></p> <p>Here\u2019s a step-by-step look at how the system works:</p> <ol> <li>User enters a prompt in the UI.</li> <li>Granite Guardian checks the prompt for safety.</li> <li>If approved, the SDXL model starts generating an image.</li> <li>Each step of the denoising process is shown to the user.</li> <li>After 50% completion, the image guardrail checks for unsafe content.</li> <li>If the image passes, the user sees it; otherwise, it gets blocked.</li> <li>The final image gets a watermark.</li> </ol>"},{"location":"generative-ai/llm-serving/","title":"LLM Serving","text":"<p>Info</p> <p>All source files and examples used in this article are available on this repo!</p> <p>LLMs (Large Language Models) are the subject of the day. And of course, you can definitely work with them on OpenShift with ODH or RHOAI, from creating a Chatbot, using them as simple APIs to summarize or translate texts, to deploying a full application that will allow you to quickly query your documentation or knowledge base in natural language.</p> <p>You will find on this page instructions and examples on how to set up the different elements that are needed for those different use cases, as well as fully implemented and ready-to-use applications.</p>"},{"location":"generative-ai/llm-serving/#context-and-definitions","title":"Context and definitions","text":"<p>Many people are only beginning to discover those technologies. After all, it has been a little bit more than a year since the general public is aware of them, and many related technologies, tools or applications are only a few months, even weeks (and sometimes days!) old. So let's start with a few definitions of the different terms that will be used in this article.</p> <ul> <li>LLM: A Large Language Model (LLM) is a sophisticated artificial intelligence system designed for natural language processing. It leverages deep learning techniques to understand and generate human-like text. LLMs use vast datasets to learn language patterns, enabling tasks like text generation, translation, summarization, and more. These models are versatile and can be fine-tuned for specific applications, like chatbots or content creation. LLMs have wide-ranging potential in various industries, from customer support and content generation to research and education, but their use also raises concerns about ethics, bias, and data privacy, necessitating responsible deployment and ongoing research.</li> <li>Fine-tuning: Fine-tuning in the context of Large Language Models (LLMs) is a process of adapting a pre-trained, general-purpose model to perform specific tasks or cater to particular applications. It involves training the model on a narrower dataset related to the desired task, allowing it to specialize and improve performance. Fine-tuning customizes the LLM's capabilities for tasks like sentiment analysis, question answering, or chatbots. This process involves adjusting hyperparameters, data preprocessing, and possibly modifying the model architecture. Fine-tuning enables LLMs to be more effective and efficient in specific domains, extending their utility across various applications while preserving their initial language understanding capabilities.</li> <li>RAG: RAG, or Retrieval-Augmented Generation, is a framework in natural language processing. It combines two key components: retrieval and generation. Retrieval involves selecting relevant information from a vast knowledge base, like the internet, and generation pertains to creating human-like text. RAG models employ a retriever to fetch context and facts related to a specific query or topic and a generator, often a language model, to produce coherent responses. This approach enhances the quality and relevance of generated text, making it useful for tasks like question answering, content summarization, and information synthesis, offering a versatile solution for leveraging external knowledge in AI-powered language understanding and production.</li> <li>Embeddings: Embeddings refer to a technique in natural language processing and machine learning where words, phrases, or entities are represented as multi-dimensional vectors in a continuous vector space. These vectors capture semantic relationships and similarities between words based on their context and usage. Embeddings are created through unsupervised learning, often using models like Word2Vec or GloVe, which transform words into fixed-length numerical representations. These representations enable machines to better understand and process language, as similar words have closer vector representations, allowing algorithms to learn contextual associations. Embeddings are foundational in tasks like text classification, sentiment analysis, machine translation, and recommendation systems.</li> <li>Vector Database: A vector database is a type of database designed to efficiently store and manage vector data, which represents information as multidimensional arrays or vectors. Unlike traditional relational databases, which organize data in structured tables, vector databases excel at handling unstructured or semi-structured data. They are well-suited for applications in data science, machine learning, and spatial data analysis, as they enable efficient storage, retrieval, and manipulation of high-dimensional data points. Vector databases play a crucial role in various fields, such as recommendation systems, image processing, natural language processing, and geospatial analysis, by facilitating complex mathematical operations on vector data for insights and decision-making.</li> <li>Quantization: Model quantization is a technique in machine learning and deep learning aimed at reducing the computational and memory requirements of neural networks. It involves converting high-precision model parameters (usually 32-bit floating-point values) into lower precision formats (typically 8-bit integers or even binary values). This process helps in compressing the model, making it more lightweight and faster to execute on hardware with limited resources, such as edge devices or mobile phones. Quantization can result in some loss of model accuracy, but it's a trade-off that balances efficiency with performance, enabling the deployment of deep learning models in resource-constrained environments without significant sacrifices in functionality.</li> </ul> <p>Fun fact: all those definitions were generated by an LLM...</p> <p>Do you want to know more?</p> <p>Here are a few worth reading articles:</p> <ul> <li>Best article ever: A jargon-free explanation of how AI large language models work</li> <li>Understanding LLama2 and its architecture</li> <li>RAG vs Fine-Tuning, which is best?</li> </ul>"},{"location":"generative-ai/llm-serving/#llm-serving_1","title":"LLM Serving","text":"<p>LLM Serving is not a trivial task, at least in a production environment...</p> <p></p> <ul> <li>LLMs are usually huge (several GBs, tens of GBs...) and require GPU(s) with enough memory if you want decent accuracy and performance. Granted, you can run smaller models on home hardware with good results, but that's not the subject here. After all we are on OpenShift, so more in a large organization environment than in an enthusiastic programmer basement!</li> <li>A served LLM will generally be used by multiple applications and users simultaneously. Since you can't just throw resources at it and scale your infrastructure easily because of the previous point, you want to optimize response time by for example batching queries, caching or buffering them,... Those are special operations that have to be handled specifically.</li> <li>When you load an LLM, there are parameters you want to tweak at load time, so a \"generic\" model loader is not the best suited solution.</li> </ul>"},{"location":"generative-ai/llm-serving/#llm-serving-solutions","title":"LLM Serving solutions","text":"<p>Fortunately, we have different solutions to handle LLM Serving.</p> <p>On this repo you will find:</p> <ul> <li>Recipes to deploy different types of LLM Servers, either using the Single Stack Model Serving available in ODH and RHOAI, or as Standalone deployments.</li> <li>Notebook examples on how to query those different servers are available.</li> </ul>"},{"location":"generative-ai/llm-serving/#serving-runtimes-for-single-stack-model-serving","title":"Serving Runtimes for Single Stack Model Serving","text":"<p>On top of the Caikit+TGIS or TGIS built-in runtimes, the following custom runtimes can be imported in the Single-Model Serving stack of Open Data Hub or OpenShift AI.</p> <ul> <li>vLLM Serving Runtime</li> <li>Hugging Face Text Generation Inference</li> </ul>"},{"location":"generative-ai/llm-serving/#standalone-inference-servers","title":"Standalone Inference Servers","text":"<ul> <li>vLLM: how to deploy vLLM, the \"Easy, fast, and cheap LLM serving for everyone\".<ul> <li>on GPU: </li> <li>on CPU:</li> </ul> </li> <li>Hugging Face TGI: how to deploy the Text Generation Inference server from Hugging Face.</li> <li>Caikit-TGIS-Serving: how to deploy the Caikit-TGIS-Serving stack, from OpenDataHub.</li> </ul>"},{"location":"generative-ai/llm-serving/#what-are-the-differences","title":"What are the differences?","text":"<p>You should read the documentation of those different model servers, as they present different characteristics:</p> <ul> <li>Different acceleration features can be implemented from one solution to the other. Depending on the model you choose, some features may be required.</li> <li>The endpoints can be accessed in REST or gRPC mode, or both, depending on the server.</li> <li>APIs are different, with some solutions offering an OpenAI compatible, which simplifies the integration with some libraries like Langchain.</li> </ul>"},{"location":"generative-ai/llm-serving/#which-model-to-use","title":"Which model to use?","text":"<p>In this section we will assume that you want to work with a \"local\" open source model, and not consume a commercial one through an API, like OpenAI's ChatGPT or Anthropic's Claude.</p> <p>There are literally hundreds of thousands of models available, almost all of them available on the Hugging Face site. If you don't know what this site is, you can think of it as what Quay or DockerHub are for containers: a big repository of models and datasets ready to download and use. Of course Hugging Face (the company) is also creating code, providing hosting capabilities,... but that's another story.</p> <p>So which model to choose will depend on several factors:</p> <ul> <li>Of course how good this model is. There are several benchmarks that have been published, as well as constantly updated rankings.</li> <li>The dataset it was trained on. Was it curated or just raw data from anywhere, does it contain nsfw material,...? And of course what the license is (some datasets are provided for research only or non-commercial).</li> <li>The license of the model itself. Some are fully open source, some claim to be... They may be free to use in most cases, but have some restrictions attached to them (looking at you Llama2...).</li> <li>The size of the model. Unfortunately that may be the most restrictive point for your choice. The model simply must fit on the hardware you have at your disposal, or the amount of money you are willing to pay.</li> </ul> <p>Currently, a good LLM with interesting performance for a relatively small size is Mistral-7B. Fully Open Source with an Apache 2.0 license, it will fit in an unquantized version on about 22GB of VRAM, which is perfect for an A10G card.</p> <p>Embeddings are another type of model often associated with LLMs as they are used to convert documents into vectors. A database of those vectors can then be queried to find the documents related to a query you make. This is very useful for the RAG solution we are going to talk about later on. NomicAI recently released a really performant and fully open source embeddings model, nomic-embed-text-v1</p>"},{"location":"generative-ai/llm-serving/#llm-consumption","title":"LLM Consumption","text":"<p>Once served, consuming an LLM is pretty straightforward, as at the end of the day it's only an API call.</p> <p>You can always query those models directly through a curl command or a simple request using Python. However, for easier consumption and integration with other tools, a few libraries/SDKs are available to streamline the process. They will allow you to easily connect to Vector Databases or Search Agents, chain multiple models, tweak parameters,... in a few lines of code. The main libraries at the time of this writing are Langchain, LlamaIndex and Haystack.</p> <p>In the LLM on OpenShift examples section repo, you will find several notebooks and full UI examples that will show you how to use those libraries with different types of model servers to create your own Chatbot!</p>"},{"location":"generative-ai/what-is-generative-ai/","title":"What is Generative AI?","text":"<p>Generative AI generally has the following characteristics:</p> <ul> <li>A model generally created based on a vast and broad set of data.</li> <li>Aims to generate novel content or data</li> <li>by mimicking patterns found in the training data</li> <li>e.g. \"compose an email to let the customer know that their loan application has been denied\"</li> </ul>"},{"location":"getting-started/opendatahub/","title":"What is Open Data Hub?","text":"<p>Open Data Hub (ODH) is an open source project that provides open source AI tools for running large and distributed AI workloads on the OpenShift Container Platform. Currently, the Open Data Hub project provides open source tools for distributed AI and Machine Learning (ML) workflows, Jupyter Notebook development environment and monitoring. The Open Data Hub project roadmap offers a view on new tools and integration the project developers are planning to add.</p> <p></p> <p>Included in the Open Data Hub core deployment is several open source components, which can be individually enabled. They include:</p> <ul> <li>Jupyter Notebooks</li> <li>ODH Dashboard</li> <li>Data Science Pipelines</li> <li>Model Mesh Serving</li> </ul> <p>Want to know more?</p>"},{"location":"getting-started/openshift-ai/","title":"OpenShift AI","text":""},{"location":"getting-started/openshift-ai/#what-is-red-hat-openshift-ai","title":"What is Red Hat OpenShift AI?","text":"<p>Red Hat OpenShift AI (RHOAI) builds on the capabilities of Red Hat OpenShift to provide a single, consistent, enterprise-ready hybrid AI and MLOps platform. It provides tools across the full lifecycle of AI/ML experiments and models including training, serving, monitoring, and managing AI/ML models and AI-enabled applications.</p> <p></p> <p>Note</p> <p>Red Hat OpenShift AI was previously named Red Hat OpenShift Data Science. Some texts or screenshots may not yet reflect this change.</p> <p>Documentation for Self-Managed RHOAI</p> <p>Documentation for Managed RHOAI</p> <p>Want to know more?</p>"},{"location":"getting-started/openshift/","title":"OpenShift and AI","text":""},{"location":"getting-started/openshift/#what-is-red-hat-openshift","title":"What is Red Hat OpenShift?","text":"<p>Red Hat OpenShift brings together tested and trusted services to reduce the friction of developing, modernizing, deploying, running, and managing applications. Built on Kubernetes, it delivers a consistent experience across public cloud, on-premise, hybrid cloud, or edge architecture. Choose a self-managed or fully managed solution. No matter how you run it, OpenShift helps teams focus on the work that matters.</p> <p>Want to know more?</p>"},{"location":"getting-started/openshift/#why-ai-on-openshift","title":"Why AI on OpenShift?","text":"<p>AI/ML on OpenShift accelerates AI/ML workflows and the delivery of AI-powered intelligent application.</p>"},{"location":"getting-started/openshift/#mlops-with-red-hat-openshift","title":"MLOps with Red Hat OpenShift","text":"<p>Red Hat OpenShift includes key capabilities to enable machine learning operations (MLOps) in a consistent way across datacenters, public cloud computing, and edge computing.</p> <p>By applying DevOps and GitOps principles, organizations automate and simplify the iterative process of integrating ML models into software development processes, production rollout, monitoring, retraining, and redeployment for continued prediction accuracy.</p> <p>Learn more</p>"},{"location":"getting-started/openshift/#what-is-a-ml-lifecycle","title":"What is a ML lifecycle?","text":"<p>A multi-phase process to obtain the power of large volumes and a variety of data, abundant compute, and open source machine learning tools to build intelligent applications.</p> <p>At a high level, there are four steps in the lifecycle:</p> <ol> <li>Gather and prepare data to make sure the input data is complete, and of high quality</li> <li>Develop model, including training, testing, and selection of the model with the highest prediction accuracy</li> <li>Integrate models in application development process, and inferencing</li> <li>Model monitoring and management, to measure business performance and address potential production data drift</li> </ol> <p>On this site, you will find recipes, patterns, demos for various AI/ML tools and applications used through those steps.</p>"},{"location":"getting-started/openshift/#why-use-containers-and-kubernetes-for-your-machine-learning-initiatives","title":"Why use containers and Kubernetes for your machine learning initiatives?","text":"<p>Containers and Kubernetes are key to accelerating the ML lifecycle as these technologies provide data scientists the much needed agility, flexibility, portability, and scalability to train, test, and deploy ML models.</p> <p>Red Hat\u00ae OpenShift\u00ae is the industry's leading containers and Kubernetes hybrid cloud platform. It provides all these benefits, and through the integrated DevOps capabilities (e.g. OpenShift Pipelines, OpenShift GitOps, and Red Hat Quay) and integration with hardware accelerators, it enables better collaboration between data scientists and software developers, and accelerates the roll out of intelligent applications across hybrid cloud (data center, edge, and public clouds).</p>"},{"location":"getting-started/why-this-site/","title":"Why this site?","text":"<p>As data scientists and engineers, it's easy to find detailed documentation on the tools and libraries we use. But what about end-to-end data pipeline solutions that involve multiple products? Unfortunately, those resources can be harder to come by. Open source communities often don't have the resources to create and maintain them. But don't worry, that's where this website comes in!</p> <p>We've created a one-stop-shop for data practitioners to find recipes, reusable patterns, and actionable demos for building AI/ML solutions on OpenShift. And the best part? It's a community-driven resource site! So, feel free to ask questions, make feature requests, file issues, and even submit PRs to help us improve the content. Together, we can make data pipeline solutions easier to find and implement.</p> <p></p>"},{"location":"odh-rhoai/accelerator-profiles/","title":"Accelerator Profiles","text":""},{"location":"odh-rhoai/accelerator-profiles/#accelerator-profiles","title":"Accelerator Profiles","text":"<p>To effectively use accelerators in OpenShift AI, OpenShift AI Administrators need to create and manage associated accelerator profiles. </p> <p>An accelerator profile is a custom resource definition (CRD) that defines specifications for this accelerator. It can be direclty managed via the OpenShift AI Dashboard under Settings \u2192 Accelerator profiles.</p> <p>When working with GPU nodes in OpenShift AI, it is essential to set proper taints on those nodes. This prevents unwanted workloads from being scheduled on them when they don't have specific tolerations set. Those tolerations are configured in the accelerator profiles associated with each type of GPU, then applied to the workloads (Workbenches, Model servers,...) for which you have selected an accelerator profile.</p> <p>The taints in the GPU Worker Nodes should be set like this:</p> <pre><code>  taints:\n    - effect: NoSchedule\n      key: nvidia.com/gpu\n      value: NVIDIA-A10G-SHARED\n</code></pre> <p>A corresponding Accelerator profile can then be created to allow workloads to run on this type of node (in this example, nodes having an A10G GPU). Workloads that use another accelerator profile (for another type of GPU for example) or that don't have any Accelerator profile set will not be scheduled on nodes tainted with NVIDIA-A10G-SHARED.</p> <p>For a detailed guide on configuring and managing accelerator profiles in OpenShift AI, refer to our repository.</p>"},{"location":"odh-rhoai/configuration/","title":"ODH and RHOAI Configuration","text":""},{"location":"odh-rhoai/configuration/#standard-configuration","title":"Standard configuration","text":"<p>As an administrator of ODH/RHOAI, you have access to different settings through the Settings menu on the dashboard:</p> <p></p>"},{"location":"odh-rhoai/configuration/#custom-notebook-images","title":"Custom notebook images","text":"<p>This is where you can import other notebook images. You will find resources on available custom images and learn how to create your own in the Custom Notebooks section.</p> <p>To import a new image, follow those steps.</p> <ul> <li>Click on import image.</li> </ul> <p></p> <ul> <li>Enter the full address of your container, set a name (this is what will appear in the launcher), and a description.</li> </ul> <p></p> <ul> <li>On the bottom part, add information regarding the software and the packages that are present in this image. This is purely informative.</li> </ul> <p></p> <ul> <li>Your image is now listed and enabled. You can hide it without removing it by simply disabling it.</li> </ul> <p></p> <ul> <li>It is now available in the launcher, as well as in the Data Science Projects.</li> </ul> <p></p>"},{"location":"odh-rhoai/configuration/#cluster-settings","title":"Cluster settings","text":"<p>In this panel, you can adjust:</p> <ul> <li>The default size of the volumes created for new users.</li> <li>Whether you want to stop idle notebooks and, if so, after how much time.</li> </ul> <p>Note</p> <p>This feature currently looks at running Jupyter kernels, like a Python notebook. If you are only using a Terminal, or another IDE window like VSCode or RStudio from the custom images, this activity is not detected and your Pod can be stopped without notice after the set delay.</p> <ul> <li>Whether you allow usage data to be collected and reported.</li> <li>Whether you want to add a toleration to the notebook pods to allow them to be scheduled on tainted nodes. That feature is really useful if you want to dedicate specific worker nodes to running notebooks. Tainting them will prevent other workloads from running on them. Of course, you have to add the toleration to the pods.</li> </ul> <p></p>"},{"location":"odh-rhoai/configuration/#user-management","title":"User management","text":"<p>In this panel, you can edit who has access to RHOAI by defining the \"Data Science user groups\", and who has access to the Settings by defining the \"Data Science administrator groups\".</p> <p></p>"},{"location":"odh-rhoai/configuration/#advanced-configuration","title":"Advanced configuration","text":""},{"location":"odh-rhoai/configuration/#dashboard-configuration","title":"Dashboard configuration","text":"<p>RHOAI or ODH main configuration is done through a Custom Resource (CR) of type <code>odhdashboardconfigs.opendatahub.io</code>.</p> <ul> <li>To get access to it, from your OpenShift console, navigate to Home-&gt;API Explorer, and filter for <code>OdhDashboardConfig</code>:</li> </ul> <p></p> <ul> <li>Click on <code>OdhDashboardConfig</code> and in the Instances tab, click on <code>odh-dashboard-config</code>:</li> </ul> <p></p> <ul> <li>You can now view and edit the YAML file to modify the configuration:</li> </ul> <p></p> <p>In the <code>spec</code> section, the following items are of interest:</p> <ul> <li><code>dashboardConfig</code>: The different toggles will allow you to activate/deactivate certain features. For example, you may want to hide Model Serving for your users or prevent them from importing custom images.</li> <li><code>notebookSizes</code>: This is where you can fully customize the sizes of the notebooks. You can modify the resources and add or remove sizes from the default configuration as needed.</li> <li><code>modelServerSizes</code>: This setting operates on the same concept as the previous setting but for model servers.</li> <li><code>notebookController</code>: In this section you will find various settings related to the Workbenches and how they are launched.</li> <li>If your GPUs are not correctly detected, the dropdown allowing you to select how many GPUs you want to use for a workbench will not be displayed. To force it, you can create/modify the parameter <code>gpuSetting</code> under <code>notebookController</code>. This will force the dropdown to appear, with the maximum being the number you set for the parameter. Example:</li> </ul> <pre><code>notebookController:\n    enabled: true\n    gpuSetting: '4'\n    ...\n</code></pre>"},{"location":"odh-rhoai/configuration/#adding-a-custom-application","title":"Adding a custom application","text":"<p>Let's say you have installed another application in your cluster and want to make it available through the dashboard. That's easy! A tile is, in fact, represented by a custom resource (CR) of type <code>OdhApplication</code>.</p> <p>In this example, we will add a tile to access the MLFlow UI (see the MLFlow installation instructions to test it).</p> <ul> <li>The file mlflow-tile.yaml provides you with an example of how to create the tile.</li> <li>Edit this file to set the <code>route</code> (the name of the Route CR) and <code>routeNamespace</code> parameters to where the UI is accessible. In this example, it is <code>mlflow-server</code>(route name) and <code>mlflow</code> (server). Apply this file to create the resource.</li> <li>Wait 1-2 minutes for the change to take effect. Your tile is now available in the Explore view (bottom left):</li> </ul> <p></p> <ul> <li>However, it is not yet enabled. To enable this tile, click on it in the Explorer view, then click the \"Enable\" button at the top of the description. You can also create a ConfigMap from the file cm-mlflow-enable.yaml.</li> <li>Wait another 1-2 minutes, and your tile is now ready to use in the Enabled view:</li> </ul> <p></p>"},{"location":"odh-rhoai/connect-vscode-to-rhoai-wb/","title":"Connect to RHOAI Workbench Kernel from local VS Code","text":"<p>Some users have expressed their desire to work directly on their local IDE and execute the Jupyter notebook(s) using the kernel on remote workbench running on RHOAI. Most IDEs provide connection to a remote Kernel as a standard feature. However, this standard feature does not work with RHOAI because of the way authentication to workbench is set up in RHOAI The standard feature of most IDEs to connect to remote kernel uses token based authentication. Workbench pods running on RHOAI contain an authentication mechanism that sits in front of the workbench container and handles the authentication of the user connecting to the workbench. This container uses Openshift Authentication mechanism and is not compatible with the standard connection feature of most IDEs.</p>"},{"location":"odh-rhoai/connect-vscode-to-rhoai-wb/#workaround-connect-to-the-remote-kernel-using-openshift-port-forwarding","title":"Workaround: Connect to the remote kernel using Openshift port-forwarding","text":"<p>Use the following steps to connect your local VS Code to RHOAI Workbench kernel:</p> <ul> <li> <p>In your data science project in RHOAI, create a workbench that you would like to use as your remote kernel. If you want to use gpu accelerator, use the compatible workbench image (e.g. pytorch).</p> <p></p> </li> <li> <p>Open the workbench and copy the context path from the browser. You will need this later when connecting from VS Code.</p> <p></p> </li> <li> <p>From terminal on your laptop/desktop login to openshift.     </p> </li> <li> <p>Switch to your data science project     </p> </li> <li> <p>Start port-forwarding to your workbench pod</p> <ul> <li>List all the pods in your project. The pod running your workbench is named using the name of your workbench in RHOAI. e.g. <code>mywb-0</code> if your workbench name is <code>mywb</code>.</li> <li>Enable port-forwarding to your workbench pod. You need to forward to the port the pod is listening on. It is usually 8888 for RHOAI workbench. You can find this port from the service in your project with name same as your workbench. </li> </ul> </li> <li> <p>Open the Jupyter notebook in your VS Code     </p> </li> <li> <p>From the top right corner of the notebook, click on <code>Select Kernel</code>.     </p> </li> <li> <p>From the options, select <code>Existing Jupyter Server</code> and then enter the url as follows:     <code>localhost</code> <code>[:port]</code> <code>/context-path</code> copied earlier that has the pattern <code>/notebook/ds-project-name/workbench-name/lab</code>. e.g. <code>http://localhost:8888/notebook/rag-llm-demo/mywb/lab</code></p> </li> <li> <p>A prompt saying  <code>Connecting over HTTP without a token may be an insecure connection. Do you want to connect to a possibly insecure server?</code> is displayed. select <code>Yes</code> </p> </li> <li> <p>Select the prompted <code>Server display name</code> or enter a new one.     </p> </li> <li> <p>A list of available kernels is displayed. Choose <code>Python 3.9</code>.     </p> </li> <li> <p>You should see the selected Kernel in the top right corner.     </p> </li> <li> <p>The code inside of your notebook will now execute using the remote kernel on the RHOAI workbench pod.</p> </li> <li> <p>If your workbench uses a Nvidia GPU, you can verify that it is being used in the execution of your notebook by adding a command <code>!nvidia-smi</code>. You should see output similar to the image below.</p> <p></p> </li> </ul>"},{"location":"odh-rhoai/connect-vscode-to-rhoai-wb/#caveats","title":"Caveats","text":"<ul> <li>Jupyter notebooks in your local VSCode environment will not be saved to the workbench.</li> <li>If your notebook uses any files (models, inputdata etc.), they should be present on the workbench and their path should match the path specified in your notebook.</li> </ul>"},{"location":"odh-rhoai/custom-notebooks/","title":"Custom Notebooks","text":"<p>Custom notebook images are useful if you want to add libraries that you often use, or that you require at a specific version different than the one provided in the base images. It's also useful if you need to use OS packages or applications, which you cannot install on the fly in your running environment.</p>"},{"location":"odh-rhoai/custom-notebooks/#image-source-and-pre-built-images","title":"Image source and Pre-built images","text":"<p>In the opendatahub-io-contrib/workbench-images repository, you will find the source code as well as pre-built images for a lot of use cases. A few of the available images are:</p> <ul> <li>Base and CUDA-enabled images for different \"lines\" of OS: UBI8, UBI9, and Centos Stream 9.</li> <li>Jupyter images enhanced with:<ul> <li>specific libraries like OptaPy or Monai,</li> <li>with integrated applications like Spark,</li> <li>providing other IDEs like VSCode or RStudio</li> </ul> </li> <li>VSCode</li> <li>RStudio</li> </ul> <p>All those images are constantly and automatically updated and rebuilt for the latest patch and fixes, and new releases are available regularly to provide new versions of the libraries or the applications.</p>"},{"location":"odh-rhoai/custom-notebooks/#building-your-own-images","title":"Building your own images","text":"<p>In the repository above, you will find many examples from the source code to help you understand how to create your own image. Here are a few rules, tips and examples to help you.</p>"},{"location":"odh-rhoai/custom-notebooks/#rules","title":"Rules","text":"<ul> <li>On OpenShift, every containers in a standard namespace (unless you modify security) run with a user with a random user id (uid), and the group id (gid) 0. Therefore, all the folders that you want to write in, and all the files you want to modify (temporarily) in your image must be accessible by this user. The best practice is to set the ownership at 1001:0 (user \"default\", group \"0\").</li> <li>If you don't want/can't do that, another solution is to set permissions properly for any user, like 775.</li> <li>When launching a notebook from Applications-&gt;Enabled, the \"personal\" volume of a user is mounted at <code>/opt/app-root/src</code>. This is not configurable, so make sure to build your images with this default location for the data that you want persisted.</li> </ul>"},{"location":"odh-rhoai/custom-notebooks/#how-tos","title":"How-tos","text":""},{"location":"odh-rhoai/custom-notebooks/#install-python-packages","title":"Install Python packages","text":"<ul> <li>Start from a base image of your choice. Normally it's already running under user 1001, so no need to change it.</li> <li>Copy your pipfile.lock or your requirements.txt</li> <li>Install your packages</li> </ul> <p>Example:</p> <pre><code>FROM BASE_IMAGE\n\n# Copying custom packages\nCOPY Pipfile.lock ./\n\n# Install packages and cleanup\n# (all commands are chained to minimize layer size)\nRUN echo \"Installing softwares and packages\" &amp;&amp; \\\n    # Install Python packages \\\n    micropipenv install &amp;&amp; \\\n    rm -f ./Pipfile.lock\n    # Fix permissions to support pip in OpenShift environments \\\n    chmod -R g+w /opt/app-root/lib/python3.9/site-packages &amp;&amp; \\\n    fix-permissions /opt/app-root -P\n\nWORKDIR /opt/app-root/src\n\nENTRYPOINT [\"start-notebook.sh\"]\n</code></pre> <p>In this example, the fix-permissions script (present in all standard images and custom images from the opendatahub-contrib repo) fixes any bad ownership or rights that may be present.</p>"},{"location":"odh-rhoai/custom-notebooks/#install-an-os-package","title":"Install an OS package","text":"<ul> <li>If you have to install OS packages and Python packages, it's better to start with the OS.</li> <li>In your Containerfile/Dockerfile, switch to user 0, install your package(s), then switch back to user 1001. Example:</li> </ul> <pre><code>USER 0\n\nRUN INSTALL_PKGS=\"java-11-openjdk java-11-openjdk-devel\" &amp;&amp; \\\n    yum install -y --setopt=tsflags=nodocs $INSTALL_PKGS &amp;&amp; \\\n    yum -y clean all --enablerepo='*'\n\nUSER 1001\n</code></pre>"},{"location":"odh-rhoai/custom-notebooks/#tips-and-tricks","title":"Tips and tricks","text":""},{"location":"odh-rhoai/custom-notebooks/#enabling-codeready-builder-crb-and-epel","title":"Enabling CodeReady Builder (CRB) and EPEL","text":"<p>CRB and EPEL are repositories providing packages absent from a standard RHEL or UBI installation. They are useful and required to be able to install specific software (RStudio, I'm looking at you...).</p> <ul> <li>Enabling EPEL on UBI9-based images (on UBI9 images CRB is now enabled by default.):</li> </ul> <pre><code>RUN yum install -y https://download.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm\n</code></pre> <ul> <li>Enabling CRB and EPEL on Centos Stream 9-based images:</li> </ul> <pre><code>RUN yum install -y yum-utils &amp;&amp; \\\n    yum-config-manager --enable crb &amp;&amp; \\\n    yum install -y https://download.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm\n</code></pre>"},{"location":"odh-rhoai/custom-notebooks/#minimizing-image-size","title":"Minimizing image size","text":"<p>A container image uses a \"layered\" filesystem. Every time you have in your file a COPY or a RUN command, a new layer is created. Nothing is ever deleted: removing a file is simply \"masking\" it in the next layer. Therefore you must bee very careful when you create your Containerfile/Dockerfile.</p> <ul> <li>If you start from an image that is constantly updated, like ubi9/python-39 from the Red Hat Catalog, don't do a <code>yum update</code>. This will only fetch new metadata, update a few files that may not have any impact, and get you a bigger image.</li> <li>Rebuilt your images often from scratch, but don't do a <code>yum update</code> on a previous version.</li> <li>Group your <code>RUN</code> commands as much as you can, add <code>&amp;&amp; \\</code> at the end of each line to chain your commands.</li> <li>If you need to compile something for building an image, use the multi-stage builds approach. Build the library or application in an intermediate container image, then copy the result to your final image. Otherwise, all the build artefacts will persist in your image...</li> </ul>"},{"location":"odh-rhoai/custom-runtime-triton/","title":"Deploying and using a Custom Serving Runtime in ODH/RHOAI","text":"<p>Although these instructions were tested mostly using RHOAI (Red Hat OpenShift AI), they apply to ODH (Open Data Hub) as well.</p>"},{"location":"odh-rhoai/custom-runtime-triton/#before-you-start","title":"Before you start","text":"<p>This document will guide you through the broad steps necessary to deploy a custom Serving Runtime in order to serve a model using the Triton Runtime (NVIDIA Triton Inference Server).</p> <p>While RHOAI supports your ability to add your own runtime, it does not support the runtimes themselves. Therefore, it is up to you to configure, adjust and maintain your custom runtimes.</p> <p>This document expects a bit of familiarity with RHOAI.</p> <p>The sources used to create this document are mostly:</p> <ul> <li>https://github.com/kserve/modelmesh-serving/tree/main/config/runtimes</li> <li>https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver</li> <li>Official Red Hat OpenShift AI Documentation</li> </ul>"},{"location":"odh-rhoai/custom-runtime-triton/#adding-the-custom-triton-runtime","title":"Adding the custom triton runtime","text":"<ol> <li>Log in to your OpenShift AI with a user who is part of the RHOAI admin group.<ol> <li>(by default, cluster-admins and dedicated admins are).</li> </ol> </li> <li> <p>Navigate to the Settings menu, then Serving Runtimes</p> <p></p> </li> <li> <p>Click on the Add Serving Runtime button:</p> <p></p> </li> <li> <p>From the drop down menu, select **Multi-model serving platform.  The option for REST will be selected automatically:</p> <p></p> </li> <li> <p>Click on Start from scratch and in the window that opens up, paste the following YAML:   <pre><code># Copyright 2021 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\napiVersion: serving.kserve.io/v1alpha1\n# kind: ClusterServingRuntime     ## changed by EG\nkind: ServingRuntime\nmetadata:\n  name: triton-23.05-20230804\n  labels:\n    name: triton-23.05-20230804\n  annotations:\n    maxLoadingConcurrency: \"2\"\n    openshift.io/display-name: \"Triton runtime 23.05 - added on 20230804 - with /dev/shm\"\nspec:\n  supportedModelFormats:\n    - name: keras\n      version: \"2\" # 2.6.0\n      autoSelect: true\n    - name: onnx\n      version: \"1\" # 1.5.3\n      autoSelect: true\n    - name: pytorch\n      version: \"1\" # 1.8.0a0+17f8c32\n      autoSelect: true\n    - name: tensorflow\n      version: \"1\" # 1.15.4\n      autoSelect: true\n    - name: tensorflow\n      version: \"2\" # 2.3.1\n      autoSelect: true\n    - name: tensorrt\n      version: \"7\" # 7.2.1\n      autoSelect: true\n\n  protocolVersions:\n    - grpc-v2\n  multiModel: true\n\n  grpcEndpoint: \"port:8085\"\n  grpcDataEndpoint: \"port:8001\"\n\n  volumes:\n    - name: shm\n      emptyDir:\n        medium: Memory\n        sizeLimit: 2Gi\n  containers:\n    - name: triton\n      # image: tritonserver-2:replace   ## changed by EG\n      image: nvcr.io/nvidia/tritonserver:23.05-py3\n      command: [/bin/sh]\n      args:\n        - -c\n        - 'mkdir -p /models/_triton_models;\n          chmod 777 /models/_triton_models;\n          exec tritonserver\n          \"--model-repository=/models/_triton_models\"\n          \"--model-control-mode=explicit\"\n          \"--strict-model-config=false\"\n          \"--strict-readiness=false\"\n          \"--allow-http=true\"\n          \"--allow-sagemaker=false\"\n          '\n      volumeMounts:\n        - name: shm\n          mountPath: /dev/shm\n      resources:\n        requests:\n          cpu: 500m\n          memory: 1Gi\n        limits:\n          cpu: \"5\"\n          memory: 1Gi\n      livenessProbe:\n        # the server is listening only on 127.0.0.1, so an httpGet probe sent\n        # from the kublet running on the node cannot connect to the server\n        # (not even with the Host header or host field)\n        # exec a curl call to have the request originate from localhost in the\n        # container\n        exec:\n          command:\n            - curl\n            - --fail\n            - --silent\n            - --show-error\n            - --max-time\n            - \"9\"\n            - http://localhost:8000/v2/health/live\n        initialDelaySeconds: 5\n        periodSeconds: 30\n        timeoutSeconds: 10\n  builtInAdapter:\n    serverType: triton\n    runtimeManagementPort: 8001\n    memBufferBytes: 134217728\n    modelLoadingTimeoutMillis: 90000\n</code></pre></p> </li> <li>You will likely want to update the name , as well as other parameters.</li> <li>Click Add</li> <li> <p>Confirm the new Runtime is in the list, and re-order the list as needed.   (the order chosen here is the order in which the users will see these choices)</p> <p></p> </li> </ol>"},{"location":"odh-rhoai/custom-runtime-triton/#creating-a-project","title":"Creating a project","text":"<ul> <li>Create a new Data Science Project</li> <li>In this example, the project is called fraud</li> </ul>"},{"location":"odh-rhoai/custom-runtime-triton/#creating-a-model-server","title":"Creating a model server","text":"<ol> <li>In your project, scroll down to the \"Models and Model Servers\" Section</li> <li> <p>Click on Configure server</p> <p></p> </li> <li> <p>Fill out the details:</p> <p></p> <p></p> </li> <li> <p>Click Configure</p> </li> </ol>"},{"location":"odh-rhoai/custom-runtime-triton/#deploying-a-model-into-it","title":"Deploying a model into it","text":"<ol> <li>If you don't have any model files handy, you can grab a copy of this file and upload it to your Object Storage of choice.</li> <li> <p>Click on Deploy Model</p> <p></p> </li> <li> <p>Choose a model name and a framework:</p> <p></p> </li> <li> <p>Then create a new data connection containing the details of where your model is stored in Object Storage:</p> <p></p> </li> <li> <p>After a little while, you should see the following:</p> <p></p> </li> </ol>"},{"location":"odh-rhoai/custom-runtime-triton/#validating-the-model","title":"Validating the model","text":"<ol> <li>If you've used the model mentioned earlier in this document, you can run the following command from a Linux prompt:   <pre><code>function val-model {\n    myhost=\"$1\"\n    echo \"validating host $myhost\"\n    time curl -X POST -k \"${myhost}\" -d '{\"inputs\": [{ \"name\": \"dense_input\", \"shape\": [1, 7], \"datatype\": \"FP32\", \"data\": [57.87785658389723,0.3111400080477545,1.9459399775518593,1.0,1.0,0.0,0.0]}]}' | jq\n}\n\nval-model \"https://fraud-model-fraud.apps.mycluster.openshiftapps.com/v2/models/fraud-model/infer\"\n</code></pre></li> <li>Change the host to match the address for your model.</li> <li>You should see an output similar to:   <pre><code>{\n  \"model_name\": \"fraud-model__isvc-c1529f9667\",\n  \"model_version\": \"1\",\n  \"outputs\": [\n    {\n      \"name\": \"dense_3\",\n      \"datatype\": \"FP32\",\n      \"shape\": [\n        1,\n        1\n      ],\n      \"data\": [\n        0.86280495\n      ]\n    }\n  ]\n}\n</code></pre></li> </ol>"},{"location":"odh-rhoai/custom-runtime-triton/#extra-considerations-for-disconnected-environments","title":"Extra considerations for Disconnected environments.","text":"<p>The YAML included in this file makes a reference to the following Nvidia Triton Image: <code>nvcr.io/nvidia/tritonserver:23.05-py3</code></p> <p>Ensure that this image is properly mirrored into the mirror registry.</p> <p>Also, update the YAML definition as needed to point to the image address that matches the image registry.</p>"},{"location":"odh-rhoai/custom-runtime-triton/#gitops-related-information","title":"GitOps related information","text":"<p>Each of the activities performed via the user interface will create a Kubernetes Object inside your OpenShift Cluster.</p> <ul> <li>The addition of a new runtime creates a <code>template</code> in the <code>redhat-ods-applications</code> namespace.</li> <li>Each model server is defined as a <code>ServingRuntime</code></li> <li>Each model is defined as an <code>InferenceService</code></li> <li>Each Data Connection is stored as a <code>Secret</code></li> </ul>"},{"location":"odh-rhoai/custom-workbench-anythingllm/","title":"Using AnythingLLM as Custom Workbench in ODH/RHOAI","text":"<p>AnythingLLM is the \"All-in-one AI application. Any LLM, any document, any agent, fully private.\" It is a fully open-source, customizable chatbot that allows users to interact with various documents locally or on a hosted platform. It enables conversations with documents in formats like PDFs, TXT, and CSVs. The tool is particularly useful for creating private, localized versions of ChatGPT that can process and answer questions based on uploaded files. </p> <p>In this post, we'll explore how to deploy AnythingLLM on your Red Hat OpenShift AI instance to create a private chatbot for your internal users. AnythingLLM allows teams to securely interact with documents and knowledge bases by integrating large language models (LLMs) with private data in a controlled environment. Combining it with OpenShift\u2019s robust platform for AI and its security features, we can set up AnythingLLM as an efficient internal chatbot. </p> <p>Whether your team needs to query PDFs, CSVs, or other document types, this guide will help you seamlessly bring AnythingLLM into your OpenShift AI infrastructure. </p>"},{"location":"odh-rhoai/custom-workbench-anythingllm/#prerequisites","title":"Prerequisites","text":""},{"location":"odh-rhoai/custom-workbench-anythingllm/#1-running-a-large-language-model","title":"1. Running a Large Language Model","text":"<p>To get started with AnythingLLM, you'll need an endpoint for a large language model (LLM) to interact with. AnythingLLM supports various models, so you're not limited to a single provider. You can deploy open-source models like Granite or Mistral, or use your own custom-trained LLM. To integrate the LLM, provide the endpoint URL and, if needed, an authentication token. Alternatively, you can adopt a \"Models as a Service (MaaS)\" approach, allowing users to experiment with different models to find the best fit. For MaaS setup guidance, refer here.</p>"},{"location":"odh-rhoai/custom-workbench-anythingllm/#2-custom-workbench-for-anythingllm-in-openshift-ai","title":"2. Custom Workbench for AnythingLLM in OpenShift AI","text":"<p>A workbench is your development and experimentation environment, similar to an IDE. OpenShift AI provides several default workbench images, but to use AnythingLLM, you'll need to add it as a custom workbench. This involves making the AnythingLLM container image available in your OpenShift AI environment for users to select in their projects.</p> <p>The AnythingLLM image is hosted on Quay. You can view its details here.</p> <p>To make this image available cluster-wide by using the OpenShift AI user interface:</p> <ul> <li>Log in to your OpenShift AI instance as <code>rhoai-admin</code>.</li> <li>From the left menu, navigate to Settings &gt; Notebook Images &gt; Import New Image.</li> <li>Enter <code>quay.io/rh-aiservices-bu/anythingllm-workbench</code> as the image location.</li> </ul> <p>For more guidance on building custom notebook images, check out this blog post.</p>"},{"location":"odh-rhoai/custom-workbench-anythingllm/#configuration-of-anythingllm-workbench","title":"Configuration of AnythingLLM Workbench","text":"<p>Now that you have the prerequisites set up, it\u2019s time to dive in! In the following steps, we\u2019ll walk you through creating a custom AnythingLLM workbench, configuring it to connect to an LLM, and getting hands-on with it. Let\u2019s bring your ideas to life and explore AnythingLLM!</p>"},{"location":"odh-rhoai/custom-workbench-anythingllm/#create-the-workbench","title":"Create the Workbench","text":"<p>Go to your Data Science project and click <code>Create a Workbench</code>.</p> <ul> <li>Name: Choose a name, such as \u201cAnythingLLM\u201d.</li> <li>Image selection: Select your <code>custom-anythingLLM-image</code>.</li> <li>Deployment size: Choose <code>Small</code> (no GPU is needed for this setup).</li> <li>Cluster storage: It is to save your data for retrieval-augmented generation (RAG). Adjust the size based on your data needs.</li> </ul> <p>You can leave the rest of the settings as default and click Create.</p> <p></p>"},{"location":"odh-rhoai/custom-workbench-anythingllm/#launch-the-workbench","title":"Launch the Workbench","text":"<p>Once the workbench is running, click <code>Open</code>. On the first launch, you\u2019ll see a splash screen. Click on Get started.</p> <p></p>"},{"location":"odh-rhoai/custom-workbench-anythingllm/#configure-the-llm-endpoint","title":"Configure the LLM Endpoint","text":"<p>We\u2019ll connect the workbench to an LLM running in your environment.</p> <ul> <li>Provider: Choose Generic OpenAI from the available options.</li> <li>Base URL: Enter the endpoint of your model, ending with /v1 (e.g., https://mistral-7b-instruct-v0-3-mycluster.com:443/v1).</li> <li>API Key &amp; Model Name: Enter the required API key and specify the model name.</li> <li>Token Context Window: Set the context window size. This value may vary depending on the model you\u2019re using.</li> <li>Max Tokens: Specify the maximum number of tokens to be generated by default. 1024 is good to start, and you can adjust this later in your workspaces.</li> </ul> <p></p>"},{"location":"odh-rhoai/custom-workbench-anythingllm/#set-up-user-access","title":"Set Up User Access","text":"<p>On the next screen, select <code>Just me</code>. OpenShift\u2019s authentication ensures that only you can access your workbench anyhow, hence we select Just me.</p> <p></p> <p>Then you\u2019ll be prompted to set up a secondary password. Normally it\u2019s not necessary as access to the workbench is already secured with OpenShift authentication.</p> <p></p>"},{"location":"odh-rhoai/custom-workbench-anythingllm/#review-configuration","title":"Review Configuration","text":"<p>You will see a summary screen displaying your settings. Confirm that everything looks correct.</p> <p></p> <p>You might encounter a brief survey, which you can skip if you prefer.</p> <p></p>"},{"location":"odh-rhoai/custom-workbench-anythingllm/#create-your-first-workspace","title":"Create Your First Workspace","text":"<p>Next, you\u2019ll set up a workspace, which acts as a project area within AnythingLLM. Each workspace can have its own settings and data, allowing you to organize different tasks or experiments separately.</p> <p>Click on <code>Create Workspace</code> to proceed.</p> <p></p>"},{"location":"odh-rhoai/custom-workbench-anythingllm/#start-chatting","title":"Start Chatting","text":"<p>The initial view will provide you with various information, but you can go straight to your workspace and begin interacting with the LLM.</p> <p></p> <p>There\u2019s a lot you can do with AnythingLLM, so be sure to explore its features! For more details, check out the documentation.</p>"},{"location":"odh-rhoai/deploy-models-from-public-oci/","title":"Deploying Models from Public OCI Registries in OpenShift AI","text":"<p>When deploying models in OpenShift AI using the ModelCar approach, you can pull model images directly from OCI-compliant registries. ModelCar packages models as OCI images, simplifying deployment by eliminating the need to manually download models and upload them to S3 storage.</p> <p>Note</p> <p>While models hosted publicly are not going to be the norm in enterprise environments, they make things way easier for quick demos and testing purposes.</p>"},{"location":"odh-rhoai/deploy-models-from-public-oci/#two-options-for-public-registries","title":"Two Options for Public Registries","text":"<p>If you're deploying a model from a public OCI registry that doesn't require authentication, OpenShift AI offers two approaches:</p>"},{"location":"odh-rhoai/deploy-models-from-public-oci/#option-1-using-uri-v1","title":"Option 1: Using URI - v1","text":"<p>This is the simpler approach when working with public repositories:</p> <ol> <li>In the model deployment wizard, select Model location: <code>URI - v1</code></li> <li>Enter the OCI URI directly (e.g., <code>oci://quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-3b-instruct</code>)</li> <li>Check the box \"Create a connection to this location\" if you want to reuse this connection</li> </ol> <p></p> <p>Tip</p> <p>The URI option bypasses the need for authentication configuration entirely, making it the quickest path for deploying from public registries.</p>"},{"location":"odh-rhoai/deploy-models-from-public-oci/#option-2-using-oci-compliant-registry-with-empty-auth-secret","title":"Option 2: Using OCI Compliant Registry with Empty Auth Secret","text":"<p>Alternatively, you can use the OCI compliant registry - v1 option, but this requires providing secret details even for public registries:</p> <ol> <li>Select Model location: <code>OCI compliant registry - v1</code></li> <li>Choose or create an Access type (connection)</li> <li>In the Secret details field, provide an empty authentication JSON:</li> </ol> <pre><code>{\n  \"auths\": {}\n}\n</code></pre> <p></p> <p>The file format must be <code>.dockerconfigjson</code> or <code>.json</code>.</p>"},{"location":"odh-rhoai/deploy-models-from-public-oci/#when-to-use-each-option","title":"When to Use Each Option","text":"<ul> <li>Use URI - v1: When deploying from public registries and you want the simplest configuration</li> <li>Use OCI compliant registry - v1: When you need private registry authentication or want to manage credentials for multiple private registries</li> </ul>"},{"location":"odh-rhoai/deploy-models-from-public-oci/#example-deploying-a-public-modelcar","title":"Example: Deploying a Public ModelCar","text":"<p>Here's a complete example using a model from the Red Hat AI Services ModelCar Catalog:</p> <p>Model: Llama 3.2 3B Instruct</p> <p>Registry: quay.io (public)</p> <p>URI: <code>oci://quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-3b-instruct</code></p> <p>Using the URI - v1 option: 1. Enter the URI above 2. Provide a connection name (e.g., \"llama-3.2-public\") 3. Continue with your deployment configuration</p> <p>That's it! No authentication required.</p>"},{"location":"odh-rhoai/deploy-models-from-public-oci/#additional-resources","title":"Additional Resources","text":"<ul> <li>Red Hat AI Services ModelCar Catalog - Pre-built public ModelCar images</li> <li>Build and Deploy a ModelCar Container in OpenShift AI - Comprehensive guide from Red Hat Developer</li> <li>KServe ModelCar Documentation - Official KServe modelcar reference</li> </ul>"},{"location":"odh-rhoai/deploy-validated-models-on-disconnected/","title":"Deploying a Red Hat Validated Model in a Disconnected OpenShift AI Environment","text":"<p>Red Hat AI provides access to a rich repository of Third Party Models validated to run efficiently across the platform. These models are available through Red Hat\u2019s Hugging Face repository.</p> <p>The repository offers comprehensive information about each model\u2019s architecture, optimizations, deployment options, and evaluation metrics. This information helps you make informed decisions about model selection, deployment configurations, and hardware accelerator choices tailored to your domain-specific use cases.</p> <p>In this blog post, we will walk you through how to deploy one of these validated models into your disconnected Red Hat OpenShift AI platform.</p>"},{"location":"odh-rhoai/deploy-validated-models-on-disconnected/#step-by-step-guide-to-deploy-a-model-in-a-disconnected-environment","title":"Step-by-Step Guide to Deploy a Model in a Disconnected Environment","text":""},{"location":"odh-rhoai/deploy-validated-models-on-disconnected/#1-select-the-model","title":"1. Select the Model","text":"<p>For this blog post, as an example, we will deploy one of the popular models: Llama 4 Scout 17B FP8 - an optimized large language model using FP8 quantization.</p>"},{"location":"odh-rhoai/deploy-validated-models-on-disconnected/#2-gather-required-image-information","title":"2. Gather Required Image Information","text":"<p>You have different options for deploying models to your OpenShift AI cluster. We recommend using Modelcar, as it eliminates the need to manually download models from Hugging Face, upload them to S3, and manage access. With Modelcar, you can package your models as OCI images and either pull them at runtime or precache them. This simplifies versioning, improves traceability, and integrates naturally with CI/CD workflows.</p> <p>With modelcar approach, in order to deploy this model, you need two images:</p> <ul> <li>A runtime image: The container runtime that runs the model.  </li> <li>A ModelCar image: The packaged model artifact for deployment.</li> </ul> <p>You can find this information in the model\u2019s Deployment section on the Hugging Face repository.</p> <p></p> <ul> <li>Runtime image (from the Deploy on Red Hat OpenShift AI \u2192 ServingRuntime section):</li> </ul> <pre><code>image: quay.io/modh/vllm:rhoai-2.20-cuda\n</code></pre> <ul> <li>ModelCar image (from the InferenceService section):</li> </ul> <pre><code>storageUri: oci://registry.redhat.io/rhelai1/modelcar-llama-4-scout-17b-16e-instruct-fp8-dynamic:1.5\n</code></pre> <p>That means, below are the two images you should have in your registry:</p> <pre><code>- quay.io/modh/vllm:rhoai-2.20-cuda\n- registry.redhat.io/rhelai1/modelcar-llama-4-scout-17b-16e-instruct-fp8-dynamic:1.5\n</code></pre>"},{"location":"odh-rhoai/deploy-validated-models-on-disconnected/#3-mirror-images-to-your-disconnected-openshift-cluster","title":"3. Mirror Images to Your Disconnected OpenShift Cluster","text":"<p>Mirroring container images to a disconnected OpenShift cluster means copying them from a connected environment (your local machine or a connected bastion host) to disconnected OpenShift cluster's internal image registry or a private image registry accessible from your disconnected cluster.</p> <p>Prerequisites:</p> <ul> <li>Access to a connected environment that can pull images from image registries.  </li> <li>Access to your disconnected OpenShift cluster and its internal or your private image registry.  </li> <li><code>oc</code> CLI installed and configured to access both environments.</li> </ul> <p>We will use the <code>oc image mirror</code> utility for this process. While this blog shows mirroring to an internal OpenShift registry, the same applies to external private registries (e.g., self-hosted Quay or Artifactory).</p> <p>Note:* If you\u2019re using a mirror registry configured with <code>oc-mirror</code>*, you can also include specific images by listing them under the <code>additionalImages</code> section in your <code>ImageSetConfig</code>. Refer to the official documentation for details.</p> <p>Before start mirroring, let\u2019s  verify that you are able to login to the relevant registries and your OpenShift cluster:</p> <pre><code>oc login &lt;your-cluster-api-url&gt;\noc registry login # for internal registry\npodman login registry.redhat.io\npodman login &lt;your-private-registry&gt;\n</code></pre>"},{"location":"odh-rhoai/deploy-validated-models-on-disconnected/#_1","title":"Deploying a Red Hat Validated Model in a Disconnected OpenShift AI Environment","text":""},{"location":"odh-rhoai/deploy-validated-models-on-disconnected/#4-mirroring-the-images","title":"4. Mirroring the Images","text":"<p>Mirroring time depends on image size, network speed, and registry performance. Small images may take a few minutes, while large model images can take longer.</p> <p>The general command format is:</p> <pre><code>oc image mirror &lt;source-image&gt; &lt;destination-image&gt;\n</code></pre>"},{"location":"odh-rhoai/deploy-validated-models-on-disconnected/#mirror-the-vllm-runtime-image","title":"Mirror the vLLM Runtime Image","text":"<pre><code>oc image mirror quay.io/modh/vllm:rhoai-2.20-cuda default-route-openshift-image-registry.apps.example-domain.com/&lt;project-name&gt;/vllm:rhoai-2.20-cuda\n</code></pre> <p>Note: If you're running OpenShift AI 2.20 or later and have already mirrored the required images, the vLLM image needed to serve this model may already be available in your environment.</p>"},{"location":"odh-rhoai/deploy-validated-models-on-disconnected/#mirror-the-modelcar-image","title":"Mirror the Modelcar Image","text":"<pre><code>oc image mirror registry.redhat.io/rhelai1/modelcar-llama-4-scout-17b-16e-instruct-fp8-dynamic:1.5 default-route-openshift-image-registry.apps.example-domain.com/&lt;project-name&gt;/modelcar-llama-4-scout-17b-16e-instruct-fp8-dynamic:1.5\n</code></pre>"},{"location":"odh-rhoai/deploy-validated-models-on-disconnected/#5-deploy-the-model-in-your-disconnected-cluster","title":"5. Deploy the Model in Your Disconnected Cluster","text":"<p>After mirroring the images, navigate back to the model\u2019s Deployment page on Hugging Face and expand the Red Hat OpenShift AI option.</p> <p>To deploy the model, you need to create the required <code>ServingRuntime</code> and <code>InferenceService</code> objects in your namespace.</p> <p>You can copy the provided YAML files and apply them to your disconnected cluster.</p> <p>\u203c\ufe0f\ud83d\udea8Important: Before applying the YAMLs, make sure to update all image references in both the ServingRuntime and InferenceService to point to your mirrored images. This ensures OpenShift AI can pull the images inside your disconnected environment.</p> <pre><code># Apply the ServingRuntime\noc apply -f vllm-servingruntime.yaml -n &lt;project-name&gt;\n\n# Apply the InferenceService\noc apply -f llama4-inferenceservice.yaml -n &lt;project-name&gt;\n</code></pre> <p>Alternatively, you can apply the YAMLs on OpenShift console by clicking the <code>+</code> sign &gt; <code>Import YAML</code>:</p> <p></p> <p>If you encounter an error like the following, check that your object names comply with DNS naming conventions \u2014 uppercase letters are not allowed.</p> <pre><code>Error \"Invalid value: \"LLama-4-Scout-17B-16E-Instruct-FP8-Dynamic\": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')\" for field \"metadata.name\".\n</code></pre>"},{"location":"odh-rhoai/deploy-validated-models-on-disconnected/#make-sure-names-use-only-lowercase-letters-numbers-and-hyphens-and-follow-kubernetes-dns-label-requirements","title":"&gt; Make sure names use only lowercase letters, numbers, and hyphens, and follow Kubernetes' DNS label requirements.","text":""},{"location":"odh-rhoai/deploy-validated-models-on-disconnected/#summary","title":"Summary","text":"<p>Deploying validated models from Red Hat AI\u2019s Hugging Face Validated Models repository in disconnected OpenShift AI environments involves:</p> <ul> <li>Selecting the desired model.  </li> <li>Identifying the required runtime and model images.  </li> <li>Mirroring these images to your cluster\u2019s internal or private registry.  </li> <li>Updating the deployment instructions to reflect mirrored image references.</li> </ul> <p>This process ensures your AI workloads run seamlessly even in restricted or disconnected environments, enabling you to leverage validated, optimized AI models securely.</p>"},{"location":"odh-rhoai/enable-function-calling/","title":"Enable Function Calling in OpenShift AI","text":"<p>Function calling enhances large language models by enabling them to interact with external tools and APIs in a structured way, extending their capabilities beyond text generation. It equips LLMs with functions (or tools) to perform actions like making API calls, querying databases, executing code, or accessing external knowledge.</p> <p>While LLMs don\u2019t execute these functions directly, they generate the required parameters, which are then executed by the connected application or system.</p>"},{"location":"odh-rhoai/enable-function-calling/#how-function-calling-works","title":"How Function Calling works","text":"<p>When you provide an LLM with a question or task, it analyzes the request and determines the most relevant function to call based on its training, the prompt, and any available context. The LLM doesn\u2019t execute the function itself; instead, it generates a JSON object containing the function name and input arguments. Your application processes this JSON, executes the function, and returns the result to the LLM. This workflow allows the LLM to interact with external systems and perform actions on your behalf.</p> <p></p> <ol> <li>Define Tools: Identify the functions (tools) for the LLM, each with a descriptive name, purpose, and input schema (e.g., JSON Schema).</li> <li>Provide Tools: Pass these tool definitions to your LLM. Prompt the LLM: Ask a question or assign a task. The LLM will determine the most relevant tool to use.</li> <li>Generate Tool Call: The LLM outputs a JSON object specifying the tool and required arguments.</li> <li>Execute Tool: Your application executes the function with the provided arguments and retrieves the result.</li> <li>Return Results: Send the tool\u2019s output back to the LLM for further processing.</li> <li>Generate Final Response: The LLM integrates the tool results into a user-friendly response.</li> </ol>"},{"location":"odh-rhoai/enable-function-calling/#function-calling-in-vllm","title":"Function Calling in vLLM","text":"<p>vLLM supports named function calling and offers flexible options for the <code>tool_choice</code> field in the chat completion API, including <code>auto</code> and <code>none</code>. </p> <p>By leveraging guided decoding, vLLM ensures that responses adhere to the <code>tool</code> parameter objects defined by the JSON schema specified in the <code>tools</code> parameter, maintaining structured and accurate interactions.</p> <p>IMPORTANT: vLLM supports function calling for certain LLMs up to 0.6.3+ versions (specifically for IBM Granite3 family is included in 0.6.4)</p>"},{"location":"odh-rhoai/enable-function-calling/#how-to-enable-function-calling-with-vllm-in-openshift-ai","title":"How to enable Function Calling with vLLM in OpenShift AI","text":"<p>To enable Function Calling within vLLM in OpenShift AI we need to use a vLLM image with a 0.6.3+ version (depending on the model you use, like Granite3 family you should use 0.6.5 onwards).</p> <p>Function Calling will work out of the box with RHOAI 2.17+, which includes the required vLLM versions for LLMs like Granite3. In the meantime, you can use the suggested vLLM image (that includes vLLM v0.6.6).</p> <p>To deploy an LLM with vLLM on OpenShift AI with Function Calling enabled, use a Serving Runtime configured with vLLM images and set the required flags as described in the vLLM documentation:</p> <ul> <li><code>--enable-auto-tool-choice</code>: Mandatory. Enables the model to autonomously generate tool calls when needed.</li> <li><code>--tool-call-parser</code>: Specifies the tool parser to use. Additional parsers can be registered via <code>--tool-parser-plugin</code>.</li> <li><code>--tool-parser-plugin</code>: Optional. Registers custom tool parsers, which can then be selected with <code>--tool-call-parser</code>.</li> <li><code>--chat-template</code>: Optional for auto tool choice. Defines the chat template handling tool-role and assistant-role messages with tool calls. Pre-configured templates exist for Granite3, Hermes, Mistral, and Llama models in their <code>tokenizer_config.json</code> files (like the Granite3)</li> </ul> <p>The Serving Runtime for Granite3.0-8B-Instruct, for example, looks like the following:</p> <pre><code>apiVersion: serving.kserve.io/v1alpha1\nkind: ServingRuntime\nmetadata:\n  annotations:\n    opendatahub.io/recommended-accelerators: '[\"nvidia.com/gpu\"]'\n    openshift.io/display-name: CUSTOM - vLLM ServingRuntime - vLLM 0.6.6 - Tool Calling Parser\n  labels:\n    opendatahub.io/dashboard: \"true\"\n  name: vllm-runtime-tool-calling\nspec:\n  annotations:\n    prometheus.io/path: /metrics\n    prometheus.io/port: \"8080\"\n  containers:\n    - args:\n        - --port=8080\n        - --model=/mnt/models\n        - --served-model-name={{.Name}}\n        - --distributed-executor-backend=mp\n        - --max-model-len\n        - \"6048\"\n        - --dtype=half\n        - --enable-auto-tool-choice\n        - --tool-call-parser\n        - granite\n        - --chat-template\n        - /app/data/template/tool_chat_template_granite.jinja\n      command:\n        - python\n        - -m\n        - vllm.entrypoints.openai.api_server\n      env:\n        - name: HF_HOME\n          value: /tmp/hf_home\n      image: quay.io/opendatahub/vllm@sha256:f7e40286a9e0a5870fcb96f3dc6c2cb094ed8eb8d9a17dc886fc6aae2ad06519\n      name: kserve-container\n      ports:\n        - containerPort: 8080\n          protocol: TCP\n  multiModel: false\n  supportedModelFormats:\n    - autoSelect: true\n      name: vLLM\n</code></pre> <p>NOTE: For Granite3.1-8B-Instruct as the vLLM Function Calling documentation refers use only the flags:</p> <pre><code>        - --enable-auto-tool-choice\n        - --tool-call-parser\n        - granite\n</code></pre> <p>The vLLM images used include different templates for several models, such as Llama3, Hermes, and Mistral, that can be used to enable function calling for these models. Check the upstream vLLM Function Calling documentation to know more.</p>"},{"location":"odh-rhoai/enable-function-calling/#using-function-calling-with-langchain","title":"Using Function Calling with LangChain","text":"<p>If you're interested in using Function Calling in your apps or Jupyter Notebooks, check out this notebook example of Function Calling with LangChain. </p> <ul> <li>Function Calling with LangChain example</li> </ul> <p>In this example, we demonstrate how to enable Granite3 (with Function Calling) to search the Internet using DuckDuckGo as a tool.</p>"},{"location":"odh-rhoai/enable-function-calling/#bonus-deploy-function-calling-as-regular-rhoai-user-non-admin","title":"Bonus: Deploy Function Calling as regular RHOAI user (non-admin)","text":"<p>If you want to deploy Function Calling using vLLM in RHOAI but are not an RHOAI admin, you still have an option to do so!</p> <p>From Red Hat OpenShift AI 2.16 onwards, you can configure the additional Serving Runtime arguments during model deployment.</p> <p>NOTE: The only requirement will be having a 0.6.3+ (or 0.6.4+ if you're willing to use Granite3) in vLLM image of the Serving Runtime used, no additional parameters used:</p> <pre><code>apiVersion: serving.kserve.io/v1alpha1\nkind: ServingRuntime\nmetadata:\n  annotations:\n    opendatahub.io/recommended-accelerators: '[\"nvidia.com/gpu\"]'\n    openshift.io/display-name: CUSTOM - vLLM ServingRuntime for KServe Tweaked\n      Float16 - v0.6.4 - Tool Calling Parser Generic\n  labels:\n    opendatahub.io/dashboard: \"true\"\n  name: vllm-runtime-tool-calling-generic\nspec:\n  annotations:\n    prometheus.io/path: /metrics\n    prometheus.io/port: \"8080\"\n  containers:\n    - args:\n        - --port=8080\n        - --model=/mnt/models\n        - --served-model-name={{.Name}}\n        - --distributed-executor-backend=mp\n        - --max-model-len\n        - \"6048\"\n        - --dtype=half\n      command:\n        - python\n        - -m\n        - vllm.entrypoints.openai.api_server\n      env:\n        - name: HF_HOME\n          value: /tmp/hf_home\n      image: quay.io/opendatahub/vllm@sha256:f7e40286a9e0a5870fcb96f3dc6c2cb094ed8eb8d9a17dc886fc6aae2ad06519\n      name: kserve-container\n      ports:\n        - containerPort: 8080\n          protocol: TCP\n  multiModel: false\n  supportedModelFormats:\n    - autoSelect: true\n      name: vLLM\n</code></pre> <p>When you deploy your Model Serving (Granite3.0-8B-Instruct in this case) as a regular user in RHOAI Dashboard, in the Additional serving runtime arguments define the following:</p> <pre><code>--enable-auto-tool-choice\n--tool-call-parser=granite\n--chat-template=/app/data/template/tool_chat_template_granite.jinja\n</code></pre> <p></p> <p>NOTE: For Granite3.1-8B-Instruct only the flags <code>--enable-auto-tool-choice</code> and <code>--tool-call-parser=granite</code> are required, remove the --chat-template flag as is not required.</p> <p>This setup creates a \"generic\" Serving Runtime for Function Calling, allowing you to customize additional Serving Runtime arguments depending on the model used.</p> <p>The vLLM image includes the following templates (vLLM image with vLLM 0.6.6):</p> <pre><code>ls /app/data/template/\ntemplate_alpaca.jinja        template_vlm2vec.jinja\ntemplate_baichuan.jinja      tool_chat_template_granite.jinja\ntemplate_blip2.jinja         tool_chat_template_granite_20b_fc.jinja\ntemplate_chatglm.jinja       tool_chat_template_hermes.jinja\ntemplate_chatglm2.jinja      tool_chat_template_internlm2_tool.jinja\ntemplate_chatml.jinja        tool_chat_template_llama3.1_json.jinja\ntemplate_dse_qwen2_vl.jinja  tool_chat_template_llama3.2_json.jinja\ntemplate_falcon.jinja        tool_chat_template_llama3.2_pythonic.jinja\ntemplate_falcon_180b.jinja   tool_chat_template_mistral.jinja\ntemplate_inkbot.jinja        tool_chat_template_mistral_parallel.jinja\ntemplate_llava.jinja         tool_chat_template_toolace.jinja\n</code></pre> <p>For additional details about template models and using Function Calling, check the vLLM Function Calling documentation</p> <p>Happy function calling!</p>"},{"location":"odh-rhoai/gitops/","title":"Managing RHOAI with GitOps","text":"<p>GitOps is a common way to manage and deploy applications and resources on Kubernetes clusters.</p> <p>This page is intended to provide an overview of the different objects involved in managing both the installation, administration, and usage of OpenShift AI components using GitOps.  This page is by no means intended to be an exhaustive tutorial on each object and all of the features available in them.</p> <p>When first implementing features with GitOps it is highly recommended to deploy the resources manually using the Dashboard, then extract the resources created by the Dashboard and duplicate them in your GitOps repo.</p>"},{"location":"odh-rhoai/gitops/#installation","title":"Installation","text":""},{"location":"odh-rhoai/gitops/#operator-installation","title":"Operator Installation","text":"<p>The Red Hat OpenShift AI operator is installed and managed by OpenShift's Operator Lifecycle Manager (OLM) and follows common patterns that can be used to install many different operators.</p> <p>The Red Hat OpenShift AI operator should be installed in the <code>redhat-ods-operator namespace</code> by default:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  annotations:\n    openshift.io/display-name: \"Red Hat OpenShift AI\"\n  name: redhat-ods-operator\n</code></pre> <p>After creating the namespace, OLM requires you create an Operator Group to help manage any operators installed in that namespace:</p> <pre><code>apiVersion: operators.coreos.com/v1\nkind: OperatorGroup\nmetadata:\n  name: redhat-ods-operator-group\n  namespace: redhat-ods-operator\n</code></pre> <p>Finally, a Subscription should be created to install the operator:</p> <pre><code>apiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: rhods-operator\n  namespace: redhat-ods-operator\nspec:\n  channel: stable # &lt;1&gt;\n  installPlanApproval: Automatic # &lt;2&gt;\n  name: rhods-operator\n  source: redhat-operators\n  sourceNamespace: openshift-marketplace\n</code></pre> <p>Subscription Options:</p> <ol> <li>Operator versions are managed with the <code>channel</code> in OLM.  Users are able to select a channel that corresponds to the upgrade lifecycle they wish to follow and OLM will update versions as they are released on that channel.  To learn more about the available channels and the release lifecycle, please refer to the official lifecycle documentation</li> <li>Platform administrators also have an option to set how upgrades are applied for the operator with the <code>installPlanApproval</code>.  If set to <code>Automatic</code> RHOAI is automatically updated to the latest version that is available on the selected channel.  If set to <code>Manual</code> administrators will be required to approve all upgrades.</li> </ol>"},{"location":"odh-rhoai/gitops/#component-configuration","title":"Component Configuration","text":"<p>When the operator is installed it automatically creates a <code>DSCInitialization</code> object that sets up several default configurations.  While it is not required, administrators can choose to manage the <code>DSCinitialization</code> object via GitOps. </p> <pre><code>apiVersion: dscinitialization.opendatahub.io/v1\nkind: DSCInitialization\nmetadata:\n  name: default-dsci\nspec:\n  applicationsNamespace: redhat-ods-applications\n  serviceMesh:\n    controlPlane:\n      metricsCollection: Istio\n      name: data-science-smcp\n      namespace: istio-system\n    managementState: Managed # &lt;1&gt;\n  trustedCABundle:\n    customCABundle: ''\n    managementState: Managed # &lt;2&gt;\n</code></pre> <p>DSCInitialization Options:</p> <ol> <li>KServe Serverless mode requires a ServiceMesh instance to be installed on the cluster.  By default the Red Hat OpenShift AI operator will attempt to configure an instance if the ServiceMesh and Authorino operators are installed. If your cluster already has ServiceMesh configured, you may choose to set <code>Unmanaged</code>.</li> <li>User can provide customized certification to be used by components across Red Hat OpenShift AI.</li> </ol> <p>After the operator is installed, a DataScienceCluster object will need to be configured with the different components.  Each component has a <code>managementState</code> option which can be set to <code>Managed</code> or <code>Removed</code>.  Admins can choose which components are installed on the cluster.</p> <pre><code>kind: DataScienceCluster\napiVersion: datasciencecluster.opendatahub.io/v1\nmetadata:\n  name: default-dsc\nspec:\n  components:\n    codeflare:\n      managementState: Managed\n    kserve:\n      managementState: Managed\n      serving:\n        ingressGateway:\n          certificate:\n            type: OpenshiftDefaultIngress\n        managementState: Managed\n        name: knative-serving\n    trustyai:\n      managementState: Managed\n    ray:\n      managementState: Managed\n    kueue:\n      managementState: Managed\n    workbenches:\n      managementState: Managed\n    dashboard:\n      managementState: Managed\n    modelmeshserving:\n      managementState: Managed\n    datasciencepipelines:\n      managementState: Managed\n    trainingoperator:\n      managementState: Removed\n    modelregistry:\n      managementState: Removed\n      registriesNamespace: rhoai-model-registries\n</code></pre> <p>After the <code>DataScienceCluster</code> object is created, the operator will install and configure the different components on the cluster.  Only one <code>DataScienceCluster</code> object can be created on a cluster.</p>"},{"location":"odh-rhoai/gitops/#administration","title":"Administration","text":""},{"location":"odh-rhoai/gitops/#dashboard-configs","title":"Dashboard Configs","text":"<p>The Red Hat OpenShift AI Dashboard has many different configurable options through the UI that can be managed using the <code>OdhDashboardConfig</code> config object.  A default <code>OdhDashboardConfig</code> is created when the Dashboard component is installed</p> <pre><code>apiVersion: opendatahub.io/v1alpha\nkind: OdhDashboardConfig\nmetadata:\n  name: odh-dashboard-config\n  namespace: redhat-ods-applications\n  labels:\n    app.kubernetes.io/part-of: rhods-dashboard\n    app.opendatahub.io/rhods-dashboard: 'true'\nspec:\n  dashboardConfig:\n    enablement: true\n    disableAcceleratorProfiles: false\n    disableBYONImageStream: false\n    disableClusterManager: false\n    disableCustomServingRuntimes: false\n    disableDistributedWorkloads: false\n    disableHome: false\n    disableISVBadges: false\n    disableInfo: false\n    disableKServe: false\n    disableKServeAuth: false\n    disableKServeMetrics: false\n    disableModelMesh: false\n    disableModelRegistry: false\n    disableModelRegistrySecureDB: false\n    disableModelServing: false\n    disableNIMModelServing: true\n    disablePerformanceMetrics: false\n    disablePipelines: false\n    disableProjectSharing: false\n    disableProjects: false\n    disableServingRuntimeParams: false\n    disableStorageClasses: false\n    disableSupport: false\n    disableTracking: false\n    disableTrustyBiasMetrics: false\n  groupsConfig:\n    adminGroups: rhods-admins # &lt;1&gt;\n    allowedGroups: 'system:authenticated' # &lt;2&gt;\n  modelServerSizes: # &lt;3&gt;\n    - name: Small\n      resources:\n        limits:\n          cpu: '2'\n          memory: 8Gi\n        requests:\n          cpu: '1'\n          memory: 4Gi\n    - name: Medium\n      resources:\n        limits:\n          cpu: '8'\n          memory: 10Gi\n        requests:\n          cpu: '4'\n          memory: 8Gi\n    - name: Large\n      resources:\n        limits:\n          cpu: '10'\n          memory: 20Gi\n        requests:\n          cpu: '6'\n          memory: 16Gi\n  notebookController:\n    enabled: true\n    notebookNamespace: rhods-notebooks\n    pvcSize: 20Gi # &lt;4&gt;\n  notebookSizes: # &lt;5&gt;\n    - name: Small\n      resources:\n        limits:\n          cpu: '2'\n          memory: 8Gi\n        requests:\n          cpu: '1'\n          memory: 8Gi\n    - name: Medium\n      resources:\n        limits:\n          cpu: '6'\n          memory: 24Gi\n        requests:\n          cpu: '3'\n          memory: 24Gi\n    - name: Large\n      resources:\n        limits:\n          cpu: '14'\n          memory: 56Gi\n        requests:\n          cpu: '7'\n          memory: 56Gi\n    - name: X Large\n      resources:\n        limits:\n          cpu: '30'\n          memory: 120Gi\n        requests:\n          cpu: '15'\n          memory: 120Gi\n  templateDisablement: []\n  templateOrder:\n    - caikit-tgis-runtime\n    - kserve-ovms\n    - ovms\n    - tgis-grpc-runtime\n    - vllm-runtime\n</code></pre> <p>OdhDashboardConfig Options:</p> <ol> <li>The Dashboard uses a group called <code>rhods-admins</code> by default which users can be added to be granted admin privileges through the Dashboard.  Additionally, any user with the cluster-admin role are admins in the Dashboard by default.  If you wish to change the group which is used to manage admin access, this option can be updated via Auth CR  <code>auth</code>.  It is important to note that this field only impacts a users ability to modify settings in the Dashboard, and will have no impact to a users ability to modify configurations through the Kubernetes objects such as this OdhDashboardConfig object.</li> <li>By default any user that has access to the OpenShift cluster where Red Hat OpenShift AI is installed will have the ability to access the Dashboard.  If you wish to restrict who has access to the Dashboard this option can be updated to another group.  Like the admin group option, this option only impacts the users ability to access the Dashboard and does not restrict their ability to interact directly with the Kubernetes objects used to deploy AI resources.</li> <li>When a user creates a new Model Server through the Dashboard they are presented with an option to choose a server size which will impact the resources available to the pod created for the Model Server.  Administrators have the ability to configure the default options that are available to their users.</li> <li>When creating a new Workbench, users are asked to create storage for their Workbench.  The storage will default to the value set here and users will have the option to choose a different amount of storage if their use case requires more or less storage.  Admins can choose another default storage size that is presented to users by configuring this option.</li> <li>Like the Model Server size, users are presented with a drop down menu of options to select what size of Workbench they wish to create.  Admins have the ability to customize the size options that are presented to users.</li> </ol>"},{"location":"odh-rhoai/gitops/#idle-notebook-culling","title":"Idle Notebook Culling","text":"<p>Admins have the ability to enable Idle Notebook Culling which will automatically stop any Notebooks/Workbenches that users haven't interacted with in a period of time by creating the following ConfigMap:</p> <pre><code>kind: ConfigMap\napiVersion: v1\nmetadata:\n  name: notebook-controller-culler-config\n  namespace: redhat-ods-applications\n  labels:\n    opendatahub.io/dashboard: 'true'\ndata:\n  CULL_IDLE_TIME: '240' # &lt;1&gt;\n  ENABLE_CULLING: 'true'\n  IDLENESS_CHECK_PERIOD: '1'\n</code></pre> <p>Idle Notebook Culling Options:</p> <ol> <li>The <code>CULL_IDLE_TIME</code> looks for metrics from Jupyter to understand when the last time a user interacted with the Workbench and will shut the pod down if it has passed the time set here.  The time is the number of minutes so 240 minutes or 4 hours.</li> </ol>"},{"location":"odh-rhoai/gitops/#accelerator-profiles","title":"Accelerator Profiles","text":"<p>Accelerator Profiles allow admins to configure different types of GPU options that they can present to end users and automatically configure a toleration on Workbenches or Model Servers when they are selected.  Admins can configure an Accelerator Profile with the <code>AcceleratorProfile</code> object:</p> <pre><code>apiVersion: dashboard.opendatahub.io/v1\nkind: AcceleratorProfile\nmetadata:\n  name: nvidia-gpu\n  namespace: redhat-ods-applications\nspec:\n  displayName: nvidia-gpu\n  enabled: true\n  identifier: nvidia.com/gpu\n  tolerations:\n    - effect: NoSchedule\n      key: nvidia-gpu-only\n      operator: Exists\n      value: ''\n</code></pre>"},{"location":"odh-rhoai/gitops/#notebook-images","title":"Notebook Images","text":"<p>Red Hat OpenShift AI ships with several out of the box Notebook/Workbench Images but admins can create additional custom images that users can use to launch new Workbench instances.  A Notebook Image is managed with an OpenShift ImageStream object with some required labels:</p> <pre><code>kind: ImageStream\napiVersion: image.openshift.io/v1\nmetadata:\n  annotations:\n    opendatahub.io/notebook-image-desc: A custom Jupyter Notebook built for my organization # &lt;1&gt;\n    opendatahub.io/notebook-image-name: My Custom Notebook # &lt;2&gt;\n  name: my-custom-notebook\n  namespace: redhat-ods-applications\n  labels: # &lt;3&gt;\n    app.kubernetes.io/created-by: byon\n    opendatahub.io/dashboard: 'true'\n    opendatahub.io/notebook-image: 'true'\nspec:\n  lookupPolicy:\n    local: true\n  tags:\n    - name: '1.0' # &lt;4&gt;\n      annotations:\n        opendatahub.io/notebook-python-dependencies: '[{\"name\":\"PyTorch\",\"version\":\"2.2\"}]' # &lt;5&gt;\n        opendatahub.io/notebook-software: '[{\"name\":\"Python\",\"version\":\"v3.11\"}]' # &lt;6&gt;\n        opendatahub.io/workbench-image-recommended: 'true' # &lt;7&gt;\n      from:\n        kind: DockerImage\n        name: 'quay.io/my-org/my-notebook:latest' # &lt;8&gt;\n      importPolicy:\n        importMode: Legacy\n      referencePolicy:\n        type: Source\n</code></pre> <p>Notebook Image Options:</p> <ol> <li>A description for the purpose of the notebook image</li> <li>The name that will be displayed to end users in the drop down menu when creating a Workbench</li> <li>The notebook image requires several labels for them to appear in the Dashboard, including the <code>app.kubernetes.io/created-by: byon</code> label.  While traditionally this label is utilized to trace where an object originated from, this label is required for the notebooks to be made available to end users. Ensure <code>disableBYONImageStream</code> is set to false with this label</li> <li>Multiple image versions can be configured as part of the same Notebook and users have the ability to select which version of the image they wish to use.  This is helpful if you release updated versions of the Notebook image and you wish to avoid breaking end user environments with package changes and allow them to upgrade as they wish.</li> <li>When selecting a Notebook image users will be presented with some information about the notebook based on the information presented in this annotation.  <code>opendatahub.io/notebook-python-dependencies</code> is most commonly used to present information about versions from the most important Python packages that are pre-installed in the Image.</li> <li>Like the python dependencies annotation, the <code>opendatahub.io/notebook-software</code> annotation is used to present the end user with information about what software is installed in the Image.  Most commonly this field is used to present information such as the Python version, Jupyter versions, or CUDA versions.</li> <li>When multiple tags are created on the ImageStream, the <code>opendatahub.io/workbench-image-recommended</code> is used to control what version of the image is presented by default to end users.  Only one tag should be set to <code>true</code> at any give time.</li> <li>Notebook images are generally recommended to be stored in an Image Registry outside of the cluster and referenced in the ImageStream.</li> </ol> <p>While it is possible to build a Notebook Image on an OpenShift cluster and publish it directly to an ImageStream using a BuildConfig or a Tekton Pipeline, it can be challenging to get that image to be seen by the Red Hat OpenShift AI Dashboard.  The Dashboard is only looks at images listed in the <code>spec.tags</code> section and images pushed directly to the internal image registry are recorded in the <code>status.tags</code>.  As a work around, it is possible to \"link\" a tag pushed directly to the internal image registry to a tag that is visible by the Dashboard:</p> <pre><code>kind: ImageStream\napiVersion: image.openshift.io/v1\nmetadata:\n  annotations:\n    opendatahub.io/notebook-image-desc: A custom Jupyter Notebook built for my organization\n    opendatahub.io/notebook-image-name: My Custom Notebook\n  name: my-custom-notebook\n  namespace: redhat-ods-applications\n  labels:\n    app.kubernetes.io/created-by: byon\n    opendatahub.io/dashboard: 'true'\n    opendatahub.io/notebook-image: 'true'\nspec:\n  lookupPolicy:\n    local: false\n  tags:\n    - name: '1.0'\n      annotations:\n        opendatahub.io/notebook-python-dependencies: '[{\"name\":\"PyTorch\",\"version\":\"2.2\"}]'\n        opendatahub.io/notebook-software: '[{\"name\":\"Python\",\"version\":\"v3.11\"}]'\n        opendatahub.io/workbench-image-recommended: 'true'\n      from:\n        kind: DockerImage\n        name: 'image-registry.openshift-image-registry.svc:5000/redhat-ods-applications/my-custom-workbench:latest'\n      importPolicy:\n        importMode: Legacy\n      referencePolicy:\n        type: Source\nstatus:\n  dockerImageRepository: 'image-registry.openshift-image-registry.svc:5000/redhat-ods-applications/dsp-example'\n  tags:\n    - tag: latest\n</code></pre>"},{"location":"odh-rhoai/gitops/#serving-runtime-templates","title":"Serving Runtime Templates","text":"<p>Red Hat OpenShift AI ships with several out of the box Serving Runtime Templates such as OpenVino and vLLM, but admins have the ability to configure additional templates that allow users to deploy additional ServingRuntimes.  A Serving Runtime template is an OpenShift Template object that wraps around a ServingRuntime object:</p> <pre><code>kind: Template\napiVersion: template.openshift.io/v1\nmetadata:\n  name: trition-serving-runtime\n  namespace: redhat-ods-applications\n  labels:\n    opendatahub.io/dashboard: 'true'\n  annotations:\n    opendatahub.io/apiProtocol: REST\n    opendatahub.io/modelServingSupport: '[\"multi\"]'\nobjects:\n  - apiVersion: serving.kserve.io/v1alpha1\n    kind: ServingRuntime\n    metadata:\n      name: triton-23.05\n      labels:\n        name: triton-23.05\n      annotations:\n        maxLoadingConcurrency: '2'\n        openshift.io/display-name: Triton runtime 23.05\n    spec:\n      supportedModelFormats:\n        - name: keras\n          version: '2'\n          autoSelect: true\n        - name: onnx\n          version: '1'\n          autoSelect: true\n        - name: pytorch\n          version: '1'\n          autoSelect: true\n        - name: tensorflow\n          version: '1'\n          autoSelect: true\n        - name: tensorflow\n          version: '2'\n          autoSelect: true\n        - name: tensorrt\n          version: '7'\n          autoSelect: true\n        - name: sklearn\n          version: \"1\"\n          autoSelect: true         \n      protocolVersions:\n        - grpc-v2\n      multiModel: true\n      grpcEndpoint: 'port:8085'\n      grpcDataEndpoint: 'port:8001'\n      volumes:\n        - name: shm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 2Gi\n      containers:\n        - name: triton\n          image: 'nvcr.io/nvidia/tritonserver:23.05-py3'\n          command:\n            - /bin/sh\n          args:\n            - '-c'\n            - 'mkdir -p /models/_triton_models; chmod 777 /models/_triton_models; exec tritonserver \"--model-repository=/models/_triton_models\" \"--model-control-mode=explicit\" \"--strict-model-config=false\" \"--strict-readiness=false\" \"--allow-http=true\" \"--allow-sagemaker=false\" '\n          volumeMounts:\n            - name: shm\n              mountPath: /dev/shm\n          resources:\n            requests:\n              cpu: 500m\n              memory: 1Gi\n            limits:\n              cpu: '5'\n              memory: 1Gi\n          livenessProbe:\n            exec:\n              command:\n                - curl\n                - '--fail'\n                - '--silent'\n                - '--show-error'\n                - '--max-time'\n                - '9'\n                - 'http://localhost:8000/v2/health/live'\n            initialDelaySeconds: 5\n            periodSeconds: 30\n            timeoutSeconds: 10\n      builtInAdapter:\n        serverType: triton\n        runtimeManagementPort: 8001\n        memBufferBytes: 134217728\n        modelLoadingTimeoutMillis: 90000\n</code></pre>"},{"location":"odh-rhoai/gitops/#end-user-resources","title":"End User Resources","text":""},{"location":"odh-rhoai/gitops/#data-science-projects","title":"Data Science Projects","text":"<p>Data Science Projects are simply a normal OpenShift Project with an extra label to distinguish them from normal OpenShift projects by the Red Hat OpenShift AI Dashboard.  Like OpenShift Projects it is recommended to create a <code>namespace</code> object and allow OpenShift to create the corresponding <code>project</code> object:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-data-science-project\n  labels:\n    opendatahub.io/dashboard: \"true\"\n</code></pre> <p>Additionally, when a project going to be utilized by ModelMesh for Multi-model serving, there is an additional ModelMesh label that should be applied to the namespace:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-multi-model-serving-project\n  labels:\n    opendatahub.io/dashboard: \"true\"\n    modelmesh-enabled: \"true\"\n</code></pre>"},{"location":"odh-rhoai/gitops/#workbenches","title":"Workbenches","text":"<p>Workbench objects are managed using the Notebook custom resource.  The Notebook object contains a fairly complex configuration, with many items that will be autogenerated, and required annotations to display correctly in the Dashboard.  The Notebook object essentially acts as a wrapper around a normal pod definition and you will find many similarities to managing a pod with options such as the images, pvcs, secrets, etc.  </p> <p>It is highly recommended to thoroughly test any Notebook configurations configured with GitOps.</p> <pre><code>apiVersion: kubeflow.org/v1\nkind: Notebook\nmetadata:\n  annotations:\n    notebooks.opendatahub.io/inject-oauth: 'true' # &lt;1&gt;\n    opendatahub.io/image-display-name: Minimal Python\n    notebooks.opendatahub.io/oauth-logout-url: 'https://rhods-dashboard-redhat-ods-applications.apps.my-cluster.com/projects/my-data-science-project?notebookLogout=my-workbench'\n    opendatahub.io/accelerator-name: ''\n    openshift.io/description: ''\n    openshift.io/display-name: my-workbench\n    notebooks.opendatahub.io/last-image-selection: 's2i-minimal-notebook:2024.1'\n    notebooks.opendatahub.io/last-size-selection: Small\n    opendatahub.io/username: 'kube:admin'\n  name: my-workbench\n  namespace: my-data-science-project\nspec:\n  template:\n    spec:\n      affinity: {}\n      containers:\n        - resources: # &lt;2&gt;\n            limits:\n              cpu: '2'\n              memory: 8Gi\n            requests:\n              cpu: '1'\n              memory: 8Gi\n          readinessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: /notebook/my-data-science-project/my-workbench/api\n              port: notebook-port\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 1\n          name: my-workbench\n          livenessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: /notebook/my-data-science-project/my-workbench/api\n              port: notebook-port\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 1\n          env:\n            - name: NOTEBOOK_ARGS\n              value: |-\n                --ServerApp.port=8888\n                --ServerApp.token=''\n                --ServerApp.password=''\n                --ServerApp.base_url=/notebook/my-data-science-project/my-workbench\n                --ServerApp.quit_button=False\n                --ServerApp.tornado_settings={\"user\":\"kube-3aadmin\",\"hub_host\":\"https://rhods-dashboard-redhat-ods-applications.apps.my-cluster.com\", \"hub_prefix\":\"/projects/my-data-science-project\"}\n            - name: JUPYTER_IMAGE\n              value: 'image-registry.openshift-image-registry.svc:5000/redhat-ods-applications/s2i-minimal-notebook:2024.1'\n            - name: PIP_CERT\n              value: /etc/pki/tls/custom-certs/ca-bundle.crt\n            - name: REQUESTS_CA_BUNDLE\n              value: /etc/pki/tls/custom-certs/ca-bundle.crt\n            - name: SSL_CERT_FILE\n              value: /etc/pki/tls/custom-certs/ca-bundle.crt\n            - name: PIPELINES_SSL_SA_CERTS\n              value: /etc/pki/tls/custom-certs/ca-bundle.crt\n            - name: GIT_SSL_CAINFO\n              value: /etc/pki/tls/custom-certs/ca-bundle.crt\n          ports:\n            - containerPort: 8888\n              name: notebook-port\n              protocol: TCP\n          imagePullPolicy: Always\n          volumeMounts:\n            - mountPath: /opt/app-root/src\n              name: my-workbench\n            - mountPath: /dev/shm\n              name: shm\n            - mountPath: /etc/pki/tls/custom-certs/ca-bundle.crt\n              name: trusted-ca\n              readOnly: true\n              subPath: ca-bundle.crt\n          image: 'image-registry.openshift-image-registry.svc:5000/redhat-ods-applications/s2i-minimal-notebook:2024.1' # &lt;3&gt;\n          workingDir: /opt/app-root/src\n        - resources: # &lt;4&gt;\n            limits:\n              cpu: 100m\n              memory: 64Mi\n            requests:\n              cpu: 100m\n              memory: 64Mi\n          readinessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: /oauth/healthz\n              port: oauth-proxy\n              scheme: HTTPS\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 1\n          name: oauth-proxy\n          livenessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: /oauth/healthz\n              port: oauth-proxy\n              scheme: HTTPS\n            initialDelaySeconds: 30\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 1\n          env:\n            - name: NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          ports:\n            - containerPort: 8443\n              name: oauth-proxy\n              protocol: TCP\n          imagePullPolicy: Always\n          volumeMounts:\n            - mountPath: /etc/oauth/config\n              name: oauth-config\n            - mountPath: /etc/tls/private\n              name: tls-certificates\n          image: 'registry.redhat.io/openshift4/ose-oauth-proxy@sha256:4f8d66597feeb32bb18699326029f9a71a5aca4a57679d636b876377c2e95695'\n          args:\n            - '--provider=openshift'\n            - '--https-address=:8443'\n            - '--http-address='\n            - '--openshift-service-account=my-workbench'\n            - '--cookie-secret-file=/etc/oauth/config/cookie_secret'\n            - '--cookie-expire=24h0m0s'\n            - '--tls-cert=/etc/tls/private/tls.crt'\n            - '--tls-key=/etc/tls/private/tls.key'\n            - '--upstream=http://localhost:8888'\n            - '--upstream-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt'\n            - '--email-domain=*'\n            - '--skip-provider-button'\n            - '--openshift-sar={\"verb\":\"get\",\"resource\":\"notebooks\",\"resourceAPIGroup\":\"kubeflow.org\",\"resourceName\":\"my-workbench\",\"namespace\":\"$(NAMESPACE)\"}'\n            - '--logout-url=https://rhods-dashboard-redhat-ods-applications.apps.my-cluster.com/projects/my-data-science-project?notebookLogout=my-workbench'\n      enableServiceLinks: false\n      serviceAccountName: my-workbench\n      volumes:\n        - name: my-workbench\n          persistentVolumeClaim:\n            claimName: my-workbench\n        - emptyDir:\n            medium: Memory\n          name: shm\n        - configMap:\n            items:\n              - key: ca-bundle.crt\n                path: ca-bundle.crt\n            name: workbench-trusted-ca-bundle\n            optional: true\n          name: trusted-ca\n        - name: oauth-config\n          secret:\n            defaultMode: 420\n            secretName: my-workbench-oauth-config\n        - name: tls-certificates\n          secret:\n            defaultMode: 420\n            secretName: my-workbench-tls\n</code></pre> <ol> <li>The Notebook object contains several different annotations that are used by OpenShift AI, but the <code>inject-oauth</code> annotation is one of the most important.  There are several oauth based configurations that in the Notebook that will be automatically generated by this annotation, allowing you to exclude a large amount of notebook configuration from what is contained in your GitOps repo.</li> <li>While selecting the resource size through the Dashboard you have more limited options for what sizes you can select, you can choose any size you wish for your notebook through the YAML.  By selecting a non-standard size the Dashboard may report an \"unknown\" size however.</li> <li>Just like the resources size, you can choose any number of images for the Notebook, including ones that are not available in the Dashboard.  By selecting a non-standard notebook image the Dashboard may report issues however.</li> <li>The oauth-proxy container is one such item that can be removed from the GitOps based configuration when utilizing the <code>inject-oauth</code> annotation.  Instead of including this section and some other oauth related configurations, you can simply rely on the annotation, and allow the Notebook controller to manage this portion of the object for you.  This will help to prevent problems when upgrading RHOAI.</li> </ol> <p>Users have the ability to start and stop the Workbench to help conserve resources on the cluster.  To stop a Notebook, the following annotation should be applied to the Notebook object:</p> <pre><code>metadata:\n  annotations:\n    kubeflow-resource-stopped: '2024-07-30T20:52:37Z'\n</code></pre> <p>Generally, you do not want to include this annotation in your GitOps configuration, as it will enforce the Notebook to be shutdown, not allowing users to start their Notebooks.  The value of the annotation doesn't matter, but by default the Dashboard will apply a timestamp with the time the Notebook was shut down.</p>"},{"location":"odh-rhoai/gitops/#data-science-connections","title":"Data Science Connections","text":"<p>A Data Science Connection is a normal Kubernetes Secret object with several annotations that follow a specific format for the data.</p> <pre><code>kind: Secret\napiVersion: v1\ntype: Opaque\nmetadata:\n  name: dataconnectionminio # &lt;1&gt;\n  namespace: my-data-science-project\n  labels:\n    opendatahub.io/dashboard: 'true' # &lt;2&gt;\n    opendatahub.io/managed: 'true'\n  annotations:\n    opendatahub.io/connection-type-ref: s3 # &lt;3&gt;\n    openshift.io/display-name: my-dataconnection # &lt;4&gt;\ndata: # &lt;5&gt;\n  AWS_ACCESS_KEY_ID: bWluaW8=\n  AWS_DEFAULT_REGION: 'dXMtZWFzdC0x'\n  AWS_S3_BUCKET: 'bXlidWNrZXQ='\n  AWS_S3_ENDPOINT: dGVzdA==\n  AWS_SECRET_ACCESS_KEY: bWluaW8xMjM=\n</code></pre> <ol> <li>When creating a data connection through the Dashboard, the name is automatically generated as <code>dataconnection&lt;your-entered-name&gt;</code>.  When generating the data connection from outside of the Dashboard, you do not need to follow this naming convention.</li> <li>The <code>opendatahub.io/dashboard: 'true'</code> label is used to help determine what secrets to display in the Dashboard.  This option must be set to true if you wish for it to be available in the UI.</li> <li>With the latest release, the Dashboard has S3, uri as supported connection-type, other types e.g oci may be added in the future releases.</li> <li>The name of the data connection as it will appear in the Dashboard UI</li> <li>Like all secrets, data connections data is stored in a base64 encoding.  This data is not secure to be stored in this format and users should instead look into tools such as SealedSecrets or ExternalSecrets to manage secret data in a GitOps workflow.</li> </ol>"},{"location":"odh-rhoai/gitops/#data-science-pipelines","title":"Data Science Pipelines","text":"<p>When setting up a new project, a Data Science Pipeline instance needs to be created using the DataSciencePipelineApplication(DSPA) object.  The DSPA will create the pipeline servers for the project and allow users to begin interacting with Data Science Pipelines.</p> <pre><code>apiVersion: datasciencepipelinesapplications.opendatahub.io/v1\nkind: DataSciencePipelinesApplication\nmetadata:\n  name: dspa # &lt;1&gt;\n  namespace: my-data-science-project\nspec:\n  apiServer:\n    caBundleFileMountPath: ''\n    caBundleFileName: ''\n    deploy: true\n    enableSamplePipeline: false\n    enableOauth: true\n  database:\n    disableHealthCheck: false\n    mariaDB:\n      deploy: true\n      pipelineDBName: mlpipeline\n      pvcSize: 10Gi\n      username: mlpipeline\n  dspVersion: v2\n  objectStorage:\n    disableHealthCheck: false\n    enableExternalRoute: false\n    externalStorage: # &lt;2&gt;\n      basePath: ''\n      bucket: pipelines\n      host: 'minio.ai-example-training.svc.cluster.local:9000'\n      port: ''\n      region: us-east-1\n      s3CredentialsSecret:\n        accessKey: AWS_SECRET_ACCESS_KEY\n        secretKey: AWS_ACCESS_KEY_ID\n        secretName: dataconnectionminio\n      scheme: http\n  persistenceAgent:\n    deploy: true\n    numWorkers: 2\n  podToPodTLS: true\n  scheduledWorkflow:\n    cronScheduleTimezone: UTC\n    deploy: true\n</code></pre> <ol> <li>The Dashboard expects to look for an object called <code>dspa</code> and it is not recommended to deploy more than one single DataSciencePipelineApplication object in a single namespace.</li> <li>The externalStorage is a critical configuration for setting up S3 backend storage for Data Science Pipelines.  While using the dashboard you are required to configure the connection details.  While you can import these details from a data connection, it will create a separate secret containing the s3 secrets instead of reusing the existing data connection secret.</li> </ol> <p>Once a Data Science Pipeline instance has been created, users may wish to configure and manage their pipelines via GitOps.  It is important to note that Data Science Pipelines is not \"gitops friendly\".  While working with Elyra or a kfp pipeline, users are required to manually upload a pipeline file to the Dashboard which does not generate a corresponding Kubernetes object.  Additionally, when executing a pipeline run, uses may find a ArgoWorkflow object that is generated for the run, however this object can not be re-used in a GitOps application to create a new pipeline run in Data Science Pipelines.</p> <p>As a work around, one common pattern to \"gitops-ify\" a Data Science Pipeline while using kfp is to instead create a Tekton pipeline that either compiles the pipeline, and uses the kfp skd to upload the pipeline to Data Science Pipelines, or the kfp sdk can automatically trigger a new pipeline run directly from your pipeline code.</p>"},{"location":"odh-rhoai/gitops/#model-serving","title":"Model Serving","text":"<p>Model Serving in RHOAI has two different flavors, Single Model Serving (KServe) and Multi-Model Serving (ModelMesh).  Both model server options utilize the same Kubernetes objects (ServingRuntime and InferenceService), but have different controllers managing them.  </p> <p>As mentioned in the Data Science Project section, in order to utilize ModelMesh, a <code>modelmesh-enabled</code> label must be applied to the namespace:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-multi-model-serving-project\n  labels:\n    opendatahub.io/dashboard: \"true\"\n    modelmesh-enabled: \"true\"\n</code></pre> <p>When creating a model server through the Dashboard, users can select a \"Serving Runtime Template\" which will create a ServingRuntime instance in their namespace which can be managed via GitOps.  The ServingRuntime helps to define different things such as the container definition, the supported model types, and available ports.</p> <pre><code>apiVersion: serving.kserve.io/v1alpha1\nkind: ServingRuntime\nmetadata:\n  annotations: # &lt;1&gt;\n    enable-route: 'true'\n    opendatahub.io/accelerator-name: ''\n    opendatahub.io/apiProtocol: REST\n    opendatahub.io/recommended-accelerators: '[\"nvidia.com/gpu\"]'\n    openshift.io/display-name: multi-model-server\n  name: multi-model-server\n  labels:\n    opendatahub.io/dashboard: 'true'\nspec:\n  supportedModelFormats:\n    - autoSelect: true\n      name: openvino_ir\n      version: opset1\n    - autoSelect: true\n      name: onnx\n      version: '1'\n    - autoSelect: true\n      name: tensorflow\n      version: '2'\n  builtInAdapter:\n    env:\n      - name: OVMS_FORCE_TARGET_DEVICE\n        value: AUTO\n    memBufferBytes: 134217728\n    modelLoadingTimeoutMillis: 90000\n    runtimeManagementPort: 8888\n    serverType: ovms\n  multiModel: true\n  containers:\n    - args:\n        - '--port=8001'\n        - '--rest_port=8888'\n        - '--config_path=/models/model_config_list.json'\n        - '--file_system_poll_wait_seconds=0'\n        - '--grpc_bind_address=0.0.0.0'\n        - '--rest_bind_address=0.0.0.0'\n      image: 'quay.io/modh/openvino_model_server@sha256:428c00232cbf3b38a3929a0d22d0e13c6388ce353e3853cc2956d175eacf6724'\n      name: ovms\n      resources:\n        limits:\n          cpu: '2'\n          memory: 8Gi\n        requests:\n          cpu: '1'\n          memory: 4Gi\n      volumeMounts:\n        - mountPath: /dev/shm\n          name: shm\n  protocolVersions:\n    - grpc-v1\n  grpcEndpoint: 'port:8085'\n  volumes:\n    - emptyDir:\n        medium: Memory\n        sizeLimit: 2Gi\n      name: shm\n  replicas: 1\n  tolerations: []\n  grpcDataEndpoint: 'port:8001'\n</code></pre> <p>While KServe and ModelMesh share the same object definition, they have some subtle differences, in particular the annotations that are available on them.  <code>enable-route</code> is one annotation that is available on a ModelMesh ServingRuntime that is not available on a KServe based Model Server.</p> <p>The InferenceService is responsible for a definition of the model that will be deployed as well as which ServingRuntime it should use to deploy it.</p> <pre><code>apiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  annotations:\n    openshift.io/display-name: fraud-detection-model\n    serving.kserve.io/deploymentMode: ModelMesh\n  name: fraud-detection-model\n  labels:\n    opendatahub.io/dashboard: 'true'\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: onnx\n        version: '1'\n      name: ''\n      resources: {}\n      runtime: multi-model-server  # &lt;1&gt;\n      storage:\n        key: aws-connection-multi-model\n        path: models/fraud-detection-model/frauddetectionmodel.onnx\n</code></pre> <ol> <li>The runtime must match the name of the ServingRuntime object that you wish to utilize to deploy the model.</li> </ol> <p>One major difference between ModelMesh and KServe is which object is responsible for creating and managing the pod where the model is deployed.  </p> <p>With KServe, the ServingRuntime acts as a \"pod template\" and each InferenceService creates its own pod to deploy a model.  A ServingRuntime can be used by multiple InferenceServices and each InferenceService will create a separate pod to deploy a model.</p> <p>By contrast, a ServingRuntime creates a pod with ModelMesh, and the InferenceService simply tells the model server pod what models to load and from where.  With ModelMesh a single ServingRuntime with multiple InferenceServices will create a single pod to load all of the models.</p>"},{"location":"odh-rhoai/gitops/#securing-a-model-server","title":"Securing a Model Server","text":"<p>To secure a model server using KServe, you must have the Authorino operator installed.</p> <p>Additionally, you must had the following two annotations to the <code>InferenceService</code> object you wish to secure:</p> <pre><code>apiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  annotations:\n    security.opendatahub.io/enable-auth: 'true'\n    serving.knative.openshift.io/enablePassthrough: 'true'\n  name: my-model-server\n</code></pre> <p>At this point in time, the model server endpoint will be secured and require a bearer token to be passed as an <code>Authorization</code> header to the endpoint that is associated with a k8s user or service account:</p> <pre><code>curl --location 'https://my-model-server-my-namespace.apps.my-cluster.com/v1/models' \\\n  --header 'Authorization: Bearer blah-blah-blah-blah' \n</code></pre> <p>To access the endpoint, the user the token is associated with must have <code>get</code> permissions on the <code>InferenceService</code> object.</p> <p>By default, a user with admin or editor access to the namespace should have the appropriate permissions.  Your user token can be obtained using the following command:</p> <pre><code>oc whoami --show-token \n</code></pre> <p>However, it is not recommended to utilize this token in use cases beyond testing, as it is a temporary token that will expire after 24 hours by default.</p> <p>For kubernetes applications, you can instead create a Service Account object and grant that Service Account the appropriate permissions.  To create a Service Account you can use the following object:</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-service-account\n  namespace: my-namespace\n</code></pre> <p>The following <code>Role</code> and <code>RoleBinding</code> can be created and used to grant permissions to the Service Account:</p> <pre><code>---\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: my-model-server-view\n  namespace: my-namespace\n  labels:\n    opendatahub.io/dashboard: 'true'\nrules:\n  - verbs:\n      - get\n    apiGroups:\n      - serving.kserve.io\n    resources:\n      - inferenceservices\n    resourceNames:\n      - my-model-server\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: my-service-account-my-model-server-view\n  namespace: my-namespace\n  labels:\n    opendatahub.io/dashboard: 'true'\nsubjects:\n  - kind: ServiceAccount\n    name: my-service-account\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: my-model-server-view\n</code></pre> <p>When the Service Account is mounted to an application pod, the user token is automatically mounted to the pod's filesystem and can be read from the following location:</p> <pre><code>cat /var/run/secrets/kubernetes.io/serviceaccount/token\n</code></pre> <p>Alternatively, if the application consuming the model server endpoint exists outside of the kubernetes cluster, you can create a Service Account with a Legacy k8s token using the following instead:</p> <pre><code>---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-service-account\n  namespace: my-namespace\nsecrets:\n  - name: my-service-account\n---\nkind: Secret\napiVersion: v1\nmetadata:\n  annotations:\n    kubernetes.io/service-account.name: my-service-account\n  name: my-service-account\n  namespace: my-namespace\ntype: kubernetes.io/service-account-token\n</code></pre> <p>This will generate a static token that can be accessed with the following:</p> <pre><code>oc get secrets my-service-account -n my-namespace --template={{.data.token}} | base64 -d\n</code></pre> <p>As with the other example, the Service Account must have <code>get</code> access to the <code>InferenceService</code> which can be granted with the <code>Role</code> and <code>RoleBinding</code> included above.</p>"},{"location":"odh-rhoai/gitops/#argocd-health-checks","title":"ArgoCD Health Checks","text":"<p>Out of the box, ArgoCD and OpenShift GitOps ship with a health check for a KServe InferenceService which is not compatible with a ModelMesh InferenceService.  When attempting to deploy a ModelMesh based InferenceService, ArgoCD will report the object as degraded.</p> <p>Custom health checks can be added to your ArgoCD instance that are compatible with both KServe and ModelMesh as well as other RHOAI objects to resolve this issue.  The Red Hat AI Services Practice maintains several custom health checks that you can utilize in your own ArgoCD instance here.</p>"},{"location":"odh-rhoai/gpu-pruner/","title":"GPU Pruner","text":"<p>In this repo, you will find the source code and usage instructions for the GPU Pruner.</p> <p>In certain environments it is very easy for cluster users to request GPUs and then (either accidentally or not accidentally) not consume GPU resources. We needed a method to proactively identify this type of use, and scale down workloads that are idle from the GPU hardware perspective, compared to the default for Notebook resources which is web activity. It is totally possible for a user to consume a GPU from a pod PoV but never actually run a workload on it!</p> <p>The <code>gpu-pruner</code> is a non-destructive idle culler that works with Red Hat OpenShift AI/Kubeflow provided APIs (<code>InferenceService</code> and <code>Notebook</code>), as well as generic <code>Deployment</code>, <code>ReplicaSet</code> and <code>StatefulSet</code>.</p> <p>The culler politely pauses workloads that appear idle by scaling them down to 0 replicas. Features may be added in the future for better notifications, but the idea is that a user can simply re-enable the workload when they are ready to test/demo again.</p> <p>It works is by querying cluster NVIDIA DCGM metrics and looking at a window of GPU utilization per pod. A scaling decision is made by looking up the pods metadata, and using owner-references to figure out the owning resource.</p>"},{"location":"odh-rhoai/image-puller/","title":"Using Kubernetes Image Puller Operator to Speed Up Start-Up Times","text":"<p>OpenShift AI provides a powerful suite of tools for building, training, and deploying AI/ML models. As with any powerful platform, optimizing start-up times for workbenches and model deployments can further enhance the overall user experience.</p> <p>By using Kubernetes Image Puller Operator, we can pre-download necessary images and speed up the launch of workbenches and model deployments. In this blog post, we'll explore how Kubernetes Image Puller works and how it can be used to optimize start-up times in OpenShift AI environments.</p>"},{"location":"odh-rhoai/image-puller/#what-is-the-kubernetes-image-puller","title":"What is the Kubernetes Image Puller?","text":"<p>Kubernetes Image Puller is a Kubernetes-native solution designed to automatically pre-pull container images to reduce start-up times for containerized workloads. By ensuring that the required container images are pre-fetched and stored locally, the Kubernetes Image Puller minimizes the need for pulling images from image registries during pod startup, thereby improving performance and reducing latency. </p>"},{"location":"odh-rhoai/image-puller/#why-use-kubernetes-image-puller-for-openshift-ai","title":"Why Use Kubernetes Image Puller for OpenShift AI?","text":"<p>Using Kubernetes Image Puller for OpenShift AI can have several benefits:</p> <ul> <li>Faster Start-Up Times: Pre-downloading workbench, runtime images, or ModelCars eliminates delays associated with image pulls during start-up. For example, workbench images often include a variety of tooling and dependencies, making them significantly larger than traditional microservice images.</li> <li>Reduced Latency: By minimizing the time needed to pull images from registries, you can ensure that your workbenches and model deployments are available faster.</li> <li>Improved Efficiency: By keeping a local cache of images, the operator can reduce load on image registries and improve network efficiency.</li> </ul> <p>While the Kubernetes Image Puller is highly effective in optimizing start-up times, it's important to ensure that your nodes have enough disk space to accommodate the pre-pulled images. Large images such as workbenches, serving runtimes with CUDA support, or model images in ModelCars can consume significant storage on each node. Insufficient storage may lead to pod evictions due to storage pressure.</p>"},{"location":"odh-rhoai/image-puller/#how-to-set-up-kubernetes-image-puller","title":"How to Set Up Kubernetes Image Puller?","text":"<p>1) Install Kubernetes Image Puller Operator: Begin by installing the Kubernetes Image Puller Operator, a community operator designed to install, configure, and manage Kubernetes Image Puller deployments on your OpenShift cluster. You can typically do this using the OpenShift web console Administrator view &gt; Operators &gt; Operator Hub and searching for or by applying below Subscription on commandline:</p> <pre><code>---\napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: kubernetes-imagepuller-operator\n  namespace: openshift-operators\nspec:\n  channel: stable\n  installPlanApproval: Automatic\n  name: kubernetes-imagepuller-operator\n  source: community-operators\n  sourceNamespace: openshift-marketplace\n</code></pre> <p>2) Specify the Images to Pre-download: List the images you wish to pre-download. The images should be specified in the <code>name=image-address</code> format. Then, add them as a semicolon-separated list in the images field of the <code>KubernetesImagePuller</code> Custom Resource.</p> <p>Here's an example configuration:</p> <pre><code>kind: KubernetesImagePuller\napiVersion: che.eclipse.org/v1alpha1\nmetadata:\n  name: image-puller\n  namespace: openshift-operators\nspec:\n  daemonsetName: k8s-image-puller\n  images: |\n    jupyter-notebook=quay.io/modh/odh-generic-data-science-notebook@sha256:36454fcf796ea284b1e551469fa1705d64c4b2698a8bf9df82a84077989faf5e;\n    code-server=image-registry.openshift-image-registry.svc:5000/redhat-ods-applications/code-server-notebook:2024.2;\n    openvino-model-server=quay.io/modh/openvino_model_server@sha256:c2d063dc4085455aae87f0d94e63cb7d88ba772662e888cb28f46226a8ac4542\n</code></pre> <ul> <li><code>images</code> field: Lists the images that will be pre-pulled. Each entry is specified as <code>name=image-address</code>. Ensure that each image is identified by its full address, including the registry and tag, to ensure proper resolution during the pull process.</li> </ul> <p>In this example, three images are pre-defined:</p> <ul> <li><code>odh-generic-data-science</code>: A Jupyter notebook image.</li> <li><code>code-server</code>: A code server image .</li> <li><code>openvino-model-server</code>: OpenVINO model server image.</li> <li></li> </ul> <p>3) After applying your configuration, a DaemonSet definition will be created, and a pod will be launched on each node. Each pod will contain the images you specified as containers, and it will begin the image pulling process.If you would like to learn more check Kubernetes Image Puller GitHub page.</p>"},{"location":"odh-rhoai/kserve-timeout/","title":"KServe Timeout Issues","text":"<p>When deploying large models or when relying on node autoscaling with KServe, KServe may timeout before a model has successfully deployed due to the default progress deadline of 10 minutes set by KNative Serving.</p> <p>When a pod takes longer than 10 minutes to deploy that leverages KNative Serving, like KServe does, KNative Serving will automatically back the pod deployment off and mark it as failed.  This can happen for a number of reasons including deploying large models that take longer than 10m minutes to pull from S3 or if you are leveraging node autoscaling to reduce the consumption of expensive GPU nodes.</p> <p>To resolve this issue, KNative supports an annotation that can be added to a KServe <code>InferenceService</code> or <code>ServingRuntime</code> objects which will set a custom timeout for the KNative service object.  Please note that the annotation is not set on the top level objects <code>metadata.annotations</code> sections, but instead an annotation field under <code>spec</code> that gets applied to the KNative Service object.</p> <p>It is generally recommended to set the annotation on the <code>InferenceService</code> as that object also contains information about the model you are deploying and is the most likely to impact the deployment time of the model server.</p>"},{"location":"odh-rhoai/kserve-timeout/#inference-service","title":"Inference Service","text":"<p>The following annotation on the <code>InferenceService</code> will update the default timeout:</p> <pre><code>apiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: my-model-server\nspec:\n  predictor:\n    annotations:\n      serving.knative.dev/progress-deadline: 30m\n</code></pre>"},{"location":"odh-rhoai/kserve-timeout/#serving-runtime","title":"Serving Runtime","text":"<p>Alternatively, the following annotation on the <code>ServingRuntime</code> will also update the default timeout:</p> <pre><code>apiVersion: serving.kserve.io/v1alpha1\nkind: ServingRuntime\nmetadata:\n  name: my-serving-runtime\nspec:\n  annotations:\n    serving.knative.dev/progress-deadline: 30m\n</code></pre>"},{"location":"odh-rhoai/kserve-uwm-dashboard-metrics/","title":"RHOAI Metrics Dashboard for Single Serving Models","text":"<p>Enable RHOAI User Workload Metrics for Single Serving Models and deploy the Grafana Metrics Dashboard to monitor the performance of your Single Serving Models and the resources they consume.</p>"},{"location":"odh-rhoai/kserve-uwm-dashboard-metrics/#overview","title":"Overview","text":"<p>Enabling RHOAI User Workload Metrics for Single Serving Models and deploying a Grafana Metrics Dashboard provides valuable insights into the performance and resource usage of your Single Model Serving instances.</p> <p>By monitoring these metrics, you can identify bottlenecks, optimize resource allocation, and ensure efficient infrastructure utilization. This enables data-driven decisions to improve the overall performance and scalability of your AI applications.</p> <ul> <li>RHOAI Metrics Dashboard for Single Serving Models Code Repository</li> </ul>"},{"location":"odh-rhoai/kserve-uwm-dashboard-metrics/#prerequisites","title":"Prerequisites","text":"<ul> <li>OpenShift 4.10 or later</li> <li>OpenShift AI 2.10+ installed</li> <li>OpenShift AI KServe installed and configured</li> <li>NVIDIA GPU Operator installed and configured</li> </ul>"},{"location":"odh-rhoai/kserve-uwm-dashboard-metrics/#installation","title":"Installation","text":"<p>To enable RHOAI User Workload Metrics for Single Serving Models and deploy the Grafana Metrics Dashboard, perform the following steps:</p> <ul> <li>Configure Monitoring for the Single Model Serving Platform</li> <li>Configure GPU Monitoring Dashboard</li> <li>Install the RHOAI Metrics Grafana and Dashboards for Single Serving Models</li> </ul>"},{"location":"odh-rhoai/kserve-uwm-dashboard-metrics/#configure-monitoring-for-the-single-model-serving-platform","title":"Configure Monitoring for the Single Model Serving Platform","text":"<p>To configure monitoring for the Single Model Serving Platform, refer to the official documentation. The Single Model Serving Platform includes metrics for supported runtimes of the KServe component. KServe relies on the underlying model-serving runtimes to provide metrics and does not generate its own. The available metrics for a deployed model depend on its model-serving runtime.</p> <p>Additionally, you can configure monitoring for OpenShift Service Mesh to understand dependencies and traffic flow between components in the mesh.</p> <p>Once monitoring is configured for the Single Model Serving Platform, you can view the metrics in the OpenShift Web Console under the Observe Dashboards section.</p>"},{"location":"odh-rhoai/kserve-uwm-dashboard-metrics/#configure-gpu-monitoring-dashboard","title":"Configure GPU Monitoring Dashboard","text":"<p>To configure the GPU Monitoring Dashboard, refer to the official documentation. The GPU Monitoring Dashboard provides a comprehensive view of GPU utilization, memory usage, and other metrics for your GPU nodes.</p> <p>The GPU Operator exposes GPU telemetry for Prometheus using the NVIDIA DCGM Exporter. These metrics can be visualized in the OpenShift Web Console under the Observe Dashboards section, specifically in the NVIDIA DCGM Exporter Dashboard.</p> <p>Note: This step is optional but very useful for monitoring the GPU resources consumed by your Single Serving Models. If you do not enable this step, the Grafana Dashboard will not display GPU metrics.</p>"},{"location":"odh-rhoai/kserve-uwm-dashboard-metrics/#install-the-rhoai-metrics-grafana-and-dashboards-for-single-serving-models","title":"Install the RHOAI Metrics Grafana and Dashboards for Single Serving Models","text":"<p>To install the RHOAI Metrics Grafana Dashboards for Single Serving Models (for both vLLM and OpenVino), refer to the RHOAI UWM repository. The Grafana Dashboard provides a comprehensive view of the performance and resource utilization of Single Serving Models.</p> <pre><code>kubectl apply -k overlays/grafana-uwm-user-app\n</code></pre> <p>The RHOAI UWM Grafana Dashboard will deploy a Grafana instance with pre-configured dashboards for monitoring the performance of your Single Serving Models using the Grafana Operator.</p> <p>The following dashboards are currently available:</p> <ul> <li>vLLM Model Metrics Dashboard: Provides Model metrics for vLLM Single Serving Models dashboard.</li> </ul> <p></p> <ul> <li>vLLM Service Performance Dashboard: Provides Service Performance metrics for vLLM Single Serving Models dashboard.</li> </ul> <p></p> <ul> <li>OpenVino Service Model Metrics Dashboard: Provides metrics for OpenVino Single Serving Models</li> </ul> <p></p> <ul> <li>OpenVino Model Metrics Dashboard: Provides Service Performance metrics for OpenVino Single Serving Models.</li> </ul> <p></p>"},{"location":"odh-rhoai/kserve-uwm-dashboard-metrics/#conclusion","title":"Conclusion","text":"<p>By turning on RHOAI User Workload Metrics and setting up the Grafana Dashboard, you can easily track how your Single Serving Models are doing and what resources they're using. It helps you find problems, tweak resource use, and make better choices to keep your AI apps running smoothly.</p>"},{"location":"odh-rhoai/llm-guardrails/","title":"LLM GuardRails","text":""},{"location":"odh-rhoai/llm-guardrails/#llm-guardrails","title":"LLM Guardrails","text":"<p>Large Language Models (LLMs) require guardrails to ensure safety, reliability, and ethical compliance in enterprise applications. Without safeguards, they can be misused to generate harmful content, assist in illegal activities, or spread misinformation.  </p> <p>Key risks include: - Bypassing ethical constraints (e.g., fraud, hacking, or exploitation) - Unintended AI assistance in harmful or illegal actions - Lack of compliance and security in enterprise applications  </p> <p>A dedicated risk detection system is essential to filter harmful prompts before they reach the LLM, ensuring trustworthy and responsible AI deployment.</p>"},{"location":"odh-rhoai/llm-guardrails/#granite-guardian-models","title":"Granite Guardian Models","text":"<p>Granite Guardian is a fine-tuned Granite 3 Instruct model designed to detect risks in prompts and responses. It can help with risk detection along many key dimensions catalogued in the IBM AI Risk Atlas.</p> <p><code>Granite Guardian</code> enables application developers to screen user prompts and LLM responses for harmful content. These models are built on top of latest Granite family and are available at various platforms under the Apache 2.0 license:</p> <ul> <li>Granite Guardian 3.1 8B : HF</li> <li>Granite Guardian 3.1 2B : HF</li> </ul> <p></p>"},{"location":"odh-rhoai/llm-guardrails/#granite-guardian-risk-detection","title":"Granite Guardian - Risk Detection","text":"<p>Granite Guardian enhances AI safety and reliability by detecting risks in key areas:  </p> <ul> <li> <p>Harm Detection: </p> <ul> <li>Flags risks in user prompts and AI-generated responses  </li> <li>Enforces responsible AI usage  </li> </ul> </li> <li> <p>RAG Risk Assessment:</p> <ul> <li>Evaluates retrieved context relevance  </li> <li>Ensures responses are accurate and answer the query directly  </li> </ul> </li> <li> <p>Function Call Risk Detection:</p> <ul> <li>Detects hallucinations and invalid function calls  </li> <li>Prevents fabricated information in agent workflows By integrating Granite Guardian, enterprises strengthen AI security and ensure reliable outputs.</li> </ul> </li> </ul>"},{"location":"odh-rhoai/llm-guardrails/#how-does-granite-guardian-work","title":"How Does Granite Guardian Work?","text":"<p>Granite Guardian analyzes user inputs and AI responses to detect harmful, unethical, or misleading content before reaching the end user. It identifies risks such as harm, bias, jailbreaking, violence, profanity, sexual content, and unethical behavior.  </p> <p>Additionally, it assesses hallucination risks in RAG pipelines, including context relevance, groundedness, and answer relevance. The model also detects function calling hallucinations in agentic workflows, ensuring AI outputs remain accurate, secure, and reliable.</p> <p>Let\u2019s examine how Granite Guardian identifies and blocks harmful queries before they reach the main LLM.  </p> <pre><code>risky_query = \"How can I hurt someone?\"  # Risky\nprint(process_query(risky_query))\n</code></pre> <p>Granite Guardian analyzes the query and detects a risk, classifying it as unsafe. The model's output will be Yes, indicating a risky/unsafe query. As a result, the system blocks it for safety.</p> <pre><code>\ud83d\udd0d Checking Risk for Query: 'How can I hurt someone?'\n\u274c Risk detected! Query blocked for safety.\n\n\ud83c\udfaf \ud83d\udeab\ud83d\udd12 This query violates safety guidelines. Blocked\n</code></pre>"},{"location":"odh-rhoai/llm-guardrails/#example-of-using-granite-guardian-for-llm-guardrails","title":"Example of Using Granite Guardian for LLM Guardrails","text":"<p>If you're interested in implementing Granite Guardian for risk detection and LLM guardrails, check out this example notebook:  </p> <ul> <li>Granite Guardian for LLM Guardrails </li> </ul>"},{"location":"odh-rhoai/llm-guardrails/#how-it-works","title":"How It Works","text":"<p>This function processes user queries in two steps:  </p> <p>1\ufe0f. Risk Check (Guardian Model)    - Blocks risky queries with a \ud83d\udeab warning.    - Allows safe queries to proceed to the LLM.  </p> <p>2\ufe0f. Response Generation (Main LLM)    - Safe queries receive an LLM-generated response.    - Risky queries are denied access.  </p> <p>Granite Guardian provides essential risk detection for AI applications, ensuring safer, more reliable interactions with LLMs.</p>"},{"location":"odh-rhoai/model-serving-type-modification/","title":"Model Serving Type Modification","text":""},{"location":"odh-rhoai/model-serving-type-modification/#background","title":"Background","text":"<p>In each Project, you can only have one type of Model Serving: either Single-model Serving, or Multi-model Serving.</p> <p>This is due to the fact that the engines responsible of model serving share the same definitions (CRDs) for them. If both were enabled simultaneously in the same project, it would be impossible to know which one should handle a model serving definition.</p> <p>So when you serve the first model in a project, you need to choose which type of model serving you want to use. It then becomes \"fixed\" and all further model serving will be of the chose type.</p>"},{"location":"odh-rhoai/model-serving-type-modification/#changing-the-serving-type","title":"Changing the Serving Type","text":""},{"location":"odh-rhoai/model-serving-type-modification/#how-this-works","title":"How this works?","text":"<p>The behavior is controlled by a simple label in the Namespace definition of your project: <code>modelmesh-enabled</code>.</p> <ul> <li> <p>When this label does not exist, you get the initial choice panel on the Dashboard:</p> <p></p> </li> <li> <p>When this label is set to <code>true</code>, so like this:</p> <pre><code>  labels:\n    modelmesh-enabled: 'true'\n</code></pre> <p>Then Multi-model Serving (ModelMesh) is enabled:</p> <p></p> </li> <li> <p>When this label is set to <code>false</code> (or whatever value other than true to be honest, you can try \ud83d\ude09), like this:</p> <pre><code>  labels:\n    modelmesh-enabled: 'false'\n</code></pre> <p>Then Single-model Serving (ModelMesh) is enabled:</p> <p></p> </li> </ul>"},{"location":"odh-rhoai/model-serving-type-modification/#changing-the-type","title":"Changing the type","text":"<p>Important</p> <p>Starting with version 2.16 of OpenShift AI, the following procedure is not needed any more. A Change button has been added right next to the model serving type label, achieving the same in a much simpler way.</p> <p>So what to do if you have remorse at some point and want to change the serving type? The recipe is pretty simple with what we now know.</p> <p>Important</p> <p>Prerequisite: you need to be OpenShift Cluster admin, or find a friend who is, or have enough rights to modify the Namespace object from your project.</p> <ul> <li>Remove all existing served models. As the type of model serving and engine will change, the definitions won't be compatible in most cases and cause lots of trouble. If you don't use the Dashboard, remove all instances of <code>InferenceService</code> and <code>ServingRuntime</code> from your project/namespace.</li> <li> <p>In the OpenShift Console, open the YAML definition of your Namespace (not the Project, that you can't modify!).   You will find it under the Administrator perspective, under Administration-&gt;Namespaces:</p> <p></p> </li> <li> <p>In the YAML, modify, delete or add the label <code>modelmesh-enabled</code> according to the rules explained above.</p> </li> <li>The change will be immediately reflected in the Dashboard.</li> <li>You can now deploy a new model with the Serving type you chose.</li> </ul>"},{"location":"odh-rhoai/nvidia-gpus/","title":"Working with GPUs","text":""},{"location":"odh-rhoai/nvidia-gpus/#using-nvidia-gpus-on-openshift","title":"Using NVIDIA GPUs on OpenShift","text":""},{"location":"odh-rhoai/nvidia-gpus/#how-does-this-work","title":"How does this work?","text":"<p>NVIDIA GPUs can be easily installed on OpenShift. Basically it involves installing two different operators.</p> <p>The Node Feature Discovery operator will \"discover\" your cards from a hardware perspective and appropriately label the relevant nodes with this information.</p> <p>Then the NVIDIA GPU operator will install the necessary drivers and tooling to those nodes. It will also integrate into Kubernetes so that when a Pod requires GPU resources it will be scheduled on the right node, and make sure that the containers are \"injected\" with the right drivers,  configurations and tools to properly use the GPU.</p> <p>So from a user perspective, the only thing you have to worry about is asking for GPU resources when defining your pods, with something like:</p> <pre><code>spec:\n  containers:\n  - name: app\n    image: ...\n    resources:\n      requests:\n        memory: \"64Mi\"\n        cpu: \"250m\"\n        nvidia.com/gpu: 2\n      limits:\n        memory: \"128Mi\"\n        cpu: \"500m\"\n</code></pre> <p>But don't worry, OpenShift AI and Open Data Hub take care of this part for you when you launch notebooks, workbenches, model servers, or pipeline runtimes!</p>"},{"location":"odh-rhoai/nvidia-gpus/#installation","title":"Installation","text":"<p>Here is the documentation you can follow:</p> <ul> <li>OpenShift AI documentation</li> <li>NVIDIA documentation (more detailed)</li> </ul>"},{"location":"odh-rhoai/nvidia-gpus/#advanced-configuration","title":"Advanced configuration","text":""},{"location":"odh-rhoai/nvidia-gpus/#working-with-taints","title":"Working with taints","text":"<p>In many cases, you will want to restrict access to GPUs, or be able to provide choice between different types of GPUs: simply stating \"I want a GPU\" is not enough. Also, if you want to make sure that only the Pods requiring GPUs end up on GPU-enabled nodes (and not other Pods that just end up being there at random because that's how Kubernetes works...), you're at the right place!</p> <p>The only supported method at the moment to achieve this is to taint nodes, then apply tolerations on the Pods depending on where you want them scheduled. If you don't pay close attention though when applying taints on Nodes, you may end up with the NVIDIA drivers not installed on those nodes...</p> <p>In this case you must:</p> <ul> <li> <p>Apply the taints you need to your Nodes or MachineSets, for example:</p> <pre><code>apiVersion: machine.openshift.io/v1beta1\nkind: MachineSet\nmetadata:\n  ...\nspec:\n  replicas: 1\n  selector:\n    ...\n  template:\n    ...\n    spec:\n      ...\n      taints:\n        - key: restrictedaccess\n          value: \"yes\"\n          effect: NoSchedule\n</code></pre> </li> <li> <p>Apply the relevant toleration to the NVIDIA Operator.</p> <ul> <li> <p>In the <code>nvidia-gpu-operator</code> namespace, get to the Installed Operator menu, open the NVIDIA GPU Operator settings, get to the ClusterPolicy tab, and edit the ClusterPolicy.</p> <p></p> </li> <li> <p>Edit the YAML, and add the toleration in the daemonset section:</p> <pre><code>apiVersion: nvidia.com/v1\nkind: ClusterPolicy\nmetadata:\n  ...\n  name: gpu-cluster-policy\nspec:\n  vgpuDeviceManager: ...\n  migManager: ...\n  operator: ...\n  dcgm: ...\n  gfd: ...\n  dcgmExporter: ...\n  cdi: ...\n  driver: ...\n  devicePlugin: ...\n  mig: ...\n  sandboxDevicePlugin: ...\n  validator: ...\n  nodeStatusExporter: ...\n  daemonsets:\n    ...\n    tolerations:\n      - effect: NoSchedule\n        key: restrictedaccess\n        operator: Exists\n  sandboxWorkloads: ...\n  gds: ...\n  vgpuManager: ...\n  vfioManager: ...\n  toolkit: ...\n...\n</code></pre> </li> </ul> </li> </ul> <p>That's it, the operator is now able to deploy all the NVIDIA tooling on the nodes, even if they have the <code>restrictedaccess</code> taint. Repeat the procedure for any other taint you want to apply to your nodes.</p> <p>Note</p> <p>The first taint that you want to apply on GPU nodes is <code>nvidia.com/gpu</code>. This is the standard taint for which the NVIDIA Operator has a built-in toleration, so no need to add it. Likewise, Notebooks, Workbenches or other components from ODH/RHOAI that request GPUs will already have this toleration in place. For other Pods you schedule yourself, or using Pipelines, you should make sure the toleration is also applied. Doing this will ensure that only Pods really requiring GPUs are scheduled on those nodes.</p> <p>You can of course apply many different taints at the same time. You would simply have to apply the matching toleration on the NVIDIA GPU Operator, as well as on the Pods that need to run there.</p>"},{"location":"odh-rhoai/nvidia-gpus/#autoscaler-and-gpus","title":"Autoscaler and GPUs","text":"<p>As they are expensive, GPUs are good candidates to put behind an Autoscaler. But due to this there are some subtleties if you want everything to go smoothly.</p>"},{"location":"odh-rhoai/nvidia-gpus/#configuration","title":"Configuration","text":"<p>Warning</p> <p>For the autoscaler to work properly with GPUs, you have to set a specific label to the MachineSet. It will help to Autoscaler figure out (in fact simulate) what it is allowed to do. This is especially true if you have different MachineSets that feature different types of GPUs.</p> <p>As per the referenced article above, the <code>type</code> for gpus you set through the label cannot be <code>nvidia.com/gpu</code> (as you will sometimes find in the standard documentation), because it's not a valid label. Therefore, only for the autoscaling purpose, you should give the <code>type</code> a specific name with letters, numbers and dashes only, like <code>Tesla-T4-SHARED</code> in this example.</p> <ul> <li> <p>Edit the MachineSet configuration to add the label that the Autoscaler will expect:</p> <pre><code>apiVersion: machine.openshift.io/v1beta1\nkind: MachineSet\n...\nspec:\n  ...\n  template:\n    ...\n    spec:\n      metadata:\n        labels:\n          cluster-api/accelerator: Tesla-T4-SHARED\n</code></pre> </li> <li> <p>Create your ClusterAutoscaler configuration (example):</p> <pre><code>apiVersion: autoscaling.openshift.io/v1\nkind: ClusterAutoscaler\nmetadata:\n  name: \"default\"\nspec:\n  logVerbosity: 4\n  maxNodeProvisionTime: 15m\n  podPriorityThreshold: -10\n  resourceLimits:\n    gpus:\n      - type: Tesla-T4-SHARED\n        min: 0\n        max: 8\n  scaleDown:\n    enabled: true\n    delayAfterAdd: 20m\n    delayAfterDelete: 5m\n    delayAfterFailure: 30s\n    unneededTime: 5m\n</code></pre> <p>Note</p> <p>The <code>delayAfterAdd</code> parameter has to be set higher than standard value as NVIDIA tooling can take a lot of time to deploy, 10-15mn.</p> </li> <li> <p>Create the MachineSet Autoscaler:</p> <pre><code>apiVersion: autoscaling.openshift.io/v1beta1\nkind: MachineAutoscaler\nmetadata:\n  name: machineset-name\n  namespace: \"openshift-machine-api\"\nspec:\n  minReplicas: 1\n  maxReplicas: 2\n  scaleTargetRef:\n    apiVersion: machine.openshift.io/v1beta1\n    kind: MachineSet\n    name: machineset-name\n</code></pre> </li> </ul>"},{"location":"odh-rhoai/nvidia-gpus/#scaling-to-zero","title":"Scaling to zero","text":"<p>As GPUs are expensive resources, you may want to scale down your MachineSet to zero to save on resources. This will however require some more configuration than just setting the minimum size to zero...</p> <p>First, some background to help you understand and enable you to solve issues that may arise. You can skip the whole explanation, but it's worth it, so please bear with me.</p> <p>When you request resources that aren't available, the Autoscaler looks at all the MachineAutoscalers that are available, with their corresponding MachineSets. But how to know which one to use? Well, it will first simulate the provisioning of a Node from each MachineSet, and see if it would fit the request. Of course, if there is already at least one Node available from a given MachineSet, the simulation would be bypassed as the Autoscaler already knows what it will get. If there are different MachineSets that fit and to choose from, the default and only \"Expander\" available for now in OpenShift to make its decision is <code>random</code>. So it will simply picks one totally randomly.</p> <p>That's all perfect and everything, but for GPUs, if you don't start the Node for real, we don't know what's in it! So that's where we have to help the Autoscaler with a small hint.</p> <ul> <li> <p>Set this annotation manually if it's not there. It will stick after the first scale up though, along with some other annotations the Autoscaler will add, thanks for its newly discovered knowledge.</p> <pre><code>apiVersion: machine.openshift.io/v1beta1\nkind: MachineSet\nmetadata:\n  annotations:\n    machine.openshift.io/GPU: \"1\"\n</code></pre> </li> </ul> <p>Now to the other issue that may happen if you are in an environment with multiple Availability Zones (AZ)...</p> <p>Although when you define a MachineSet you can set the AZ and have all the Nodes spawned properly in it, the Autoscaler simulator is not that clever. So it will simply pick a Zone at random. If this is not the one where you want/need your Pod to run, this will be a problem...</p> <p>For example, you may already have a Persistent Volume (PV) attached to you Notebook. If your storage does now support AZ-spanning (like AWS EBS volumes), your PV is bound to a specific AZ. If the Simulator creates a virtual Node in a different AZ, there will be a mismatch, your Pod would not be schedulable on this Node, and the Autoscaler will (wrongly) conclude that it cannot use this MachineSet for a scale up!</p> <p>Here again, we have to give a hint to the Autoscaler to what the Node will look like in the end.</p> <ul> <li> <p>In you MachineSet, in the labels that will be added to the node, add information regarding the topology of the Node, as well as for the volumes that may be attached to it. For example:</p> <pre><code>apiVersion: machine.openshift.io/v1beta1\nkind: MachineSet\nmetadata:\nspec:\n  template:\n    spec:\n      metadata:\n        labels:\n          ...\n          topology.kubernetes.io/zone: us-east-2a\n          topology.ebs.csi.aws.com/zone: us-east-2a\n</code></pre> </li> </ul> <p>With this, the simulated Node will be at the right place, and the Autoscaler will consider the MachineSet valid for scale up!</p> <p>Reference material:</p> <ul> <li>https://cloud.redhat.com/blog/autoscaling-nvidia-gpus-on-red-hat-openshift</li> <li>https://access.redhat.com/solutions/6055181</li> <li>https://bugzilla.redhat.com/show_bug.cgi?id=1943194</li> </ul>"},{"location":"odh-rhoai/nvidia-gpus/#gpu-partitioning-sharing","title":"GPU Partitioning / Sharing","text":"<p>There are also situations where the GPU(s) you have access to might be oversized for the task at hand, and having a single user or process lock-up and \"hog\" that GPU can be inefficient.  There are thankfully some partitioning strategies that can be brought to bear in order to deal with these situations.  Although there are multiple techniques, with various pros and cons, the net effect of these implementations is that what used to look like a single GPU will then look like multiple GPUs.  Obviously, there is no magic in the process, and the laws of physics still hold: there are trade-offs, and the multiple \"partitioned\" GPUs are not going to be faster or crunch more data than the real underlying physical GPU.  </p> <p>If this is a situation that you are facing, consult this repository for more detailed information and examples.</p>"},{"location":"odh-rhoai/nvidia-gpus/#time-slicing-gpu-sharing","title":"Time Slicing (GPU sharing)","text":"<p>Do you want to share GPUs between different Pods? Time Slicing is one of the solutions you can use!</p> <p>The NVIDIA GPU Operator enables oversubscription of GPUs through a set of extended options for the NVIDIA Kubernetes Device Plugin. GPU time-slicing enables workloads that are scheduled on oversubscribed GPUs to interleave with one another.</p> <p>This mechanism for enabling time-slicing of GPUs in Kubernetes enables a system administrator to define a set of replicas for a GPU, each of which can be handed out independently to a pod to run workloads on. Unlike Multi-Instance GPU (MIG), there is no memory or fault-isolation between replicas, but for some workloads this is better than not being able to share at all. Internally, GPU time-slicing is used to multiplex workloads from replicas of the same underlying GPU.</p> <ul> <li>Time Slicing Full reference</li> <li>Time Slicing Example Repository</li> </ul>"},{"location":"odh-rhoai/nvidia-gpus/#configuration_1","title":"Configuration","text":"<p>This is a simple example on how to quickly setup Time Slicing on your OpenShift cluster. In this example, we have a MachineSet that can provide nodes with one T4 card each that we want to make \"seen\" as 4 different cards so that multiple Pods requiring GPUs can be launched, even if we only have one node of this type.</p> <ul> <li> <p>Create the ConfigMap that will define how we want to slice our GPU:</p> <pre><code>kind: ConfigMap\napiVersion: v1\nmetadata:\n  name: time-slicing-config\n  namespace: nvidia-gpu-operator\ndata:\n  tesla-t4: |-\n    version: v1\n    sharing:\n      timeSlicing:\n        resources:\n        - name: nvidia.com/gpu\n          replicas: 4\n</code></pre> <p>NOTE     - The ConfigMap has to be called <code>time-slicing-config</code> and must be created in the <code>nvidia-gpu-operator</code> namespace.     - You can add many different resources with different configurations. You simply have to provide the corresponding Node label that has been applied by the operator, for example <code>name: nvidia.com/mig-1g.5gb / replicas: 2</code> if you have a MIG configuration applied to a Node with a A100.     - You can modify the value of <code>replicas</code> to present less/more GPUs. Be warned though: all the Pods on this node will share the GPU memory, with no reservation. The more slices you create, the more risks of OOM errors (out of memory) you get if your Pods are hungry (or even only one!).</p> </li> <li> <p>Modify the ClusterPolicy called <code>gpu-cluster-policy</code> (accessible from the NVIDIA Operator view in the <code>nvidia-gpu-operator</code> namespace) to point to this configuration, and eventually add the default configuration (in case you nodes are not labelled correctly, see below)</p> <pre><code>apiVersion: nvidia.com/v1\nkind: ClusterPolicy\nmetadata:\n  ...\n  name: gpu-cluster-policy\nspec:\n  ...\n  devicePlugin:\n    config:\n      default: tesla-t4\n      name: time-slicing-config\n  ...\n</code></pre> </li> <li> <p>Apply label to your MachineSet for the specific slicing configuration you want to use on it:</p> <pre><code>apiVersion: machine.openshift.io/v1beta1\nkind: MachineSet\nmetadata:\nspec:\n  template:\n    spec:\n      metadata:\n        labels:\n          nvidia.com/device-plugin.config: tesla-t4\n</code></pre> </li> </ul>"},{"location":"odh-rhoai/nvidia-gpus/#multi-instance-gpu-mig","title":"Multi-Instance GPU (MIG)","text":"<p>Multi-Instance GPU (MIG) enables a single physical GPU to be partitioned into several isolated instances, each with its own compute resources, memory, and performance profiles.</p> <p>There are two types of MIG strategies: <code>Single</code> and <code>Mixed</code>. The single MIG strategy should be utilized when all GPUs on a node have MIG enabled, while the <code>Mixed</code> MIG strategy should be utilized when not all GPUs on a node have MIG enabled.</p> <p>NOTE: MIG is only supported with the following NVIDIA GPU Types - A30, A100, A100X, A800, AX800, H100, H200, and H800.</p> <ul> <li>MIG Full reference</li> <li>MIG Single Example Repository</li> <li>MIG Mixed Example Repository</li> </ul>"},{"location":"odh-rhoai/nvidia-gpus/#multi-process-service-mps","title":"Multi-Process Service (MPS)","text":"<p>Multi-Process Service (MPS) facilitates concurrent sharing of a single GPU among multiple CUDA applications.</p> <p>MPS is an alternative, binary-compatible implementation of the CUDA Application Programming Interface (API). The MPS runtime architecture is designed to transparently enable co-operative multi-process CUDA applications.</p> <ul> <li>MPS Full reference</li> <li>MPS Example Repository</li> </ul> <p>NOTE: Despite the tests passing, MPS isn't working correctly on OpenShift currently, due to only one process per GPU can run at any time. RH and NVIDIA engineers are working to fix this issue as soon as possible.</p>"},{"location":"odh-rhoai/nvidia-gpus/#aggregating-gpus-multi-gpu","title":"Aggregating GPUs (Multi-GPU)","text":"<p>Some Large Language Models (LLMs), such as Llama-3-70B and Falcon 180B, can be too large to fit into the memory of a single GPU (vRAM). Or in some cases, GPUs that would be large-enough might be difficult to obtain.  If you find yourself in such a situation, it is natural to wonder whether an aggregation of multiple, smaller GPUs can be used instead of one single large GPU.</p> <p>Thankfully, the answer is essentially Yes. To address these challenges, we can use more advanced configurations to distribute the LLM workload across several GPUs. One option is leveraging tensor parallelism, where the LLM is split across several GPUs, with each GPU processing a portion of the model's tensors. This approach ensures efficient utilization of available resources (GPUs) across one or several workers.</p> <p>Some Serving Runtimes, such as vLLM, support tensor parallelism, allowing for both single-worker and multi-worker configurations (the difference whether your GPUs are all in the same machine, or are spread across machines). </p> <p>vLLM has been added as an Out-of-the-box serving runtime, starting with Red Hat OpenShift AI version 2.10 link to our RHOAI doc</p> <p>For a detailed guide on implementing these solutions, refer to our repository.</p> <ul> <li>Single Worker Node - Multiple GPUs Example Repository</li> <li>Multiple Worker Node - Multiple GPUs Example Repository</li> </ul>"},{"location":"odh-rhoai/nvidia-gpus/#nvidia-vgpus","title":"NVIDIA vGPUs","text":"<p>NVIDIA vGPU is a software that allows virtualisation of a whole GPU, such that it can be assigned to multiple VMs, within VMWare vSphere. In terms of OpenShift, this means that a single physical GPU could be split such that it can be assigned to several clusters.</p>"},{"location":"odh-rhoai/nvidia-gpus/#pros","title":"Pros","text":"<ul> <li> <p>Helpful in particularly resource constrainted environments where GPUs need to be spread out.</p> </li> <li> <p>Useful for redistributing GPU compute to other clusters, when a full GPU would remain underutilised.</p> </li> <li> <p>Physical GPUs can be split in vSphere via either timeslicing, or MIG, if available for the model of GPU.</p> </li> </ul>"},{"location":"odh-rhoai/nvidia-gpus/#cons","title":"Cons","text":"<ul> <li> <p>NVIDIA vGPU is a licensed product, therefore a license per vGPU-equipped OCP worker node is required. Communication between the vGPU-equipped worker and a license service will also be required.</p> </li> <li> <p>NVIDIA vGPUs require a vGPU specific driver image, which need to be either built manually, or retrieved from the NVIDIA NGC Catalog. Both of these require an active NVIDIA AI Enterprise subscription.</p> </li> <li> <p>Additional setup of the vSphere ESXi hosts is required.</p> </li> <li> <p>Workers with vGPUs have a single allocatable <code>nvidia.com/gpu</code> resource, which cannot be split any further.</p> </li> </ul>"},{"location":"odh-rhoai/nvidia-gpus/#setup","title":"Setup","text":"<p>Setting up vGPUs to be used on OpenShift requires some pre-work on the ESXi hosts themselves, that can be found in NVIDIA documentation. </p> <p>Post ESXi setup, equipping OpenShift worker nodes with vGPUs isn't directly possible with the vSphere OpenShift installer. To get around this, you'll need to create a custom RHCOS template within vSphere that is preconfigured with the vGPU PCI device. The RHCOS template can be referenced in the <code>machineSet</code> definition as shown below.</p> <pre><code>apiVersion: machine.openshift.io/v1beta1\nkind: MachineSet\n...\ntemplate:\n  ...\n  spec:\n    providerSpec:\n      value:\n        ...\n        template: &lt;vm_template_name&gt;\n</code></pre> <p>NOTE: You will need a separate <code>machineSet</code> and RHCOS template, for each vGPU configuration you need. For instance, if you're using a MIG sliced A100 80GB for your vGPUs, you may want a <code>machineSet</code> each for <code>1g.10gb</code> slices, <code>3g.40gb</code> slices and <code>7g.80gb</code> slices. </p> <p>Once the worker node has been provisioned, and the Node Feature Discovery (NFD) and NVIDIA GPU operator pods have spun up on it, the worker node should have a single allocatable <code>nvidia.com/gpu</code> resource. Unfortunately, vGPUs can't be partitioned any smaller in OpenShift, using any of the above methods (i.e. Timeslicing, MPS, MIG) therefore, a single vGPU can be allocated to only a single workload at a time.</p>"},{"location":"odh-rhoai/nvidia-gpus/#vgpu-drivers","title":"vGPU Drivers","text":"<p>As mentioned in the pros and cons list, vGPUs require specific drivers to work. These drivers can be either built according to NVIDIA's documentation, or they can be retrieved directly from the NVIDIA NGC catalog. </p> <p>The version of the NVIDIA vGPU driver, needs to correspond to the version of the vGPU Manager image, that is on the underlying ESXi host. For instance, if vGPU software version 17.6 was being used, the vGPU Manager version would be <code>550.163.02</code>, and the corresponding maximum version for the vGPU Driver would be <code>550.163.01</code>.</p> <p>If the drivers are to be built, or utilised in a disconnected OCP environment, then a private image registry is required to push the images to. In the case that driver images are pulled directly from NVIDIA NGC, an image pull secret needs to be created that contains the NGC API Key.</p> <p>To pull a specific vGPU driver image, it needs to be defined in the <code>clusterPolicy</code> CR. Below is an example that pulls directly from NVIDIA NGC.</p> <pre><code>apiVersion: nvidia.com/v1\nkind: ClusterPolicy\nmetadata:\n  ...\n  name: gpu-cluster-policy\nspec:\n  ...\n  driver:\n    repository: nvcr.io/nvidia/vgpu\n    image: vgpu-guest-driver-X\n    version: 550.163.01-rhcos4.XX\n    imagePullSecrets:\n      - ngc-secret\n</code></pre>"},{"location":"odh-rhoai/nvidia-gpus/#licensing","title":"Licensing","text":"<p>Licensing of the vGPUs is done through the NVIDIA License Portal (NLP). NVIDIA offers a solution for both connected and disconnected environments, that's their Cloud License Service (CLS) and Delegated License Service (DLS) respectively. All communication between either license services, and the vGPU-equipped worker nodes, is through HTTPS / TLS port 443.</p> <p>An NVIDIA CLS instance is hosted directly on the NLP. This means that NVIDIA maintains the CLS instance themselves, and so, the lifecycle of the CLS doesn't need to be managed directly. Comparatively, an NVIDIA DLS instance is deployed on-premises, either directly on the OpenShift cluster, or on a VM. This means that the DLS instance will need to be deployed and maintained directly, which includes downloading licenses from the NLP and manually uploading them to the instance.</p> <p>Irrespective of which license service has been chosen, a client configuration token will be able to be generated. The client configuration token contains information about which service instance generated it, and is used by the NVIDIA GPU operator to lease licenses for the vGPU-equipped workers. To do this, you need to create the following <code>configMap</code></p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetaData:\n  name: licensing-config\n  namespace: nvidia-gpu-operator\ndata:\n  client_configuration_token.tok: &lt;TOKEN&gt;\n  gridd.conf: |\n    # Description: Set Feature to be enabled\n    # Data type: integer\n    # Possible values:\n    # 0 =&gt; for unlicensed state\n    # 1 =&gt; for NVIDIA vGPU\n    # 2 =&gt; for NVIDIA RTX Virtual Workstation\n    # 4 =&gt; for NVIDIA Virtual Compute Server\n    FeatureType=1\n    # ProxyServerAddress=&lt;ADDRESS&gt; # Required if you have a proxy between OCP and CLS.\n    # ProxyServerPort=&lt;PORT&gt;\n</code></pre> <p>This <code>configMap</code> can then be referenced in the NVIDIA GPU operator <code>clusterPolicy</code> CR, as shown below:</p> <pre><code>apiVersion: nvidia.com/v1\nkind: ClusterPolicy\nmetadata:\n  ...\n  name: gpu-cluster-policy\nspec:\n  ...\n  driver:\n    ...\n    licensingConfig:\n      nlsEnabled: true\n      configMapName: \"licensing-config\"\n  ...\n</code></pre> <p>To validate that the vGPU worker nodes are licensed, NVIDIA Data Centre GPU Manager (DCGM) will export the <code>DCGM_FI_DEV_VGPU_LICENSE_STATUS</code> metric, per worker node. This will show a value of <code>1</code> if the worker node is licensed properly. Alternatively, the license service instance should show that the license(s) are allocated.</p>"},{"location":"odh-rhoai/nvidia-gpus/#references","title":"References","text":"<ul> <li> <p>NVIDIA vGPU: VMware Deployment Guide</p> </li> <li> <p>NVIDIA License System User Guide</p> </li> <li> <p>NVIDIA OpenShift Container Platform on VMware vSphere with NVIDIA vGPUs</p> </li> </ul>"},{"location":"odh-rhoai/odh-tools-and-extensions-companion/","title":"ODH Tools and Extensions Companion","text":"<p>In this repo, you will find the source code and usage instructions for the Open Data Hub Tools &amp; Extensions Companion.</p> <p>This application implements tools that can be useful to anyone working with Open Data Hub (ODH), Red Hat OpenShift AI (RHOAI), or even locally with Podman.</p> <p></p>"},{"location":"odh-rhoai/odh-tools-and-extensions-companion/#current-features","title":"Current Features","text":""},{"location":"odh-rhoai/odh-tools-and-extensions-companion/#s3-tools","title":"S3 Tools","text":"<ul> <li>S3 Buckets Management: browsing, creation, deletion</li> <li>S3 Objects Browser:<ul> <li>Single file upload, Multiple files uploads, Downloads</li> <li>File preview</li> <li>Model import from HuggingFace</li> </ul> </li> </ul>"},{"location":"odh-rhoai/odh-tools-and-extensions-companion/#gpu-tools","title":"GPU Tools","text":"<ul> <li>VRAM Estimator: helps you calculate the VRAM and number of GPUs you need for inference and training.</li> </ul>"},{"location":"odh-rhoai/odh-tools-and-extensions-companion/#settings-and-validations","title":"Settings and validations","text":"<ul> <li>S3 connection testing</li> <li>HuggingFace authentication testing</li> </ul>"},{"location":"odh-rhoai/openshift-group-management/","title":"OpenShift Group Management","text":"<p>In the Red Hat OpenShift Documentation, there are instructions on how to configure a specific list of RHOAI Administrators and RHOAI Users.</p> <p>However, if the list of users keeps changing, the membership of the groupe called <code>rhods-users</code> will have to be updated frequently. By default, in OpenShift, only OpenShift admins can edit group membership. Being a RHOAI Admin does not confer you those admin privileges, and so, it would fall to the OpenShift admin to administer that list.</p> <p>The instructions in this page will show how the OpenShift Admin can create these groups in such a way that any member of the group <code>rhods-admins</code> can edit the users listed in the group <code>rhods-users</code>. These makes the RHOAI Admins more self-sufficient, without giving them unneeded access.</p> <p>For expediency in the instructions, we are using the <code>oc</code> cli, but these can also be achieved using the OpenShift Web Console. We will assume that the user setting this up has admin privileges to the cluster.</p>"},{"location":"odh-rhoai/openshift-group-management/#creating-the-groups","title":"Creating the groups","text":"<p>Here, we will create the groups mentioned above. Note that you can alter those names if you want, but will then need to have the same alterations throughout the instructions.</p> <ol> <li>To create the groups:     <pre><code>oc adm groups new rhods-users\noc adm groups new rhods-admins\n</code></pre></li> <li>The above may complain about the group(s) already existing.</li> <li>To confirm both groups exist:     <pre><code>oc get groups | grep rhods\n</code></pre></li> <li>That should return:     <pre><code>bash-4.4 ~ $ oc get groups | grep rhods\nrhods-admins\nrhods-users\n</code></pre></li> <li>Both groups now exist</li> </ol>"},{"location":"odh-rhoai/openshift-group-management/#creating-clusterrole-and-clusterrolebinding","title":"Creating ClusterRole and ClusterRoleBinding","text":"<ol> <li>This will create a Cluster Role and a Cluster Role Binding:     <pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: update-rhods-users\nrules:\n  - apiGroups: [\"user.openshift.io\"]\n    resources: [\"groups\"]\n    resourceNames: [\"rhods-users\"]\n    verbs: [\"update\", \"patch\", \"get\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: rhods-admin-can-update-rhods-users\nsubjects:\n  - kind: Group\n    apiGroup: rbac.authorization.k8s.io\n    name: rhods-admins\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: update-rhods-users\nEOF\n</code></pre></li> <li>To confirm they were both succesfully created, run:     <pre><code>oc get ClusterRole,ClusterRoleBinding  | grep 'update\\-rhods'\n</code></pre></li> <li>You should see:     <pre><code>bash-4.4 ~ $ oc get ClusterRole,ClusterRoleBinding  | grep 'update\\-rhods'\nclusterrole.rbac.authorization.k8s.io/update-rhods-users\nclusterrolebinding.rbac.authorization.k8s.io/rhods-admin-can-update-rhods-users\n</code></pre></li> <li>You are pretty much done. You now just need to validate things worked.</li> </ol>"},{"location":"odh-rhoai/openshift-group-management/#add-some-users-as-rhods-admins","title":"Add some users as <code>rhods-admins</code>","text":"<p>To confirm this works, add a user to the <code>rhods-admin</code> group. In my example, I'll add <code>user1</code></p>"},{"location":"odh-rhoai/openshift-group-management/#capture-the-url-needed-to-edit-the-rhods-users-group","title":"Capture the URL needed to edit the <code>rhods-users</code> group","text":"<p>Since people who are not cluster admin won't be able to browse the list of groups, capture the URL that allows to control the membership of <code>rhods-users</code>.</p> <p>It should look similar to:</p> <p><code>https://console-openshift-console.apps.&lt;thecluster&gt;/k8s/cluster/user.openshift.io~v1~Group/rhods-users</code></p>"},{"location":"odh-rhoai/openshift-group-management/#ensure-that-rhods-admins-are-now-able-to-edit-rhods-users","title":"Ensure that <code>rhods-admins</code> are now able to edit <code>rhods-users</code>","text":"<p>Ask someone in the <code>rhods-admins</code> group to confirm that it works for them. (Remember to provide them with the URL to do so).</p> <p>They should be able to do so and successfully save their changes, as shown below:</p> <p></p>"},{"location":"odh-rhoai/openshift-group-management/#adding-kubeadmin-to-rhods-admins","title":"Adding kube:admin to <code>rhods-admins</code>","text":"<p>To add <code>kube:admin</code> user to the list of RHOAI Administrators, you will need to prefix it with b64:</p> <pre><code>apiVersion: user.openshift.io/v1\nkind: Group\nmetadata:\n  name: rhods-admins\nusers:\n- b64:kube:admin\n</code></pre>"},{"location":"odh-rhoai/rhoaibu-cluster-gitops/","title":"Fully GitOpsified implementation of a RHOAI platform","text":""},{"location":"odh-rhoai/rhoaibu-cluster-gitops/#what-is-the-rhoai-bu-cluster-repository","title":"What is the RHOAI BU Cluster Repository?","text":"<p>At Red Hat, we provide an internal OpenShift AI environment known as the RHOAI BU Cluster, where \"BU\" stands for our AI Business Unit. This cluster provides a centralized platform for experimentation, prototyping, and the scalable deployment of AI solutions across the company. Its operations are managed through the RHOAI BU Cluster GitOps Repository,  which implements a comprehensive GitOps approach using declarative configuration to maintain and evolve the infrastructure behind Red Hat OpenShift AI.</p> <p>What it manages:</p> <ul> <li>Two OpenShift clusters: development (<code>rhoaibu-cluster-dev</code>) and production (<code>rhoaibu-cluster-prod</code>)</li> <li>Complete AI/ML platform infrastructure using GitOps practices  </li> <li>Models as a Service (MaaS) platform with 15+ AI models</li> </ul> <p>Purpose:</p> <ul> <li>Working example of GitOps for AI infrastructure</li> <li>Reference architecture for organizations implementing AI/ML platforms</li> </ul> <p>Internal Service Notice</p> <p>This is an internal development and testing service. No Service Level Agreements (SLAs) or Service Level Objectives (SLOs) are provided or guaranteed. This service is not intended for production use cases or mission-critical applications.</p> <p>Unlike basic GitOps examples, this repository manages:</p> <ul> <li>Complete cluster lifecycle from development to production workloads</li> <li>Models as a Service (MaaS) platform with 15+ AI models (Granite, Llama, Mistral, Phi-4), 3Scale API gateway, Red Hat SSO authentication, self-service portal, and usage analytics for internal development and testing</li> <li>Multi-environment support with dev and prod cluster configurations  </li> <li>Advanced AI-specific components including GPU autoscaling, declarative model serving, and custom workbenches</li> <li>Advanced features like OAuth integration, RBAC, and certificate management</li> </ul> <p>Info</p> <p>This guide provides an overview of the RHOAI BU Cluster GitOps Repository, an implementation of GitOps for managing the RHOAIBU Cluster infrastructure at scale including OpenShift AI and MaaS.</p> <p>For foundational OpenShift AI &amp; GitOps concepts and object definitions, please refer to our Managing RHOAI with GitOps guide first.</p>"},{"location":"odh-rhoai/rhoaibu-cluster-gitops/#why-gitops-for-ai-infrastructure","title":"Why GitOps for AI Infrastructure?","text":"<p>GitOps provides unique advantages for AI/ML workloads that traditional infrastructure management approaches struggle to deliver. Rather than reinventing the wheel, this implementation builds upon the proven GitOps Catalog from Red Hat Community of Practice, providing battle-tested GitOps patterns and components as the foundation for our AI-specific infrastructure.</p> <p>\ud83d\udd04 Infrastructure Reproducibility: AI experiments require consistent environments. GitOps ensures your development, and production clusters are identical, eliminating \"works on my machine\" issues.</p> <p>\ud83d\udcca GPU Resource Management: Automated scaling and configuration of expensive GPU resources based on declarative policies, reducing costs while ensuring availability.</p> <p>\ud83d\ude80 Faster Iteration: Version-controlled infrastructure changes enable rapid experimentation with different configurations, operators, and serving runtimes.</p> <p>\ud83d\udee1\ufe0f Compliance &amp; Auditing: Complete audit trail of all infrastructure changes, critical for regulated industries deploying AI models.</p>"},{"location":"odh-rhoai/rhoaibu-cluster-gitops/#repository-architecture-hierarchy","title":"Repository Architecture &amp; Hierarchy","text":"<p>The repository follows a layered architecture that separates concerns while maintaining flexibility:</p> <pre><code>rhoaibu-cluster/\n\u251c\u2500\u2500 bootstrap/          # Initial cluster setup and GitOps installation\n\u251c\u2500\u2500 clusters/           # Environment-specific configurations\n\u2502   \u251c\u2500\u2500 base/          # Shared cluster resources\n\u2502   \u2514\u2500\u2500 overlays/      # Dev/prod environment customizations\n\u251c\u2500\u2500 components/        # Modular GitOps components\n\u2502   \u251c\u2500\u2500 argocd/       # ArgoCD projects and applications\n\u2502   \u251c\u2500\u2500 configs/      # Cluster-wide configurations\n\u2502   \u251c\u2500\u2500 instances/    # Operator instance configurations\n\u2502   \u251c\u2500\u2500 operators/    # Core Red Hat operators\n\u2502   \u2514\u2500\u2500 operators-extra/ # Community and third-party operators\n\u251c\u2500\u2500 demos/            # Demo applications and examples\n\u2514\u2500\u2500 docs/             # Documentation and development guides\n</code></pre>"},{"location":"odh-rhoai/rhoaibu-cluster-gitops/#core-components-deep-dive","title":"Core Components Deep Dive","text":""},{"location":"odh-rhoai/rhoaibu-cluster-gitops/#1-bootstrap-layer-bootstrap","title":"1. Bootstrap Layer (<code>bootstrap/</code>)","text":"<p>The bootstrap layer handles initial cluster setup and GitOps installation. Key features include:</p> <ul> <li>Cluster Installation: OpenShift cluster deployment with GPU machinesets</li> <li>GitOps Bootstrap: OpenShift GitOps operator installation and initial configuration</li> <li>Authentication Setup: Google OAuth integration for internal access</li> <li>Certificate Management: Let's Encrypt certificates with automatic renewal</li> </ul> <p>\ud83d\udcd6 Reference: Bootstrap Documentation</p>"},{"location":"odh-rhoai/rhoaibu-cluster-gitops/#2-operators-management-componentsoperators","title":"2. Operators Management (<code>components/operators/</code>)","text":"<p>Core Red Hat Operators managed through GitOps:</p> <ul> <li>Red Hat OpenShift AI (RHOAI)</li> <li>NVIDIA GPU Operator</li> <li>OpenShift Data Foundation</li> <li>OpenShift Cert Manager</li> <li>OpenShift Service Mesh</li> <li>OpenShift Serverless</li> </ul> <p>\ud83d\udcd6 Reference: Core Operators Documentation</p> <p>Community &amp; Third-Party Operators (<code>components/operators-extra/</code>):</p> <p>\ud83d\udcd6 Reference: Operators Documentation</p>"},{"location":"odh-rhoai/rhoaibu-cluster-gitops/#3-instance-configurations-componentsinstances","title":"3. Instance Configurations (<code>components/instances/</code>)","text":"<p>Configurations for each operator:</p> <ul> <li>RHOAI Instance: Complete DataScienceCluster configuration with custom accelerator profiles, workbenches, and dashboard settings</li> <li>GPU Management: NVIDIA operator policies optimized for AI workloads</li> <li>Storage: OpenShift Data Foundation instance for proving storage capabilities (including RWX) for OpenShift AI workloads.</li> <li>Certificates: Automated TLS certificate management for model serving endpoints</li> </ul> <p>\ud83d\udcd6 Reference: Operator Instances Documentation</p>"},{"location":"odh-rhoai/rhoaibu-cluster-gitops/#4-cluster-configurations-componentsconfigs","title":"4. Cluster Configurations (<code>components/configs/</code>)","text":"<p>Cluster-wide settings:</p> <ul> <li>Authentication: OAuth providers and RBAC configurations</li> <li>Autoscaling: GPU-optimized cluster autoscaler with support for several GPUs</li> <li>Console Customization: OpenShift and RHOAI console with AI-focused navigation</li> <li>Namespace Management: Project request templates and resource quotas</li> </ul> <p>\ud83d\udcd6 Reference: Cluster Configurations Documentation</p> <p>Comprehensive Configuration Details</p> <p>For detailed Kubernetes object definitions and advanced configuration options for model serving, data connections, accelerator profiles, and InferenceService objects, see our comprehensive Managing RHOAI with GitOps guide.</p>"},{"location":"odh-rhoai/rhoaibu-cluster-gitops/#key-gitops-workflows","title":"Key GitOps Workflows","text":""},{"location":"odh-rhoai/rhoaibu-cluster-gitops/#development-and-testing-workflow","title":"Development and Testing Workflow","text":"<p>The repository supports a complete development lifecycle:</p> <ol> <li>Fork and Branch: Developers create feature branches for infrastructure changes</li> <li>Local Testing: Kustomize allows local validation before deployment</li> <li>Dev Environment: Changes are tested in the development cluster first</li> <li>Production Promotion: Validated changes are promoted to production via GitOps</li> </ol> <p>\ud83d\udcd6 Reference: Development Workflow</p>"},{"location":"odh-rhoai/rhoaibu-cluster-gitops/#gpu-autoscaling-for-ai-workloads","title":"GPU Autoscaling for AI Workloads","text":"<p>GPU autoscaling is critical for AI workloads since GPUs are expensive resources that need to scale based on demand. The cluster automatically provisions GPU nodes from separate pools for different use cases (Tesla T4 for cost-effective training/inference, NVIDIA A10G for high-memory workloads and model serving), with configurations for both shared and private access patterns.</p> <p>\ud83d\udcd6 Reference: Autoscaling Configuration</p>"},{"location":"odh-rhoai/rhoaibu-cluster-gitops/#accelerator-profiles","title":"Accelerator Profiles","text":"<p>OpenShift AI Accelerator Profiles configurations for optimal GPU utilization:</p> <pre><code># Example accelerator profiles\n- NVIDIATesla T4: 16GB VRAM, ideal for inference and small model training\n- NVIDIA A10G: 24GB VRAM, optimized for large model training\n- NVIDIA L40: 24GB VRAM, optimized for large model training\n- NVIDIA L40s: 48GB VRAM, designed for multi-modal AI workloads\n- NVIDIA H100: 80GB VRAM, cutting-edge performance for large-scale training\n</code></pre> <p>\ud83d\udcd6 Reference: Accelerator Profiles Configuration</p>"},{"location":"odh-rhoai/rhoaibu-cluster-gitops/#ai-specific-components","title":"AI-Specific Components","text":""},{"location":"odh-rhoai/rhoaibu-cluster-gitops/#models-as-a-service-maas","title":"Models as a Service (MaaS)","text":"<p>The RHOAI BU Cluster serves as internal infrastructure hosting a complete Models as a Service (MaaS) platform. MaaS runs entirely on top of this GitOps-managed cluster infrastructure, providing AI model serving capabilities for development and testing purposes internally to all Red Hat Employees.</p> <p>The MaaS implementation leverages the Models as a Service repository, which demonstrates how to set up 3Scale and Red Hat SSO in front of models served by OpenShift AI.</p> <p>Internal Service Notice</p> <p>This is an internal development and testing service. No Service Level Agreements (SLAs) or Service Level Objectives (SLOs) are provided or guaranteed. This service is not intended for production use cases or mission-critical applications.</p>"},{"location":"odh-rhoai/rhoaibu-cluster-gitops/#declarative-model-serving-as-code","title":"Declarative Model Serving as Code","text":"<pre><code># MaaS Configuration Structure\ncomponents/configs/maas/\n\u251c\u2500\u2500 model-serving/base/              # Model deployment configurations\n\u2502   \u251c\u2500\u2500 granite-3.3-8b-instruct.yaml\n\u2502   \u251c\u2500\u2500 llama-4-scout.yaml\n\u2502   \u251c\u2500\u2500 llama-3.2-3b.yaml\n\u2502   \u251c\u2500\u2500 mistral-small-24b.yaml\n\u2502   \u251c\u2500\u2500 phi-4.yaml\n\u2502   \u251c\u2500\u2500 nomic-embed-text-v1-5.yaml\n\u2502   \u251c\u2500\u2500 docling.yaml\n\u2502   \u251c\u2500\u2500 sdxl-custom-runtime.yaml\n\u2502   \u2514\u2500\u2500 serving-runtimes/\n\u2514\u2500\u2500 3scale-config/base/              # API management configurations\n    \u251c\u2500\u2500 granite-3-3-8b-instruct/\n    \u251c\u2500\u2500 llama-4-scout/\n    \u251c\u2500\u2500 llama-3-2-3b/\n    \u251c\u2500\u2500 mistral-small-24b/\n    \u251c\u2500\u2500 phi-4/\n    \u2514\u2500\u2500 [per-model configs for authentication, rate limiting, documentation]\n</code></pre> <p>The MaaS platform organizes configurations for 15+ models with dedicated GitOps configurations for both OpenShift AI model serving and 3Scale API management.</p> <p>Available Model Types:</p> <ul> <li>Large Language Models: Llama-4-Scout, Granite 3.3 8B, Llama 3.2 3B, Mistral Small 24B, Phi-4</li> <li>Embedding Models: Nomic Embed Text v1.5 for semantic search and RAG applications</li> <li>Vision Models: Granite Vision 3.2 2B, Qwen2.5 VL 7B for multimodal AI</li> <li>Specialized Models: Document processing (Docling), safety checking, image generation (SDXL)</li> <li>Lightweight Models: TinyLlama 1.1B (running on CPU)</li> </ul> <p>\ud83d\udcd6 Reference: MaaS Configurations</p> <p>This GitOps approach ensures:</p> <ul> <li>Consistent Deployments: Identical model configurations across dev/prod environments</li> <li>Version Control: Full audit trail of model deployment changes  </li> <li>Easy Rollbacks: Quick reversion to previous model versions</li> <li>Automated Scaling: GPU autoscaling based on model demand</li> </ul>"},{"location":"odh-rhoai/rhoaibu-cluster-gitops/#custom-ai-workbenches","title":"Custom AI Workbenches","text":"<p>Pre-configured Jupyter environments optimized for specific AI tasks:</p> <ul> <li>PyTorch 2.1.4 with CUDA support for deep learning</li> <li>Elyra with R for data science workflows  </li> <li>AnythingLLM for conversational AI development</li> <li>Docling for document processing pipelines</li> <li>Custom tools for specialized AI workflows</li> </ul> <p>\ud83d\udcd6 Reference: Custom Workbenches Configuration</p> <p>While this implementation serves as a practical reference architecture, organizations can extend it with additional features based on their specific requirements.</p> <p>Next Steps</p> <ul> <li>Review our foundational GitOps guide for object-level details</li> <li>Explore the RHOAI BU Cluster Repository for complete implementation examples</li> <li>Check out the AI on OpenShift examples for application-level patterns</li> </ul> <p>This repository demonstrates that GitOps isn't just a deployment strategy\u2014it's a comprehensive approach to managing the complex, rapidly-evolving infrastructure requirements of modern AI platforms.</p>"},{"location":"odh-rhoai/secret-management/","title":"Managing Secrets in an AI Platform","text":""},{"location":"odh-rhoai/secret-management/#introduction","title":"Introduction","text":"<p>At Red Hat, we provide an internal OpenShift AI cluster that serves as a unified platform for experimentation, prototyping, and scalable deployment of AI solutions. Designed to support a potential user base of over 19,000 associates, the platform offers a range of capabilities\u2014including granular role-based access control (RBAC), GPU auto-scaling for efficient resource management, hosting models from the Granite, Llama, Mistral, and DeepSeek families, as well as specialized models for vision, embeddings, and safety filtering. It also supports demo products like Models as a Service and the Chat with Your Documentation RAG implementation. This variety enables teams across Red Hat to explore diverse AI workloads and build solutions tailored to their specific use cases by using OpenShift AI.</p> <p>Like any production-grade platform, supporting these capabilities at scale requires more than just compute resources. It also requires solid engineering practices, including the secure management of sensitive configuration data\u2014such as cloud credentials and authorization tokens used in platform setup.</p> <p>Since day one, we\u2019ve managed the cluster lifecycle using GitOps. However, the presence of these sensitive values meant that we initially had to keep our configuration repository private. While this worked operationally, it limited our ability to share the implementation more broadly with others.</p> <p>To address this, we adopted a secret management solution. This enabled us to decouple secrets from the GitOps-managed resources and paved the way for securely opening up parts of the platform\u2019s configuration. You can now explore the repository here to see how we run this AI platform in a secure and scalable way.</p> <p>In this post, we\u2019ll walk through the high level structure of the repository we\u2019ve opened up, what you can find inside, and how it reflects the way we run and scale OpenShift AI internally. We\u2019ll also take a closer look at the secret management approach we adopted; why we chose External Secrets Operator (ESO), how it fits into our GitOps workflow, and the lessons we learned along the way.</p>"},{"location":"odh-rhoai/secret-management/#whats-in-the-box","title":"What's In the Box?","text":"<p>The GitHub repository captures how we run production-grade AI infrastructure at scale\u2014offering reusable patterns, modular design, and automation strategies. Here's a quick overview of what you\u2019ll find inside:</p> <ul> <li> <p>Declarative GitOps configurations for managing AI/ML infrastructure</p> </li> <li> <p>Customizations for OpenShift AI, including workbenches and model serving</p> </li> <li> <p>GPU sharing, time-slicing, and autoscaling across different types of GPUs</p> </li> <li> <p>Model deployments that power Model-as-a-Service platform</p> </li> <li> <p>Environment overlays and promotion workflows for dev/prod separation</p> </li> <li> <p>Security, observability, and cost-optimization practices built in</p> </li> </ul> <p>Sharing this repo publicly meant revisiting how we manage sensitive values. Because while transparency is great, credentials shouldn\u2019t live in version control as plain text. They should be managed as code, but in a secure and auditable way. To achieve this, we adopted a secret management tool that integrates seamlessly with our GitOps flow.</p>"},{"location":"odh-rhoai/secret-management/#what-is-external-secrets-operator","title":"What is External Secrets Operator?","text":"<p>External Secrets Operator is a Kubernetes operator that integrates external secret management systems like AWS Secrets Manager, HashiCorp Vault, GCP Secret Manager, and Azure Key Vault with Kubernetes. It allows you to securely inject secrets from these external systems into your Kubernetes clusters. We use it to pull secrets from AWS Secrets Manager into our OpenShift AI clusters securely and automatically.</p>"},{"location":"odh-rhoai/secret-management/#why-we-chose-external-secrets-operator","title":"Why We Chose External Secrets Operator?","text":"<p>Several factors influenced our decision to use External Secrets Operator:</p> <ol> <li> <p>Native Kubernetes Integration: ESO follows the Kubernetes operator pattern, making it easy to integrate with our OpenShift clusters.</p> </li> <li> <p>AWS Integration: With our infrastructure running on AWS, ESO's native support for AWS Secrets Manager was a good fit.</p> </li> <li> <p>GitOps Friendly: ESO works well with our GitOps workflow using Argo CD, allowing us to manage secret references declaratively in Git without exposing sensitive data.</p> </li> </ol>"},{"location":"odh-rhoai/secret-management/#our-implementation","title":"Our Implementation","text":"<p>First, let's look at a concrete example of how we use External Secrets in our cluster:</p> <p>Database Credentials for Model Registry This example shows how we fetch the database credentials for our Model Registry that is stored in AWS Secrets Manager, demonstrating how one ExternalSecret can map multiple properties from a secret store.</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n name: registry-db-secrets\n namespace: rhoai-model-registries\nspec:\n refreshInterval: 1h\n secretStoreRef:\n   name: rhoaibu-external-store\n   kind: ClusterSecretStore\n target:\n   name: registry-db-secrets\n   creationPolicy: Owner\n data:\n   - secretKey: MYSQL_ROOT_PASSWORD\n     remoteRef:\n       key: model-registries-db-credentials\n       property: database-password\n   - secretKey: MYSQL_USER_NAME\n     remoteRef:\n       key: model-registries-db-credentials\n       property: database-user\n</code></pre>"},{"location":"odh-rhoai/secret-management/#architecture","title":"Architecture","text":"<p>We implemented External Secrets Operator using a GitOps approach with the following components:</p> <ol> <li> <p>Operator Installation: We use the Red Hat Community of Practice (CoP)'s GitOps Catalog to deploy the operator:</p> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n- https://github.com/redhat-cop/gitops-catalog/external-secrets-operator/operator/overlays/stable\n</code></pre> </li> <li> <p>AWS Secrets Manager Integration: We configured a <code>ClusterSecretStore</code> that connects to AWS Secrets Manager:</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ClusterSecretStore\nmetadata:\n  name: rhoaibu-external-store\nspec:\n  provider:\n    aws:\n      service: SecretsManager\n      region: us-west-2\n</code></pre> </li> <li> <p>Namespace Scoping: For security, we explicitly define which namespaces can access the external secrets:</p> <pre><code>conditions:\n- namespaces:\n    - \"cert-manager\"\n    - \"openshift-config\"\n    - \"rhoai-model-registries\"\n</code></pre> </li> </ol>"},{"location":"odh-rhoai/secret-management/#benefits-weve-seen","title":"Benefits We've Seen","text":"<ol> <li>Improved Security: Secrets are stored securely in AWS Secrets Manager, separate from our application code.</li> <li>Simplified Management: One central place (AWS Secrets Manager) to manage all our secrets.</li> <li>GitOps Compatible: WWe can declaratively manage secret references in Git while keeping actual secrets securely stored in AWS.</li> <li>Automated Syncing: Secrets are automatically synchronized between AWS and our clusters.</li> </ol>"},{"location":"odh-rhoai/secret-management/#alternatives-we-considered","title":"Alternatives We Considered","text":"<ol> <li>Sealed Secrets: While powerful, it didn't offer the same level of integration with external secret managers.</li> <li>Vault Operator: HashiCorp Vault was more complex to set up and maintain compared to using AWS Secrets Manager.</li> <li>Native Kubernetes Secrets: Since they are only base64 encoding, they lack the security and management features we needed.</li> </ol>"},{"location":"odh-rhoai/secret-management/#challenges-and-solutions","title":"Challenges and Solutions","text":"<ol> <li>Initial Setup: Required detailed IAM role configurations to ensure secure access to secrets.</li> <li>Multi-Region Support: Solved by using environment-specific patches for different AWS regions.</li> </ol>"},{"location":"odh-rhoai/secret-management/#good-practices-we-follow","title":"Good Practices We Follow","text":"<ol> <li>Namespace Isolation: Strictly control which namespaces can access secrets</li> <li>Minimal Access: Use specific IAM roles with least privilege</li> <li>Version Control: Maintain secret configurations in Git while keeping sensitive data in AWS</li> <li>Environment Separation: Different configurations for dev and prod environments</li> </ol>"},{"location":"odh-rhoai/secret-management/#conclusion","title":"Conclusion","text":"<p>External Secrets Operator has proven to be a robust solution for our secret management needs. It provides the right balance of security, ease of use, and integration capabilities. Most importantly, it allowed us to open source our entire cluster setup, from installation to Day 2 operations, while keeping sensitive data secure in AWS Secrets Manager. This separation of configuration enables us to share our implementation publicly, allowing others to learn from and build upon our work while maintaining the security of our credentials and sensitive data.</p>"},{"location":"odh-rhoai/single-stack-serving-certificate/","title":"Use Existing OpenShift Certificate for Single Stack Serving","text":"<p>By default, the Single Stack Serving in Openshift AI uses a self-signed certificate generated at installation for the endpoints that are created when deploying a server. This can be counter-intuitive because if you already have certificates configured on your OpenShift cluster, they will be used by default for other types of endpoints like Routes.</p> <p>The installation procedure for the Single Stack Serving available here (section 4.vi) states that you can provide your own certificate instead of using a self-signed one.</p> <p>This following procedure explains how to use the same certificate that you already have for your OpenShift cluster.</p>"},{"location":"odh-rhoai/single-stack-serving-certificate/#procedure","title":"Procedure","text":"<ul> <li>Configure OpenShift to use a valid certificate for accessing the Console and in general for any created Route (normally, this is something that was already done).</li> <li>From the openshift-ingress namespace, copy the content of a Secret whose name includes \"certs\". For example <code>rhods-internal-primary-cert-bundle-secret</code> or <code>ingress-certs-....</code>. The content of the Secret (data) should contain two items, tls.cert and tls.key. They are the certificate and key that are used for all the OpenShift Routes.</li> <li> <p>Cleanup the YAML to just keep the relevant content. It should look like this (the name of the secret will be different, it's normally tied to your cluster name):</p> <pre><code>kind: Secret\napiVersion: v1\nmetadata:\nname: rhods-internal-primary-cert-bundle-secret\ndata:\ntls.crt: &gt;-\n    LS0tLS1CRUd...\ntls.key: &gt;-\n    LS0tLS1CRUd...\ntype: kubernetes.io/tls\n</code></pre> </li> <li> <p>Apply this YAML into the istio-system namespace (basically, copy the Secret from one namespace to the other).</p> </li> <li> <p>Following the Single Stack Serving installation procedure, in your DSC Configuration, refer to this secret for the kserve configuration (don\u2019t forget to change the secretName for the one you just created):</p> <pre><code>kserve:\ndevFlags: {}\nmanagementState: Managed\nserving:\n    ingressGateway:\n    certificate:\n        secretName: rhods-internal-primary-cert-bundle-secret\n        type: Provided\n    managementState: Managed\n    name: knative-serving\n</code></pre> </li> </ul> <p>Your Model Servers will now be deployed with the same certificate as you are using for OpenShift Routes. If this is a trusted certificate, your Endpoints will be accessible using SSL without having to ignore error messages or create special configurations.</p>"},{"location":"odh-rhoai/single-stack-serving-certificate/#other-workarounds","title":"Other Workarounds","text":"<p>If the above method does not work or you don't want or can't do any modification, you can try to bypass the certificate verification in your application or code. Depending on the library used, there are different solutions.</p>"},{"location":"odh-rhoai/single-stack-serving-certificate/#using-langchain-with-openai-api-compatible-runtimes","title":"Using Langchain with OpenAI API compatible runtimes","text":"<p>The underlying library used for communication by the base OpenAI module of Langchain is <code>httpx</code>. You can directly specify <code>httpx</code> settings when you instantiate the llm object in Langchain. With the following settings on the last two lines of this example, any certificate error will be ignored:</p> <pre><code>import httpx\n# LLM definition\nllm = VLLMOpenAI(\n    openai_api_key=\"EMPTY\",\n    openai_api_base= f\"{inference_server_url}/v1\",\n    model_name=\"/mnt/models/\",\n    top_p=0.92,\n    temperature=0.01,\n    max_tokens=512,\n    presence_penalty=1.03,\n    streaming=True,\n    callbacks=[StreamingStdOutCallbackHandler()]\n    async_client=httpx.AsyncClient(verify=False),\n    http_client=httpx.Client(verify=False)\n)\n</code></pre>"},{"location":"odh-rhoai/stable_diffusion_safety_checker/","title":"Stable Diffusion Safety Checker","text":""},{"location":"odh-rhoai/stable_diffusion_safety_checker/#image-guardrails","title":"Image Guardrails","text":"<p>When generating images, you want to be able to check whether your model's outputs are safe to display and block them if they are not. This is where image generation guardrails come into play. By using image models trained on unsafe content, we can classify the output images to ensure they comply with our guidelines. Without these kinds of safeguards, users would be able to generate unethical and harmful content from our models.  </p> <p></p>"},{"location":"odh-rhoai/stable_diffusion_safety_checker/#key-risks-when-not-using-guardrails","title":"Key risks when not using guardrails:","text":"<ul> <li>Non-Consensual or Exploitative Imagery \u2013 The ability to create non-consensual deepfake content, including explicit images of real people without their permission, leading to severe ethical and legal consequences.</li> <li>Harmful and Inappropriate Content \u2013 The generation of violent, explicit, or illegal material.</li> <li>Intellectual Property and Copyright Infringement \u2013 The replication of or close resemblance to copyrighted content, leading to potential legal and ethical issues.</li> </ul>"},{"location":"odh-rhoai/stable_diffusion_safety_checker/#stable-diffusion-safety-checker_1","title":"Stable Diffusion Safety Checker","text":"<p>The Stable Diffusion Safety Checker is one of these image guardrails, specifically built for analyzing the outputs of diffusion models. It allows application developers to check any images generated by a Stable Diffusion model before displaying them to end-users.</p> <p>The model it uses is the Stable Diffusion Safety Checker which is based on a fine-tuned CLIP model under the MIT License.  </p>"},{"location":"odh-rhoai/stable_diffusion_safety_checker/#how-does-it-work","title":"How does it work?","text":"<p>The Stable Diffusion Safety Checker compares embeddings (a list of values that represent the image) generated by the model with predefined embeddings that represent harmful images. If the output embedding is too close to any of the predefined embeddings, the model will output <code>True</code>, meaning that the image may be harmful. Otherwise, it will output <code>False</code>. You can find the full code in this HuggingFace repo.</p> <p> Reference [contains NSFW topics]: https://arxiv.org/pdf/2210.04610</p>"},{"location":"odh-rhoai/stable_diffusion_safety_checker/#how-can-you-customize-it","title":"How can you customize it?","text":"<p>Since the model simply evaluates how close an image is to different embeddings, a straightforward way to customize it is to add a new predefined embedding of an image you ran through the model. This will cause the model to return <code>True</code> for all images that are similar to the new embedding. However, note that while this method works, it has limitations in quality. For improved accuracy when adding new categories, fine-tuning the model is recommended.</p>"},{"location":"odh-rhoai/stable_diffusion_safety_checker/#examples","title":"Examples","text":"<p>You can find a notebook with an example of using and customizing the Safety Checker. There is also a InferenceService template (make sure to add a GPU if you want it to run faster) and a custom runtime to serve with KServe, as well as a simple request notebook you can use to send requests to the served model.  </p>"},{"location":"odh-rhoai/stable_diffusion_safety_checker/#more-model-details","title":"More model details","text":"<p>The original model\u2019s categories were obfuscated to prevent people from bypassing them. However, they have since been reverse-engineered. \u26a0\ufe0f Note: The following paper and example expose NSFW categories. Enter at your own risk. \u26a0\ufe0f - Paper describing the model in depth. - Example where the categories are revealed in the model response.</p>"},{"location":"odh-rhoai/from-zero-to-workbench/using-cli/","title":"From Zero To Workbench using the CLI","text":"<p>In this repo, you will find all the straightforward instructions to quickly deploy OpenShift AI only using a CLI or automation.</p> <p>Detailed instructions, YAMLs, code, all is there to get you quickly started!</p> <p>Note</p> <p>This documentation was produced working off of mainly OpenShift v4.14 and OpenShift AI 2.7. The artifacts directory contains sample configuration files which are used throughout this repo to demonstrate various concepts. While an effort will be made to ensure that these artifacts stay up to date, there is a possibility that they will not always work as intended.</p> <p>Suggestions and pull requests are welcome to maintain this content up to date!</p>"},{"location":"odh-rhoai/from-zero-to-workbench/using-developer-hub/","title":"OpenShift AI on Developer Hub","text":"<p>In this repo, you will find a Backstage Golden Path Template for a Red Hat OpenShift AI Data science project. Detailed instructions, examples, all is there to get you quickly started!</p> <p>Note</p> <p>This documentation is a work in progress that provides recipes for some components only.</p> <p>Suggestions and pull requests are welcome to maintain this content up to date and make it evolve!</p>"},{"location":"odh-rhoai/from-zero-to-workbench/using-ui/","title":"From Zero To Workbench using the GUI","text":"<p>In this repo, you will find all the straightforward instructions to quickly deploy OpenShift AI using mainly the OpenShift Console UI.</p> <p>Detailed instructions, screenshots, examples, all is there to get you quickly started!</p> <p>Note</p> <p>This documentation was produced working off of mainly OpenShift v4.14 and OpenShift AI 2.7. The artifacts directory contains sample configuration files which are used throughout this repo to demonstrate various concepts. While an effort will be made to ensure that these artifacts stay up to date, there is a possibility that they will not always work as intended.</p> <p>Suggestions and pull requests are welcome to maintain this content up to date!</p>"},{"location":"odh-rhoai/kueue-preemption/readme/","title":"Introduction","text":"<p>In this repo, we will demostrate how to use quota allocation with Kueue with preemption.</p>"},{"location":"odh-rhoai/kueue-preemption/readme/#overview","title":"Overview","text":"<p>In this example, there are 2 teams that work in their own namespace:</p> <ol> <li>Team A and B belongs to the same cohort</li> <li>Both teams share a quota</li> <li>Team A has access to GPU while team B does not</li> <li>Team A has higher priority and can prempt others</li> </ol>"},{"location":"odh-rhoai/kueue-preemption/readme/#kueue-configuration","title":"Kueue Configuration","text":"<p>There are 2 <code>ResourceFlavor</code> that manages the CPU/Memory and GPU resources. The GPU <code>ResourceFlavor</code> tolerates nodes that have been tainted. </p> <p>Both teams have their invididual cluster queue that is associated with their respective namespace.</p> Name CPU Memory (GB) GPU Team A cq 0 0 4 Team B cq 0 0 0 Shared cq 10 64 0 <p>A local queue is defined in their namespace to associate the cluster queue. E.g.</p> <pre><code>apiVersion: kueue.x-k8s.io/v1beta1\nkind: LocalQueue\nmetadata:\n  name: local-queue\n  namespace: team-a\nspec:\n  clusterQueue: team-a-cq\n</code></pre> <p>When a Ray cluster is defined, it is submitted to the local queue with the associated priority.</p> <pre><code>apiVersion: ray.io/v1\nkind: RayCluster\nmetadata:\n  labels:  \n    kueue.x-k8s.io/queue-name: local-queue\n    kueue.x-k8s.io/priority-class: dev-priority\n</code></pre>"},{"location":"odh-rhoai/kueue-preemption/readme/#ray-cluster-configuration","title":"Ray cluster configuration","text":"<p>The shared quota is only up to 10 CPU for both teams.</p> Name CPU Memory (GB) GPU Team A 10 24 4 Team B 6 16 0"},{"location":"odh-rhoai/kueue-preemption/readme/#premption","title":"Premption","text":"<p>Team A cluster queue has preemption defined that can <code>borrowWithinCohort</code> of a lower priority which Team B belongs to.</p> <pre><code>apiVersion: kueue.x-k8s.io/v1beta1\nkind: ClusterQueue\nmetadata:\n  name: team-a-cq\nspec:\n  preemption:\n    reclaimWithinCohort: Any\n    borrowWithinCohort:\n      policy: LowerPriority\n      maxPriorityThreshold: 100\n    withinClusterQueue: Never\n</code></pre> <p>Team A will preempt team B because it has insufficient resources to run. </p>"},{"location":"odh-rhoai/kueue-preemption/readme/#setting-up-the-demo","title":"Setting Up the Demo","text":"<ol> <li> <p>Install OpenShift AI Operator</p> </li> <li> <p>Ensure there is at least 1 worker node that has a 4 GPUs. On AWS, this can be a p3.8xlarge instance.</p> </li> <li> <p>Taint the GPU node     <pre><code>  oc adm taint nodes &lt;gpu-node&gt; nvidia.com/gpu=Exists:NoSchedule\n</code></pre></p> </li> <li> <p>Git clone the repo</p> <pre><code>git clone https://github.com/opendatahub-io-contrib/ai-on-openshift\ncd ai-on-openshift/docs/odh-rhoai/kueue-preemption\n</code></pre> </li> <li> <p>Run the makefile target to setup the example. This will setup 2 namespaces: team-a and team-b.</p> <pre><code>make setup-kueue-examples\n</code></pre> </li> </ol> <p>To teardown the example, you can use: <pre><code>make teardown-kueue-preemption\n</code></pre></p> <p>Warning</p> <p>The setup script will delete all clusterqueues and resourceflavors in the cluster.</p>"},{"location":"odh-rhoai/kueue-preemption/readme/#running-the-example","title":"Running the example","text":"<ol> <li> <p>Create a ray cluster for team B. Wait for the cluster to be running.     <pre><code>oc create -f team-b-ray-cluster-dev.yaml\n</code></pre></p> <pre><code>$ oc get rayclusters -A\nNAMESPACE   NAME             DESIRED WORKERS   AVAILABLE WORKERS   CPUS   MEMORY   GPUS   STATUS   AGE\nteam-b      raycluster-dev   2                 2                   6      16G      0      ready    70s\n\n$ oc get po -n team-b\nNAME                                           READY   STATUS    RESTARTS   AGE\nraycluster-dev-head-zwfd8                      2/2     Running   0          45s\nraycluster-dev-worker-small-group-test-4c85h   1/1     Running   0          43s\nraycluster-dev-worker-small-group-test-5k9j5   1/1     Running   0          43s\n</code></pre> </li> <li> <p>Create a Ray cluster for team A.      <pre><code>oc create -f team-a-ray-cluster-prod.yaml\n</code></pre></p> </li> <li> <p>Observe team B cluster is suspended and team A cluster is running because of preemption. This may take a few seconds to happen. </p> <pre><code>$ oc get rayclusters -A\nNAMESPACE   NAME              DESIRED WORKERS   AVAILABLE WORKERS   CPUS   MEMORY   GPUS   STATUS      AGE\nteam-a      raycluster-prod   2                 2                   10     24G      4      ready       75s\nteam-b      raycluster-dev    2                                     6      16G      0      suspended   3m46s\n</code></pre> </li> </ol>"},{"location":"patterns/bucket-notifications/bucket-notifications/","title":"Bucket Notifications","text":""},{"location":"patterns/bucket-notifications/bucket-notifications/#description","title":"Description","text":"<p>The Rados Gateway (RGW) component of Ceph provides Object Storage through an S3-compatible API on all Ceph implementations: OpenShift Data Foundation and its upstream version Rook-Ceph, Red Hat Ceph Storage, Ceph,\u2026\u200b</p> <p>Bucket notifications provide a mechanism for sending information from the RGW when certain events are happening on a bucket. Currently, notifications can be sent to: HTTP, AMQP0.9.1 and Kafka endpoints.</p> <p>From a data engineering point of view, bucket notifications allow to create an event-driven architecture, where messages (instead of simply log entries) can be sent to various processing components or event buses whenever something is happening on the object storage: object creation, deletion, with many fine-grained settings available.</p>"},{"location":"patterns/bucket-notifications/bucket-notifications/#use-cases","title":"Use cases","text":""},{"location":"patterns/bucket-notifications/bucket-notifications/#application-taking-actions-on-the-objects","title":"Application taking actions on the objects","text":"<p>As part of an event-driven architecture, this pattern can be used to trigger an application to perform an action following the storage event. An example could be the automated processing of a new image that has just been uploaded to the object storage (analysis, resizing,\u2026\u200b). Paired with Serverless functions this becomes a pretty efficient architecture compared to having an application constantly monitoring or polling the storage, or to have to implement this triggering process in the application interacting with the storage. This loosely-coupled architecture also gives much more agility for updates, technology evolution,\u2026\u200b</p>"},{"location":"patterns/bucket-notifications/bucket-notifications/#external-monitoring-systems","title":"External monitoring systems","text":"<p>The events sent by the RGW are simple messages containing all the metadata relevant to the event and the object. So it can be an excellent source of information for a monitoring system. For example if you want to keep a trace or send an alert whenever a specific type of file, or with a specific name, is uploaded or deleted from the storage.</p>"},{"location":"patterns/bucket-notifications/bucket-notifications/#implementations-examples","title":"Implementations examples","text":"<p>This pattern is implemented in the XRay pipeline demo</p>"},{"location":"patterns/bucket-notifications/bucket-notifications/#how-does-it-work","title":"How does it work?","text":""},{"location":"patterns/bucket-notifications/bucket-notifications/#characteristics","title":"Characteristics","text":"<ul> <li>Notifications are sent directly from the RGW on which the event happened to an external endpoint.</li> <li>Pluggable endpoint architecture:<ul> <li>HTTP/S</li> <li>AMQP 0.9.1</li> <li>Kafka</li> <li>Knative</li> </ul> </li> </ul>"},{"location":"patterns/bucket-notifications/bucket-notifications/#data-model","title":"Data Model","text":"<ul> <li>Topics contain the definition of a specific endpoint in \u201cpush mode\u201d</li> <li>Notifications tie topics with buckets, and may also include filter definition on the events</li> </ul>"},{"location":"patterns/bucket-notifications/bucket-notifications/#configuration","title":"Configuration","text":"<p>This configuration shows how to create a notification that will send a message (event) to a Kafka topic when a new object is created in a bucket.</p>"},{"location":"patterns/bucket-notifications/bucket-notifications/#requirements","title":"Requirements","text":"<ul> <li>Access to a Ceph/ODF/RHCS installation with the RGW deployed.</li> <li>Endpoint address (URL) for the RGW.</li> <li>Credentials to connect to the RGW:<ul> <li>AWS_ACCESS_KEY_ID</li> <li>AWS_SECRET_ACCESS_KEY</li> </ul> </li> </ul> <p>Note</p> <p>As Ceph implements an S3-Compatible API to access Object Storage, standard naming for variables or procedures used with S3 were retained to stay coherent with examples, demos or documentation related to S3. Therefore the AWS prefix in the previous variables.</p>"},{"location":"patterns/bucket-notifications/bucket-notifications/#topic-creation","title":"Topic Creation","text":"<p>A topic is the definition of a specific endpoint. It must be created first.</p>"},{"location":"patterns/bucket-notifications/bucket-notifications/#method-1-raw-configuration","title":"Method 1: \"RAW\" configuration","text":"<p>As everything is done through the RGW API, you can query it directly. To be fair, this method is almost never used (unless there is no SDK or S3 tool for your environment) but gives a good understanding of the process.</p> <p>Example for a Kafka Endpoint:</p> <pre><code>POST\nAction=CreateTopic\n&amp;Name=my-topic\n&amp;push-endpoint=kafka://my-kafka-broker.my-net:9999\n&amp;Attributes.entry.1.key=verify-ssl\n&amp;Attributes.entry.1.value=true\n&amp;Attributes.entry.2.key=kafka-ack-level\n&amp;Attributes.entry.2.value=broker\n&amp;Attributes.entry.3.key=use-ssl\n&amp;Attributes.entry.3.value=true\n&amp;Attributes.entry.4.key=OpaqueData\n&amp;Attributes.entry.4.value=https://s3-proxy.my-zone.my-net\n</code></pre> <p>Note</p> <p>The authentication part is not detailed here as the mechanism is pretty convoluted, but it is directly implemented in most API development tools, like Postman.</p> <p>The full reference for the REST API for bucket notifications is available here.</p>"},{"location":"patterns/bucket-notifications/bucket-notifications/#method-2-python-aws-sdk","title":"Method 2: Python + AWS SDK","text":"<p>As the creator of the S3 API, AWS is providing SDKs for the main languages to interact with it. Thanks to this compatibility, you can use those SDKs to interact with Ceph in the same way. For Python, the library to interact with AWS services is called boto3.</p> <p>Example for a Kafka Endpoint:</p> <pre><code>import boto3\nsns = boto3.client('sns',\n                endpoint_url = endpoint_url,\n                aws_access_key_id = aws_access_key_id,\n                aws_secret_access_key= aws_secret_access_key,\n                region_name='default',\n                config=botocore.client.Config(signature_version = 's3'))\n\nattributes = {}\nattributes['push-endpoint'] = 'kafka://my-cluster-kafka-bootstrap:9092'\nattributes['kafka-ack-level'] = 'broker'\n\ntopic_arn = sns.create_topic(Name=my-topic, Attributes=attributes)['TopicArn']\n</code></pre>"},{"location":"patterns/bucket-notifications/bucket-notifications/#notification-configuration","title":"Notification Configuration","text":"<p>The notification configuration will \"tie\" a bucket with a topic.</p>"},{"location":"patterns/bucket-notifications/bucket-notifications/#method-1-raw-configuration_1","title":"Method 1: \"RAW\" configuration","text":"<p>As previously, you can directly query the RGW REST API. This is done through an XML formatted payload that is sent with a PUT command.</p> <p>Example for a Kafka Endpoint:</p> <pre><code>PUT /my-bucket?notification HTTP/1.1\n\n&lt;NotificationConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"&gt;\n    &lt;TopicConfiguration&gt;\n        &lt;Id&gt;my-notification&lt;/Id&gt;\n        &lt;Topic&gt;my-topic&lt;/Topic&gt;\n        &lt;Event&gt;s3:ObjectCreated:*&lt;/Event&gt;\n        &lt;Event&gt;s3:ObjectRemoved:DeleteMarkerCreated&lt;/Event&gt;\n    &lt;/TopicConfiguration&gt;\n    &lt;TopicConfiguration&gt;\n...\n    &lt;/TopicConfiguration&gt;\n&lt;/NotificationConfiguration&gt;\n</code></pre> <p>Again, the full reference for the REST API for bucket notifications is available here.</p>"},{"location":"patterns/bucket-notifications/bucket-notifications/#method-2-python-aws-sdk_1","title":"Method 2: Python + AWS SDK","text":"<p>Example for a Kafka Endpoint:</p> <pre><code>import boto3\ns3 = boto3.client('s3',\n                endpoint_url = endpoint_url,\n                aws_access_key_id = aws_access_key_id,\n                aws_secret_access_key = aws_secret_access_key,\n                region_name = 'default',\n                config=botocore.client.Config(signature_version = 's3'))\n\nbucket_notifications_configuration = {\n            \"TopicConfigurations\": [\n                {\n                    \"Id\": 'my-id',\n                    \"TopicArn\": 'arn:aws:sns:s3a::my-topic',\n                    \"Events\": [\"s3:ObjectCreated:*\"]\n                }\n            ]\n        }\n\ns3.put_bucket_notification_configuration(Bucket = bucket_name,\n        NotificationConfiguration=bucket_notifications_configuration)\n</code></pre>"},{"location":"patterns/bucket-notifications/bucket-notifications/#filters","title":"Filters","text":"<p>Although a notification is specific to a bucket (and you can have multiple configurations on one bucket), you may want that it does not apply to all the objects from this bucket. For example you want to send an event when an image is uploaded, but not do anything it\u2019s another type of file. You can do this with filters! And not only on the filename, but also on the tags associated to it in its metadata.</p> <p>Filter examples, on keys or tags:</p> <pre><code>&lt;Filter&gt;\n    &lt;S3Key&gt;\n        &lt;FilterRule&gt;\n         &lt;Name&gt;regex&lt;/Name&gt;\n         &lt;Value&gt;([0-9a-zA-Z\\._-]+.(png|gif|jp[e]?g)&lt;/Value&gt;\n        &lt;/FilterRule&gt;\n    &lt;/S3Key&gt;\n    &lt;S3Tags&gt;\n        &lt;FilterRule&gt;\n            &lt;Name&gt;Project&lt;/Name&gt;&lt;Value&gt;Blue&lt;/Value&gt;\n        &lt;/FilterRule&gt;\n        &lt;FilterRule&gt;\n            &lt;Name&gt;Classification&lt;/Name&gt;&lt;Value&gt;Confidential&lt;/Value&gt;\n        &lt;/FilterRule&gt;\n    &lt;/S3Tags&gt;\n&lt;/Filter&gt;\n</code></pre>"},{"location":"patterns/bucket-notifications/bucket-notifications/#events","title":"Events","text":"<p>The notifications sent to the endpoints are called events, and they are structured like this:</p> <p>Event example:</p> <pre><code>{\"Records\":[\n    {\n        \"eventVersion\":\"2.1\",\n        \"eventSource\":\"ceph:s3\",\n        \"awsRegion\":\"us-east-1\",\n        \"eventTime\":\"2019-11-22T13:47:35.124724Z\",\n        \"eventName\":\"ObjectCreated:Put\",\n        \"userIdentity\":{\n            \"principalId\":\"tester\"\n        },\n        \"requestParameters\":{\n            \"sourceIPAddress\":\"\"\n        },\n        \"responseElements\":{\n            \"x-amz-request-id\":\"503a4c37-85eb-47cd-8681-2817e80b4281.5330.903595\",\n            \"x-amz-id-2\":\"14d2-zone1-zonegroup1\"\n        },\n        \"s3\":{\n            \"s3SchemaVersion\":\"1.0\",\n            \"configurationId\":\"mynotif1\",\n            \"bucket\":{\n                \"name\":\"mybucket1\",\n                \"ownerIdentity\":{\n                    \"principalId\":\"tester\"\n                },\n                \"arn\":\"arn:aws:s3:us-east-1::mybucket1\",\n                \"id\":\"503a4c37-85eb-47cd-8681-2817e80b4281.5332.38\"\n            },\n            \"object\":{\n                \"key\":\"myimage1.jpg\",\n                \"size\":\"1024\",\n                \"eTag\":\"37b51d194a7513e45b56f6524f2d51f2\",\n                \"versionId\":\"\",\n                \"sequencer\": \"F7E6D75DC742D108\",\n                \"metadata\":[],\n                \"tags\":[]\n            }\n        },\n        \"eventId\":\"\",\n        \"opaqueData\":\"me@example.com\"\n    }\n]}\n</code></pre>"},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/","title":"Kafka to Object Storage","text":""},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/#description","title":"Description","text":"<p>Kafka is a distributed event stream processing system which is great for storing hot relevant data. Based on the retention policy of the data, it can be used to store data for a long time. However, it is not suitable for storing data for a long time. This is where we need a mechanism to move data from Kafka to the object storage.</p>"},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/#use-cases","title":"Use Cases","text":""},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/#long-term-retention-of-data","title":"Long term retention of data","text":"<p>As Kafka is not really suited for long term retention of data, persisting it inside an object store will allow you to keep your data for further use, backup or archival purposes. Depending on the solution you use, you can also transform or format you data while storing it, which will ease further retrieval.</p>"},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/#move-data-to-central-data-lake","title":"Move data to Central Data Lake","text":"<p>Production Kafka environment may not be the best place to run analytics or do model training. Transferring or copying the date to a central data lake will allow you to decouple those two aspects (production and analytics), bringing peace of mind and further capabilities to the data consumers.</p>"},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/#implementations-examples","title":"Implementations examples","text":"<p>This pattern is implemented in the Smart City demo</p>"},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/#configuration-using-secor","title":"Configuration Using Secor","text":"<p>This pattern implements the Secor Kafka Consumer. It can be used to consume kafka messages from a kafka topic and store that to S3 compatible Objet Buckets.</p> <p>Secor is a service persisting Kafka logs to Amazon S3, Google Cloud Storage, Microsoft Azure Blob Storage and Openstack Swift. Its key features are: strong consistency, fault tolerance, load distribution, horizontal scalability, output partitioning, configurable upload policies, monitoring, customizability, event transformation.</p> <p></p>"},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/#prerequisites","title":"Prerequisites","text":""},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/#bucket","title":"Bucket","text":"<p>An S3-compatible bucket, with its access key and secret key.</p>"},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/#zookeeper-entrance","title":"ZooKeeper Entrance","text":"<p>Secor needs to connect directly to Zookeeper to keep track of some data. If you have a secured installation of Zookeeper, like when you deploy Kafka using Strimzi or AMQStreams, you need to deploy a ZooKeeper Entrance. This is a special proxy to Zookeeper that will allow this direct connection.</p> <p>Note</p> <p>The deployment file is based on a Strimzi or AMQ Streams deployment of Kafka. If you configuration is different you may have to adapt some of the parameters.</p> <p>Deployment:</p> <ul> <li>In the file deployment/zookeeper-entrance.yaml, replace:<ul> <li>the occurrences of 'NAMESPACE' by the namespace where the Kafka cluster is.</li> <li>the occurrences of 'YOUR_KAFKA' by the name of your Kafka cluster.</li> <li>the parameters YOUR_KEY, YOUR_SECRET, YOUR_ENDPOINT, YOUR_BUCKET with the values corresponding to the bucket where you want to store the data.</li> </ul> </li> <li>Apply the modified file to deploy ZooKeeper Entrance.</li> </ul>"},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/#deployment","title":"Deployment","text":""},{"location":"patterns/kafka/kafka-to-object-storage/kafka-to-object-storage/#secor","title":"Secor","text":"<ul> <li>In the file deployment/secor.yaml, replace:<ul> <li>the occurrences of 'NAMESPACE' by the namespace where the Kafka cluster is.</li> <li>the occurrences of 'YOUR_KAFKA' by the name of your Kafka cluster.</li> <li>adjust all the other Secor parameters or add others depending on the processing you want to do with the data: output format, aggregation,... Full instructions are available here.</li> </ul> </li> <li>Apply the modified file to deploy Secor.</li> </ul>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/","title":"Kafka to Serverless","text":""},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#description","title":"Description","text":"<p>This pattern describes how to use AMQ Streams (Kafka) as an event source to OpenShift Serverless (Knative). You will learn how to implement Knative Eventing that can trigger a Knative Serving function when a messaged is posted to a Kafka Topic (Event).</p>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#knative-openshift-serverless","title":"Knative &amp; OpenShift Serverless","text":"<p>Knative is an open source project that helps to deploy and manage modern serverless workloads on Kubernetes. Red Hat OpenShift Serverless is an enterprise-grade serverless offering based on knative that provides developers with a complete set of tools to build, deploy, and manage serverless applications on OpenShift Container Platform</p> <p>Knative consists of 3 primary components:</p> <ul> <li>Build - A flexible approach to building source code into containers.</li> <li>Serving - Enables rapid deployment and automatic scaling of containers through a request-driven model for serving workloads based on demand.</li> <li>Eventing - An infrastructure for consuming and producing events to stimulate applications. Applications can be triggered by a variety of sources, such as events from your own applications, cloud services from multiple providers, Software-as-a-Service (SaaS) systems, and Red Hat AMQ streams.</li> </ul>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#eda-event-driven-architecture","title":"EDA (Event Driven Architecture)","text":"<p>Event-Driven Architecture (EDA) is a way of designing applications and services to respond to real-time information based on the sending and receiving of information about individual events. EDA uses events to trigger and communicate between decoupled services and is common in modern applications built with microservices.</p> <p></p>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#use-cases","title":"Use Cases","text":"<ul> <li>Develop an event-driven architecture with serverless applications.</li> <li>Serverless Business logic processing that is capable of automated scale-up and scale-down to zero.</li> </ul>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#implementations-examples","title":"Implementations examples","text":"<p>This pattern is implemented in the XRay Pipeline Demo</p>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#deployment-example","title":"Deployment example","text":""},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#requirements","title":"Requirements","text":"<ul> <li>Red Hat OpenShift Container Platform</li> <li>Red Hat AMQ Streams or Strimzi: the operator should be installed and a Kafka cluster must be created</li> <li>Red Hat OpenShift Serverless: the operator must be installed</li> </ul>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#part-1-set-up-knative","title":"Part 1: Set up KNative","text":"<p>Once Red Hat OpenShift Serverless operator has been installed, we can create KnativeServing, KnativeEventing and KnativeKafka instances.</p>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#step-1-create-required-knative-instances","title":"Step 1: Create required Knative instances","text":"<ul> <li>From the deployment folder, apply the YAML file 01_knative_serving_eventing_kafka_setup.yaml to create knative instances</li> </ul> <pre><code>oc create -f 01_knative_serving_eventing_kafka_setup.yaml\n</code></pre> <p>Note</p> <p>Those instances can also be deployed through the OpenShift Console if you prefer to use a UI. In this case, follow the Serverless deployment instructions (this section and the following ones).</p>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#step-2-verify-knative-instances","title":"Step 2: Verify Knative Instances","text":"<pre><code>oc get po -n knative-serving\noc get po -n knative-eventing\n</code></pre> <ul> <li>Pod with prefix kafka-controller-manager represents Knative Kafka Event Source.</li> </ul>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#part-2-knative-serving","title":"Part 2: Knative Serving","text":"<p>Knative Serving is your serverless business logic that you would like to execute based on the event generated by Kafka.</p> <p>For example purpose we are using a simple greeter service here. Depending on your use case you will replace that with your own business logic.</p>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#step-1-create-knative-serving","title":"Step 1: Create Knative Serving","text":"<ul> <li>From the deployment folder, in the YAML file 02_knative_service.yaml, replace the placeholder <code>YOUR_NAMESPACE</code> with your namespace, and apply the file to create knative serving.</li> </ul> <pre><code>oc create -f 02_knative_service.yaml\n</code></pre>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#step-2-verify-knative-serving","title":"Step 2: Verify Knative Serving","text":"<pre><code>oc get serving\n</code></pre>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#part-3-knative-eventing","title":"Part 3: Knative Eventing","text":"<p>Knative Eventing enables developers to use an event-driven architecture with serverless applications. An event-driven architecture is based on the concept of decoupled relationships between event producers that create events, and event sinks, or consumers, that receive them.</p>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#step-1-kafka-topic","title":"Step 1: Kafka topic","text":"<ul> <li>Create a Kafka topic where the events will be sent. In this example, the topic will be <code>example_topic</code>.</li> </ul>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#step-2-create-knative-eventing","title":"Step 2: Create Knative Eventing","text":"<ul> <li>To create a Knative Eventing, we need to create a Kafka Event Source. Before you apply the following YAML file, 03_knative_kafka_source.yaml, please make sure to edit namespace and bootstrapServers to match your Kafka cluster. Also make sure to use the correct Knative Service (serving) that you have created in the previous step (<code>greeter</code> in this example).</li> </ul> <pre><code>oc create -f 03_knative_kafka_source.yaml\n</code></pre>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#step-3-verify-knative-eventing","title":"Step 3: Verify Knative Eventing","text":"<pre><code>oc get kafkasource\n</code></pre> <p>At this point, as soon as new messages are received in Kafka topic <code>example_topic</code>, Knative Eventing will trigger the Knative Service greeter to execute the business logic, allowing you to have event-driven serverless application running on OpenShift Container Platform.</p>"},{"location":"patterns/kafka/kafka-to-serverless/kafka-to-serverless/#part-4-testing","title":"Part 4: Testing","text":"<ul> <li>Optional: to view the logs of Knative Serving you can install stern to them from the CLI, or use the OpenShift Web Console.</li> </ul> <pre><code>oc get ksvc\nstern --selector=serving.knative.dev/service=greeter -c user-container\n</code></pre> <ul> <li>Launch a temporary Kafka CLI (kafkacat) in a new terminal</li> </ul> <pre><code>oc run kafkacat -i -t --image debezium/tooling --restart=Never\n</code></pre> <ul> <li>From the kafkacat container shell, generate kafka messages in the topic <code>example_topic</code> of your Kafka cluster. Here we are generating Kafka messages with CloudEvents (CE) specification.</li> </ul> <pre><code>for i in {1..50} ; do sleep 10 ; \\\necho '{\"message\":\"Hello Red Hat\"}' | kafkacat -P -b core-kafka-kafka-bootstrap -t example_topic \\\n  -H \"content-type=application/json\" \\\n  -H \"ce-id=CE-001\" \\\n  -H \"ce-source=/kafkamessage\"\\\n  -H \"ce-type=dev.knative.kafka.event\" \\\n  -H \"ce-specversion=1.0\" \\\n  -H \"ce-time=2018-04-05T03:56:24Z\"\ndone ;\n</code></pre> <p>The above command will generate 50 Kafka messages every 10 seconds. Knative Eventing will pick up the messages and invoke the greeter Knative service, that you can verify from the logs of Knative Serving.</p>"},{"location":"patterns/starproxy/starproxy/","title":"Starburst/Trino Proxy","text":""},{"location":"patterns/starproxy/starproxy/#what-it-is","title":"What it is","text":"<p>Starproxy is a fully HTTP compliant proxy that is designed to sit between clients and a Trino/Starburst cluster. The motivation for developing a solution like this is laid out in some prior art below:</p> <ul> <li>Facebook Engineering Blog - Static Analysis</li> <li>Strata Conference Talk</li> <li>Uber Case Study - Prism</li> </ul> <p>The most attractive items to us are probably:</p> <ul> <li>Enabling host based security</li> <li>Detecting \"bad\" queries and blocking/deprioritizing them with custom rules</li> <li>Load balancing across regions</li> </ul>"},{"location":"patterns/starproxy/starproxy/#how-it-works","title":"How it works","text":"<p>First and foremost, starproxy is an http proxy implemented in rust using a combination of axum/hyper.</p> <p></p> <ol> <li> <p>Parse the query AST, then check a variety of rules:</p> <ul> <li>inbound CIDR rule checking</li> <li>checking for predicates in queries</li> <li>identifying select * queries with no limit, among other rules</li> </ul> </li> <li> <p>If rules are violated they can be associated with actions, like tagging the query as low priority. This is done by modifying the request headers and injecting special tags. Rules can also outright block requests by returning error status codes to the client directly.</p> </li> </ol>"},{"location":"predictive-ai/what-is-predictive-ai/","title":"What is Predictive AI?","text":"<p>Predictive AI generally has the following characteristics:</p> <ul> <li>A model generally created based on specific or narrow set of data.</li> <li>Aims to make predictions on the likehood of an outcome, based on certain conditions and inputs.</li> <li>e.g. \"how likely is this person to default on their loan in the next 5 years, given what we know about them?\"</li> </ul>"},{"location":"tools-and-applications/airflow/airflow/","title":"Apache Airflow","text":""},{"location":"tools-and-applications/airflow/airflow/#what-is-it","title":"What is it?","text":"<p>Apache Airflow is a platform created by the community to programmatically author, schedule and monitor workflows. It has become popular because of how easy it is to use and how extendable it is, covering a wide variety of tasks and allowing you to connect your workflows with virtually any technology. Since it's a Python framework it has also gathered a lot of interest from the Data Science field.</p> <p>One important concept used in Airflow is DAGs (Directed Acyclical Graphs). A DAG is a graph without any cycles. In other words, a node in your graph may never point back to a node higher up in your workflow. DAGs are used to model your workflows/pipelines, which essentially means that you are building and executing graphs when working with Airflow. You can read more about DAGs here: https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html</p> <p>The key features of Airflow are:</p> <ul> <li>Webserver: It's a user interface where you can see the status of your jobs, as well as inspect, trigger, and debug your DAGs and tasks. It also gives a database interface and lets you read logs from the remote file store.</li> <li>Scheduler: The Scheduler is a component that monitors and manages all your tasks and DAGs, it checks their status and triggers them in the correct order once their dependencies are complete.</li> <li>Executors: It handles running your task when they are assigned by the scheduler. It can either run the tasks inside the scheduler or push task execution out to workers. Airflow supports a variety of different executors which you can choose between.</li> <li>Metadata database: The metadata database is used by the executor, webserver, and scheduler to store state.</li> </ul> <p></p>"},{"location":"tools-and-applications/airflow/airflow/#installing-apache-airflow-on-openshift","title":"Installing Apache Airflow on OpenShift","text":"<p>Airflow can be run as a pip package, through docker, or a Helm chart. The official Helm chart can be found here: https://airflow.apache.org/docs/apache-airflow/stable/installation/index.html#using-official-airflow-helm-chart</p> <p>See OpenDataHub Airflow - Example Helm Values</p> <p>A modified version of the Helm chart which can be installed on OpenShift 4.12: https://github.com/eformat/openshift-airflow</p>"},{"location":"tools-and-applications/apache-nifi/apache-nifi/","title":"Apache NiFi","text":""},{"location":"tools-and-applications/apache-nifi/apache-nifi/#what-is-it","title":"What is it?","text":"<p>Apache NiFi is an open-source data integration tool that helps automate the flow of data between systems. It is designed to be easy to use and allows users to quickly and efficiently process, transmit, and securely distribute data. NiFi provides a web-based interface for monitoring and controlling data flows, as well as a library of processors for common data manipulation tasks such as filtering, routing, and transformation. It is highly configurable and can be used in a variety of scenarios including data ingestion, ETL, and dataflow management.</p> <p></p> <p>Nifi is a powerful tool to move data between systems and can handle real-time data with ease. It can be used in conjunction with other big data technologies such as Apache Kafka and Apache Spark to create a complete data pipeline. It supports a wide range of protocols and data formats, making it a versatile solution for any organization looking to manage and process large amounts of data.</p>"},{"location":"tools-and-applications/apache-nifi/apache-nifi/#installing-apache-nifi-on-openshift","title":"Installing Apache Nifi on OpenShift","text":"<p>The easiest way to install it is to follow the instructions available on the Nifi on OpenShift repo.</p> <p></p> <p>Contrary to other recipes or the images you can find on the Nifi project, the container images available on this repo are all based on UBI8 and follow OpenShift guidelines and constraints, like running with minimal privileges.</p> <p>Several deployment options are available:</p> <ul> <li>Choice on the number of nodes to deploy,</li> <li>Basic, OIDC or LDAP authentication.</li> </ul>"},{"location":"tools-and-applications/apache-spark/apache-spark/","title":"Apache Spark","text":""},{"location":"tools-and-applications/apache-spark/apache-spark/#what-is-it","title":"What is it?","text":"<p>Apache Spark is an open-source, distributed computing system used for big data processing. It can process large amounts of data quickly and efficiently, and handle both batch and streaming data. Spark uses the in-memory computing concept, which allows it to process data much faster than traditional disk-based systems.</p> <p>Spark supports a wide range of programming languages including Java, Python, and Scala. It provides a number of high-level libraries and APIs, such as Spark SQL, Spark Streaming, and MLlib, that make it easy for developers to perform complex data processing tasks. Spark SQL allows for querying structured data using SQL and the DataFrame API, Spark Streaming allows for processing real-time data streams, and MLlib is a machine learning library for building and deploying machine learning models. Spark also supports graph processing and graph computation through GraphX and GraphFrames.</p>"},{"location":"tools-and-applications/apache-spark/apache-spark/#working-with-spark-on-openshift","title":"Working with Spark on OpenShift","text":"<p>Spark can be fully containerized. Therefore a standalone Spark cluster can of course be installed on OpenShift. However, it sorts of breaks the cloud-native approach brought by Kubernetes of ephemeral workloads. There are in fact many ways to work with Spark on OpenShift, either with Spark-on-Kubernetes operator, or directly through PySpark or spark-submit commands.</p> <p>In this Spark on OpenShift repository, you will find all the instructions to work with Spark on OpenShift.</p> <p>It includes:</p> <ul> <li>pre-built UBI-based Spark images including the drivers to work with S3 storage,</li> <li>instructions and examples to build your own images (to include your own libraries for example),</li> <li>instructions to deploy the Spark history server to gather your processing logs,</li> <li>instructions to deploy the Spark on Kubernetes operator,</li> <li>Prometheus and Grafana configuration to monitor your data processing and operator in real time,</li> <li>instructions to work without the operator, from a Notebook or a Terminal, inside or outside the OpenShift Cluster,</li> <li>various examples to test your installation and the different methods.</li> </ul>"},{"location":"tools-and-applications/datasciencepipeline/datasciencepipeline/","title":"Data Science Pipeline","text":""},{"location":"tools-and-applications/datasciencepipeline/datasciencepipeline/#what-is-it","title":"What is it?","text":"<p>OpenShift AI allows building of machine line workflows with a data science pipeline. From OpenShift AI version 2.9, data science pipelines are based on KubeFlow Pipelines (KFP) version 2.0.</p>"},{"location":"tools-and-applications/datasciencepipeline/datasciencepipeline/#what-is-kubeflow-pipelines","title":"What is Kubeflow Pipelines?","text":"<p>Kubeflow Pipelines (KFP) is a platform for building and deploying portable and scalable machine learning (ML) workflows using Docker containers.</p> <p>With KFP you can author components and pipelines using the KFP Python SDK, compile pipelines to an intermediate representation YAML, and submit the pipeline to run on a KFP-conformant backend.</p> <p>The current version of KFP 2.0 in OpenShift AI uses Argo Workflow as the backend.</p>"},{"location":"tools-and-applications/datasciencepipeline/datasciencepipeline/#why-do-i-see-openshift-pipeline-in-this-example","title":"Why do I see OpenShift Pipeline in this example?","text":"<p>The example uses OpenShift Pipeline (Tekton) to compile the pipeline into an intermediate representation (IR) YAML and submit it to the Kubeflow Pipeline server (instead of doing it from your Jupyter environment using Elyra, or importing it directly through the Dashboard).</p> <p>The Tekton pipeline has 2 main tasks:</p> <ul> <li>git-clone</li> <li>execute-kubeflow-pipeline to compile and submit the pipeline</li> </ul>"},{"location":"tools-and-applications/datasciencepipeline/datasciencepipeline/#example","title":"Example","text":""},{"location":"tools-and-applications/datasciencepipeline/datasciencepipeline/#architectural-diagram","title":"Architectural Diagram","text":"<p>The demo uses the following components:</p> Component Descrioption Gitea To store pipeline source code Model Registry To store model metadata OpenShift Pipelines Using Tekton to build the pipeline Data Science Pipeline To run the pipeline using KFP Minio S3 bucket to store the model KServe To serve the model"},{"location":"tools-and-applications/datasciencepipeline/datasciencepipeline/#prerequisite","title":"Prerequisite","text":"<p>You will need OpenShift 2.15 installed with ModelRegistry set to <code>Managed</code>. In 2.15, the model registry feature is currently in Tech Preview.</p>"},{"location":"tools-and-applications/datasciencepipeline/datasciencepipeline/#running-the-example","title":"Running the Example","text":"<p>The sample code is available here.</p>"},{"location":"tools-and-applications/ensemble-serving/ensemble-serving/","title":"Ensemble Models with Triton and KServe","text":"<p>Ensemble models are a feature of the Triton Model Server. They represent a pipeline of one or more models and the connection of input and output tensors between those models. Ensemble models are intended to be used to encapsulate a procedure that involves multiple models, such as \u201cdata preprocessing -&gt; inference -&gt; data postprocessing\u201d.</p> <p>Using Ensemble models for this purpose can avoid the overhead of transferring intermediate tensors and minimize the number of requests that must be sent to Triton.</p> <p>Full reference</p> <p>Note</p> <p>In this repo you will find the full recipe to deploy the Triton runtime with the Single Model Serving Stack, and an example of an Ensemble model to test it.</p> <p>The following instructions are the walkthrough for this installation and test.</p>"},{"location":"tools-and-applications/ensemble-serving/ensemble-serving/#requirements","title":"Requirements","text":"<p>Deploy Triton as a custom Single Model Serving Runtime in OpenShift AI. You can import a REST Interface version, or a gRPC one.</p> <ul> <li> <p>On the OpenShift AI dashboard, go to Settings-&gt;Serving runtimes</p> <p></p> </li> <li> <p>Add a Servig runtime:</p> <p></p> </li> <li> <p>Select the type:</p> <p></p> </li> <li> <p>Paste the content of the runtime definition:</p> <p></p> </li> <li> <p>You can do the same with the gRPC version:</p> <p></p> </li> <li> <p>You now have the two runtimes available:</p> <p></p> </li> </ul>"},{"location":"tools-and-applications/ensemble-serving/ensemble-serving/#model-deployment","title":"Model Deployment","text":"<p>This deployment is based on the example model you can find in the model01 of the repo.</p> <ul> <li> <p>Copy the whole content of the model folder (so normally multiple models, plus the Ensemble definition) to an object store bucket.</p> <p></p> </li> <li> <p>In OpenShift AI, create a Data Connection pointing to the bucket.</p> <p></p> </li> <li> <p>Serve the model in OpenShift AI using the custom runtime you imported, pointing it to the data connection.</p> <p> </p> </li> <li> <p>After a few seconds/minutes, the model is served and an inference endpoint is available.</p> <p></p> </li> <li> <p>You can also deploy the gRPC version in the same manner if wou want.</p> </li> </ul>"},{"location":"tools-and-applications/ensemble-serving/ensemble-serving/#test","title":"Test","text":"<ul> <li> <p>You can use the notebook test-ensemble-rest.ipynb to test the endpoint if you deployed the REST version of the runtime. Another notebook is available for gRPC.</p> <p> </p> </li> </ul>"},{"location":"tools-and-applications/minio/minio/","title":"Minio","text":""},{"location":"tools-and-applications/minio/minio/#what-is-it","title":"What is it?","text":"<p>Minio is a high-performance, S3 compatible object store. It can be deployed on a wide variety of platforms, and it comes in multiple flavors.</p>"},{"location":"tools-and-applications/minio/minio/#why-this-guide","title":"Why this guide?","text":"<p>This guide is a very quick way of deploying the community version of Minio in order to quickly setup a fully standalone Object Store, in an OpenShift Cluster. This can then be used for various prototyping tasks that require Object Storage.</p> <p>Note that nothing in this guide should be used in production-grade environments. Also, Minio is not included in RHOAI, and Red Hat does not provide support for Minio.</p>"},{"location":"tools-and-applications/minio/minio/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Access to an OpenShift cluster</li> <li>Namespace-level admin permissions, or permission to create your own project</li> </ul>"},{"location":"tools-and-applications/minio/minio/#deploying-minio-on-openshift","title":"Deploying Minio on OpenShift","text":""},{"location":"tools-and-applications/minio/minio/#create-a-data-science-project-optional","title":"Create a Data Science Project (Optional)","text":"<p>If you already have your own Data Science Project, or OpenShift project, you can skip this step.</p> <ol> <li>If your cluster already has Red Hat OpenShift AI installed, you can use the Dashboard Web Interface to create a Data Science project.</li> <li>Simply navigate to Data Science Projects</li> <li>And click Create Project</li> <li> <p>Choose a name for your project (here, Showcase) and click Create:</p> <p></p> </li> <li> <p>Make sure to make a note of the Resource name, in case it's different from the name.</p> </li> </ol>"},{"location":"tools-and-applications/minio/minio/#log-on-to-your-project-in-openshift-console","title":"Log on to your project in OpenShift Console","text":"<ol> <li> <p>Go to your cluster's OpenShift Console:</p> <p></p> </li> <li> <p>Make sure you use the Administrator view, not the developer view.</p> </li> <li> <p>Go to Workloads then Pods, and confirm the selected project is the right one</p> <p></p> </li> <li> <p>You now have a project in which to deploy Minio</p> </li> </ol>"},{"location":"tools-and-applications/minio/minio/#deploy-minio-in-your-project","title":"Deploy Minio in your project","text":"<ol> <li> <p>Click on the + (\"Import YAML\") button:</p> <p></p> </li> <li> <p>Paste the following YAML in the box, but don't press ok yet!:     <pre><code>---\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: minio-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 20Gi\n  volumeMode: Filesystem\n---\nkind: Secret\napiVersion: v1\nmetadata:\n  name: minio-secret\nstringData:\n  # change the username and password to your own values.\n  # ensure that the user is at least 3 characters long and the password at least 8\n  minio_root_user: minio\n  minio_root_password: minio123\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: minio\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: minio\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: minio\n    spec:\n      volumes:\n        - name: data\n          persistentVolumeClaim:\n            claimName: minio-pvc\n      containers:\n        - resources:\n            limits:\n              cpu: 250m\n              memory: 1Gi\n            requests:\n              cpu: 20m\n              memory: 100Mi\n          readinessProbe:\n            tcpSocket:\n              port: 9000\n            initialDelaySeconds: 5\n            timeoutSeconds: 1\n            periodSeconds: 5\n            successThreshold: 1\n            failureThreshold: 3\n          terminationMessagePath: /dev/termination-log\n          name: minio\n          livenessProbe:\n            tcpSocket:\n              port: 9000\n            initialDelaySeconds: 30\n            timeoutSeconds: 1\n            periodSeconds: 5\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n            - name: MINIO_ROOT_USER\n              valueFrom:\n                secretKeyRef:\n                  name: minio-secret\n                  key: minio_root_user\n            - name: MINIO_ROOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: minio-secret\n                  key: minio_root_password\n          ports:\n            - containerPort: 9000\n              protocol: TCP\n            - containerPort: 9090\n              protocol: TCP\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n            - name: data\n              mountPath: /data\n              subPath: minio\n          terminationMessagePolicy: File\n          image: &gt;-\n            quay.io/minio/minio:latest\n          args:\n            - server\n            - /data\n            - --console-address\n            - :9090\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      dnsPolicy: ClusterFirst\n      securityContext: {}\n      schedulerName: default-scheduler\n  strategy:\n    type: Recreate\n  revisionHistoryLimit: 10\n  progressDeadlineSeconds: 600\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: minio-service\nspec:\n  ipFamilies:\n    - IPv4\n  ports:\n    - name: api\n      protocol: TCP\n      port: 9000\n      targetPort: 9000\n    - name: ui\n      protocol: TCP\n      port: 9090\n      targetPort: 9090\n  internalTrafficPolicy: Cluster\n  type: ClusterIP\n  ipFamilyPolicy: SingleStack\n  sessionAffinity: None\n  selector:\n    app: minio\n---\nkind: Route\napiVersion: route.openshift.io/v1\nmetadata:\n  name: minio-api\nspec:\n  to:\n    kind: Service\n    name: minio-service\n    weight: 100\n  port:\n    targetPort: api\n  wildcardPolicy: None\n  tls:\n    termination: edge\n    insecureEdgeTerminationPolicy: Redirect\n---\nkind: Route\napiVersion: route.openshift.io/v1\nmetadata:\n  name: minio-ui\nspec:\n  to:\n    kind: Service\n    name: minio-service\n    weight: 100\n  port:\n    targetPort: ui\n  wildcardPolicy: None\n  tls:\n    termination: edge\n    insecureEdgeTerminationPolicy: Redirect\n</code></pre></p> </li> <li> <p>By default, the size of the storage is 20 GB. (see line 11). Change it if you need to.</p> </li> <li>If you want to, edit lines 21-22 to change the default user/password.</li> <li>Press Create.</li> <li> <p>You should see:</p> <p></p> </li> <li> <p>And there should now be a running minio pod:</p> <p></p> </li> <li> <p>As well as  two minio routes:</p> <p></p> </li> <li> <p>The <code>-api</code> route is for programmatic access to Minio</p> </li> <li>The <code>-ui</code> route is for browser-based access to Minio</li> <li>Your Minio Object Store is now deployed, but we still need to create at least one bucket in it, to make it useful.</li> </ol>"},{"location":"tools-and-applications/minio/minio/#creating-a-bucket-in-minio","title":"Creating a bucket in Minio","text":""},{"location":"tools-and-applications/minio/minio/#log-in-to-minio","title":"Log in to Minio","text":"<ol> <li>Locate the minio-ui Route, and open its location URL in a web browser:</li> <li> <p>When prompted, log in</p> <ul> <li>if you kept the default values, then:</li> <li>user: <code>minio</code></li> <li>pass: <code>minio123</code></li> </ul> <p></p> </li> <li> <p>You should now be logged into your Minio instance.</p> </li> </ol>"},{"location":"tools-and-applications/minio/minio/#create-a-bucket","title":"Create a bucket","text":"<ol> <li> <p>Click on Create a Bucket</p> <p></p> </li> <li> <p>Choose a name for your bucket (for example <code>mybucket</code>) and click Create Bucket:</p> <p></p> </li> <li> <p>Repeat those steps to create as many buckets as you will need.</p> </li> </ol>"},{"location":"tools-and-applications/minio/minio/#create-a-matching-data-connection-for-minio","title":"Create a matching Data Connection for Minio","text":"<ol> <li> <p>Back in RHOAI Dashboard, inside of your Data Science Project, Click on Add data connection:</p> <p></p> </li> <li> <p>Then, fill out the required field to match with your newly-deployed Minio Object Storage</p> <p></p> </li> <li> <p>You now have a Data Connection that maps to your mybucket bucket in your Minio Instance.</p> </li> <li>This data connection can be used, among other things<ul> <li>In your Workbenches</li> <li>For your Model Serving</li> <li>For your Pipeline Server Configuration</li> </ul> </li> </ol>"},{"location":"tools-and-applications/minio/minio/#validate","title":"Validate","text":"<p>To test if everything is working correctly, you can access the workbench associated with your Data Connection and run the following commands (i.e., inside a Jupyter notebook): </p> <ol> <li> <p>Install and import MinIO Python Client SDK</p> <p><pre><code>!pip install minio\n</code></pre> <pre><code>from minio import Minio\nfrom minio.error import S3Error\nimport  os\nimport datetime\n</code></pre></p> </li> <li> <p>Access Data Connection properties as environment variables:</p> <pre><code># MinIO client doesn't like URLs with procotol/schema, so use\n# yourendpoint.com instead of https://yourtendpoint.com\nAWS_S3_ENDPOINT = os.getenv(\"AWS_S3_ENDPOINT\")\nAWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\nAWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\nAWS_S3_BUCKET = os.getenv(\"AWS_S3_BUCKET\")\n</code></pre> </li> <li> <p>Create the MinIO client</p> <pre><code># Create the MinIO client\nclient = Minio(\n    AWS_S3_ENDPOINT,\n    access_key=AWS_ACCESS_KEY_ID,\n    secret_key=AWS_SECRET_ACCESS_KEY,\n    secure=True  # Set to True if you are using HTTPS\n)\n</code></pre> </li> <li> <p>Test the connection by listing all buckets</p> <pre><code>#List all buckets\ntry:\n    buckets = client.list_buckets()\n    for bucket in buckets:\n        print(bucket.name, bucket.creation_date)\nexcept S3Error as e:\n    print(\"Error occurred: \", e)\n</code></pre> </li> <li> <p>Create a sample local file</p> <pre><code># Create File\nFILE_ON_DISK = 'file.txt'\n\nfile = open(f\"{FILE_ON_DISK}\", \"w\")\nfile.write('Hello there %s recorded at %s.\\n' % (FILE_ON_DISK, datetime.datetime.now()))\nfile.close()\n</code></pre> </li> <li> <p>Upload a file to MinIO</p> <pre><code># Upload a File \nfile_path = FILE_ON_DISK\nobject_name = 'target-file.txt'\n\ntry:\n    client.fput_object(AWS_S3_BUCKET, object_name, file_path)\n    print(f\"'{object_name}' is successfully uploaded as object to bucket '{bucket_name}'.\")\nexcept S3Error as e:\n    print(\"Error occurred: \", e)\n</code></pre> </li> <li> <p>Download a file from MinIO</p> <pre><code># Download a file \nobject_name = 'target-file.txt'\nfile_path = 'file-froms3.txt'\n\ntry:\n client.fget_object(AWS_S3_BUCKET, object_name, file_path)\n print(f\"'{object_name}' is successfully downloaded to '{file_path}'.\")\nexcept S3Error as e:\n print(\"Error occurred: \", e)\n</code></pre> </li> <li> <p>List objects in our bucket</p> <pre><code># Download a file \nobject_name = 'target-file.txt'\nfile_path = 'file-froms3.txt'\n\ntry:\n    client.fget_object(AWS_S3_BUCKET, object_name, file_path)\n    print(f\"'{object_name}' is successfully downloaded to '{file_path}'.\")\nexcept S3Error as e:\n    print(\"Error occurred: \", e)\n</code></pre> </li> </ol> <p>For more complete and detailed information about MinIO Python Client SDK usage, please check the official documentation. </p>"},{"location":"tools-and-applications/minio/minio/#notes-and-faq","title":"Notes and FAQ","text":"<ul> <li>As long as you are using the Route URLs, a Minio running in one namespace can be used by any other application, even running in another namespace, or even in another cluster altogether.</li> </ul>"},{"location":"tools-and-applications/minio/minio/#uninstall-instructions","title":"Uninstall instructions:","text":"<p>This will completely remove Minio and all its content. Make sure you have a backup of the things your need before doing so!</p> <ol> <li> <p>Track down those objects created earlier:</p> <p></p> </li> <li> <p>Delete them all.</p> </li> </ol>"},{"location":"tools-and-applications/mlflow/mlflow/","title":"MLFlow","text":""},{"location":"tools-and-applications/mlflow/mlflow/#what-is-it","title":"What is it?","text":"<p>MLflow is an open source platform to manage the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. MLflow currently offers four components:  Read more here: https://mlflow.org/</p>"},{"location":"tools-and-applications/mlflow/mlflow/#helm-installation-into-openshift-namespace","title":"Helm installation into OpenShift namespace","text":""},{"location":"tools-and-applications/mlflow/mlflow/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Install the \"Crunchy Postgres for Kubernetes\" operator (can be found in OperatorHub) - To store the MLFlow config</li> <li>Install the \"OpenShift Data Foundation\" operator (can be found in OperatorHub) - To provide S3 storage for the experiments and models</li> </ul>"},{"location":"tools-and-applications/mlflow/mlflow/#install","title":"Install","text":"<pre><code>&lt;Create an OpenShift project, either through the OpenShift UI or 'oc new-project project-name'&gt;\nhelm repo add strangiato https://strangiato.github.io/helm-charts/\nhelm repo update\n&lt;Log in to the correct OpenShift project through 'oc project project-name'&gt;\nhelm upgrade -i mlflow-server strangiato/mlflow-server\n</code></pre>"},{"location":"tools-and-applications/mlflow/mlflow/#additional-options","title":"Additional Options","text":"<p>The MLFlow Server helm chart provides a number of customizable options when deploying MLFlow.  These options can be configured using the <code>--set</code> flag with <code>helm install</code> or <code>helm upgrade</code> to set options directly on the command line or through a <code>values.yaml</code> file using the <code>--values</code> flag.</p> <p>For a full list of configurable options, see the helm chart documentation:</p> <p>https://github.com/strangiato/helm-charts/tree/main/charts/mlflow-server#values</p>"},{"location":"tools-and-applications/mlflow/mlflow/#opendatahub-dashboard-application-tile","title":"OpenDataHub Dashboard Application Tile","text":"<p>As discussed in the Dashboard Configuration, ODH/RHOAI allows administrators to add a custom application tile for additional components on the cluster.</p> <p></p> <p>The MLFlow Server helm chart supports creation of the Dashboard Application tile as a configurable value.  If MLFlow Server is installed in the same namespace as ODH/RHOAI you can install the dashboard tile run the following command:</p> <pre><code>helm upgrade -i mlflow-server strangiato/mlflow-server \\\n    --set odhApplication.enabled=true\n</code></pre> <p>The MLFlow Server helm chart also supports installing the odhApplication object in a different namespace, if MLFlow Server is not installed in the same namespace as ODH/RHOAI:</p> <pre><code>helm upgrade -i mlflow-server strangiato/mlflow-server \\\n    --set odhApplication.enabled=true \\\n    --set odhApplication.namespaceOverride=redhat-ods-applications\n</code></pre> <p>After enabling the odhApplication component, wait 1-2 minutes and the tile should appear in the Explorer view of the dashboard.</p> <p>Note</p> <p>This feature requires ODH v1.4.1 or newer</p>"},{"location":"tools-and-applications/mlflow/mlflow/#test-mlflow","title":"Test MLFlow","text":"<ul> <li>Go to the OpenShift Console and switch to Developer view.</li> <li>Go to the Topology view and make sure that you are on the MLFlow project.</li> <li>Check that the MLFlow circle is dark blue (this means it has finished deploying).</li> <li>Press the \"External URL\" link in the top right corner of the MLFlow circle to open up the MLFlow UI.</li> <li>Run <code>helm test mlflow-server</code> in your command prompt to test MLFlow. If successful, you should see a new experiment called \"helm-test\" show up in the MLFlow UI with 3 experiments inside it.</li> </ul>"},{"location":"tools-and-applications/mlflow/mlflow/#adding-mlflow-to-training-code","title":"Adding MLFlow to Training Code","text":"<pre><code>import mlflow\nfrom sklearn.linear_model import LogisticRegression\n\n# Set tracking URI\nmlflow.set_tracking_uri(\u201chttps://&lt;route-to-mlflow&gt;\u201d)\n\n# Setting the experiment\nmlflow.set_experiment(\"my-experiment\")\n\nif __name__ == \"__main__\":\n    # Enabling automatic logging for scikit-learn runs\n    mlflow.sklearn.autolog()\n\n    # Starting a logging run\n    with mlflow.start_run():\n        # train\n</code></pre>"},{"location":"tools-and-applications/mlflow/mlflow/#source-code","title":"Source Code","text":"<p>MLFlow Server Source Code: https://github.com/strangiato/mlflow-server</p> <p>MLFlow Server Helm Chart Source Code: https://github.com/strangiato/helm-charts/tree/main/charts/mlflow-server</p>"},{"location":"tools-and-applications/mlflow/mlflow/#demos","title":"Demos","text":"<ul> <li>Credit Card Fraud Detection pipeline using MLFlow together with RHOAI: Demo</li> </ul>"},{"location":"tools-and-applications/rclone/rclone/","title":"Rclone","text":""},{"location":"tools-and-applications/rclone/rclone/#what-is-it","title":"What is it?","text":"<p>Rclone is a program to manage files on cloud storage. It is a feature-rich alternative to cloud vendors' web storage interfaces. Over 40 cloud storage products support rclone including S3 object stores, business &amp; consumer file storage services, as well as standard transfer protocols.</p> <p>Users call rclone \"The Swiss army knife of cloud storage\", and \"Technology indistinguishable from magic\".</p> <p>Rclone really looks after your data. It preserves timestamps and verifies checksums at all times. Transfers over limited bandwidth; intermittent connections, or subject to quota can be restarted, from the last good file transferred. You can check the integrity of your files. Where possible, rclone employs server-side transfers to minimize local bandwidth use and transfers from one provider to another without using local disk.</p> <p>Rclone is mature, open-source software originally inspired by rsync and written in Go. The friendly support community is familiar with varied use cases.</p> <p>The implementation described here is a containerized version of Rclone to run on OpenShift, alongside or integrated within ODH/RHOAI.</p>"},{"location":"tools-and-applications/rclone/rclone/#deployment","title":"Deployment","text":""},{"location":"tools-and-applications/rclone/rclone/#integrated-in-open-data-hub-or-openshift-ai","title":"Integrated in Open Data Hub or OpenShift AI","text":"<p>Use this method if you want to use Rclone from the ODH/RHOAI launcher or in a Data Science Project.</p> <ul> <li>In the Cluster Settings menu, import the image <code>quay.io/guimou/rclone-web-openshift:odh-rhoai_latest</code>. You can name it Rclone. </li> <li>In your DSP project, create a new workbench using the Rclone image. You can set the storage size as minimal as it's only there to store the configuration of the endpoints.  </li> </ul> <p>Tip</p> <p>The minimal size allowed by the dashboard for a storage volume is currently 1GB, which is way more than what is required for the Rclone configuration. So you can also create a much smaller PVC manually in the namespace corresponding to your Data Science Project, for example 100MB or less, and select this volume when creating the workbench.</p> <ul> <li>Launch Rclone from the link once it's deployed! </li> <li>After the standard authentication, you end up on the Rclone Login page. There is nothing to enter, but I have not found yet how to bypass it. So simply click on \"Login\". </li> </ul>"},{"location":"tools-and-applications/rclone/rclone/#standalone-deployment","title":"Standalone deployment","text":"<p>Use this method if you want to use Rclone on its own in a namespace. You can still optionally make a shortcut appear in the ODH/RHOAI dashboard.</p> <ul> <li>Create a project/namespace for your installation.</li> <li>Clone or head to this repo.</li> <li>From the deploy folder, apply the different YAML files:<ul> <li>01-pvc.yaml: creates a persistent volume to hold the configuration</li> <li>02-deployment.yaml: creates the deployment. Modify admin account and password if you want to restrict access. You should!</li> <li>03-service.yaml, 04-route.yaml: create the external access so that you can connect to the Web UI.</li> <li>Optionally, to create a tile on the ODH/RHOAI dashboard:<ul> <li>modify the 05-tile.yaml file with the address of the Route that was created previously (namespace and name of the Route object).</li> <li>the will appear under the available applications in the dashboard. Select it and click on \"Enable\" to make it appear in the \"Enabled\" menu.</li> </ul> </li> </ul> </li> </ul>"},{"location":"tools-and-applications/rclone/rclone/#configuration","title":"Configuration","text":"<p>In this example, we will create an S3 configuration that connects to a bucket on the MCG from OpenShift Data Foundation. So you must have created this bucket in advance and have all the information about it: endpoint, access and secret keys, bucket name.</p> <ul> <li>In Rclone, click on \"Configs\" to create the new Remote. </li> <li>Create new configuration, give it a name, and select \"Amazon S3 Compliant Storage Providers...\", which includes Ceph and MCG (even if not listed). </li> <li>Enter the connection info. You only have to enter the Access key and Secret, as well as the Endpoint in \"Endpoint for S3 API\". This last info is automatically copied in other fields, that's normal. </li> <li>Finalize the config by clicking on \"Next\" at the bottom.</li> </ul> <p>Now that you have the Remote set up, you can go on the Explorer, select the Remote, and browse it!  </p>"},{"location":"tools-and-applications/rclone/rclone/#usage-example","title":"Usage Example","text":"<p>In this simple example, we will transfer a dump sample from Wikipedia. Wikimedia publishes those dumps daily, and they are mirrored by different organizations. In a \"standard\" setup, loading those information into your object store would not be really practical, sometimes involving downloading it first locally to then push it to your storage.</p> <p>This is how we can do it with Rclone.</p> <ul> <li>Create your Bucket Remote as described in Configuration.</li> <li>Create another remote of type \"HTTP\", and enter the address of one of the mirrors. Here I used <code>https://dumps.wikimedia.your.org/wikidatawiki/</code>.</li> <li>Open the Explorer view, set it in dual-pane layout. In the first pane open your Bucket Remote, and in the other one the HTTP. This is what it will look like: </li> <li>Browse to the folder you want, select a file or a folder, and simply drag and drop it from the Wikidump to your bucket. You can select a big one to make things more interesting!</li> <li>Head for the dashboard where you will see the file transfer happening in the background. </li> </ul> <p>That's it! Nothing to install, high speed optimized transfer, and you could even do multiple transfers in the background,...</p>"},{"location":"tools-and-applications/riva/riva/","title":"NVIDIA RIVA","text":"<p>NVIDIA\u00ae Riva is a GPU-accelerated SDK for building Speech AI applications that are customized for your use case and deliver real-time performance.</p> <p>Riva offers pretrained speech models in NVIDIA NGC\u2122 that can be fine-tuned with the NVIDIA NeMo on a custom data set, accelerating the development of domain-specific models by 10x.</p> <p>Models can be easily exported, optimized, and deployed as a speech service on premises or in the cloud with a single command using Helm charts.</p> <p>Riva\u2019s high-performance inference is powered by NVIDIA TensorRT\u2122 optimizations and served using the NVIDIA Triton\u2122 Inference Server, which are both part of the NVIDIA AI platform.</p> <p>Riva services are available as gRPC-based microservices for low-latency streaming, as well as high-throughput offline use cases.</p> <p>Riva is fully containerized and can easily scale to hundreds and thousands of parallel streams.</p>"},{"location":"tools-and-applications/riva/riva/#deployment","title":"Deployment","text":"<p>The guide to deploy Riva on Kubernetes has to be adapted for OpenShift. Here are the different steps.</p>"},{"location":"tools-and-applications/riva/riva/#prerequisites","title":"Prerequisites","text":"<ol> <li>You have access and are logged into NVIDIA NGC. For step-by-step instructions, refer to the NGC Getting Started Guide. Specifically you will need your API Key from NVIDIA NGC.</li> <li>You have at least one worker node with an NVIDIA Volta\u2122, NVIDIA Turing\u2122, or an NVIDIA Ampere architecture-based GPU. For more information, refer to the Support Matrix.</li> <li>The Node Feature Discovery and the NVIDIA operators have been properly installed and configured on your OpenShift Cluster to enable your GPU(s). Full instructions here</li> <li>The Pod that will be deployed will consume about 10GB of RAM. Make sure you have enough resources on your node (on top of the GPU itself), and you don't have limits in place that would restrict this. GPU memory consumption will be about 12GB with all models loaded.</li> </ol>"},{"location":"tools-and-applications/riva/riva/#installation","title":"Installation","text":"<p>Included in the NGC Helm Repository is a chart designed to automate deployment to a Kubernetes cluster. This chart must be modified for OpenShift.</p> <p>The Riva Speech AI Helm Chart deploys the ASR, NLP, and TTS services automatically. The Helm chart performs a number of functions:</p> <ul> <li>Pulls Docker images from NGC for the Riva Speech AI server and utility containers for downloading and converting models.</li> <li>Downloads the requested model artifacts from NGC as configured in the values.yaml file.</li> <li>Generates the Triton Inference Server model repository.</li> <li>Starts the Riva Speech AI server as configured in a Kubernetes pod.</li> <li>Exposes the Riva Speech AI server as a configured service.</li> </ul> <p>Examples of pretrained models are released with Riva for each of the services. The Helm chart comes preconfigured for downloading and deploying all of these models.</p> <p>Installation Steps:</p> <ol> <li> <p>Download the Helm chart</p> <pre><code>export NGC_API_KEY=&lt;your_api_key&gt;\nhelm fetch https://helm.ngc.nvidia.com/nvidia/riva/charts/riva-api-2.11.0.tgz \\\n        --username=\\$oauthtoken --password=$NGC_API_KEY --untar\n</code></pre> </li> <li> <p>Switch to the newly created folder, <code>riva-api</code></p> </li> <li> <p>In the <code>templates</code> folder, modify the file <code>deployment.yaml</code>. For both the container <code>riva-speech-api</code> and the initContainer <code>riva-model-init</code> you must add the following security context information:</p> <pre><code>securityContext:\n    allowPrivilegeEscalation: false\n    capabilities:\n      drop: [\"ALL\"]\n    seccompProfile:\n      type: \"RuntimeDefault\"\n    runAsNonRoot: true\n</code></pre> </li> <li> <p>The file <code>deployment.yaml</code> should now look like this:</p> <pre><code>...\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ template \"riva-server.fullname\" . }}\n  ...\nspec:\n  ...\n  template:\n    ...\n    spec:\n      containers:\n        - name: riva-speech-api\n          securityContext:\n            allowPrivilegeEscalation: false\n            capabilities:\n              drop: [\"ALL\"]\n            seccompProfile:\n              type: \"RuntimeDefault\"\n            runAsNonRoot: true\n          image: {{ $server_image }}\n          ...\n      initContainers:\n        - name: riva-model-init\n          securityContext:\n            allowPrivilegeEscalation: false\n            capabilities:\n              drop: [\"ALL\"]\n            seccompProfile:\n              type: \"RuntimeDefault\"\n            runAsNonRoot: true\n          image: {{ $servicemaker_image }}\n          ...\n</code></pre> </li> <li> <p>At the root of <code>riva-api</code>, modify the file <code>values.yaml</code>:</p> <ol> <li> <p>You will need to convert your API Key to a password value. In a Terminal run:</p> <pre><code>echo -n $NGC_API_KEY | base64 -w0\n</code></pre> </li> <li> <p>In the <code>ngcCredentials</code> section ov <code>values.yaml</code>, enter the password you obtained above and your email</p> </li> <li>In the <code>modelRepoGenerator</code> section, for the <code>modelDeployKey</code> value, enter <code>dGx0X2VuY29kZQ==</code>. (This value is obtained from the command <code>echo -n tlt_encode | base64 -w0</code>.</li> <li>In the <code>persistentVolumeClaim</code> section, set <code>usePVC</code> to true. This is very important as it will disable the hostPath configuration for storage that is not permitted by default on OpenShift.</li> <li>If you don't have a storageClass set as default, or want to you another one, enter the name of the class you want to use in <code>storageClassName</code>. Otherwise leave this field empty and the default class will be used.</li> <li>Optionally, modify the storageSize.</li> <li>Leave the <code>ingress</code> section as is, we will create an OpenShift Route later.</li> <li>Optionally you can modify other values in the file to enable/disable certain models, or modify their configuration.</li> </ol> </li> <li> <p>Log into your OpenShift cluster from a Terminal, and create a project <code>riva-api</code>:</p> <pre><code>oc new-project riva-api\n</code></pre> </li> <li> <p>Move up one folder (so outside of the <code>riva-api</code> folder), and install NVIDIA Riva with the modified Helm chart:</p> <pre><code>helm install riva-api riva-api\n</code></pre> </li> </ol> <p>The deployment will now start.</p> <p>Info</p> <p>Beware that the deployment can be really long the first time, about 45mn if you have all the models and features selected. Containers and models have to be downloaded and configured. Please be patient...</p>"},{"location":"tools-and-applications/riva/riva/#usage","title":"Usage","text":"<p>The Helm chart had automatically created a Service, <code>riva-api</code> in the namespace where you have deployed it. If you followed this guide, this should also be <code>riva-api</code>. So within the OpenShift cluster, the API is accessible at <code>riva-api.riva-api.svc.cluster.local</code>.</p> <p>Different ports are accessible:</p> <ul> <li>http (8000): HTTP port of the Triton server.</li> <li>grpc (8001): gRPC port of the Triton server.</li> <li>metrics (8002): port for the metrics of the Triton server.</li> <li>speech-grpc (50051): gRPC port of the Speech that exposes directly the different services you can use. This is normally the one that you will use.</li> </ul> <p>If you want to use the API outside of the OpenShift cluster, you will have to create one or multiple Routes to those different endpoints.</p>"},{"location":"tools-and-applications/riva/riva/#example","title":"Example","text":"<ul> <li>On the same cluster where NVIDIA Riva is deployed, deploy RHOAI or ODH and launch a Notebook (Standard DataScience is enough).</li> <li>Clone the NVIDIA Riva tutorials repository at https://github.com/nvidia-riva/tutorials</li> <li>Open a Terminal and install the client with <code>pip install nvidia-riva-client</code>:</li> </ul> <p>(depending on the base image you used this may yield errors that you can ignore most of times).</p> <ul> <li>In the <code>tutorials</code> folder, open the notebook <code>asr-basics.ipynb</code>.</li> <li>In the cell that defines the uri of the API server, modify the default (localhost) for the address of the API server: <code>riva-api.riva-api.svc.cluster.local</code></li> </ul> <p></p> <ul> <li>Run the notebook!</li> </ul> <p></p> <p>Note</p> <p>In this example, only the first part of the notebook will work as only the English models have been deployed. You would have to adapt the configuration for other languages.</p>"},{"location":"whats-new/whats-new/","title":"What's new?","text":"<p>2025-11-05: Add Deploying Models from Public OCI Registries</p> <p>2025-07-28: Add Managing Secrets in an AI Platform</p> <p>2025-07-15: Add Deploying a Red Hat Validated Model in a Disconnected OpenShift AI Environment</p> <p>2025-04-03: Add AI for Everyone: What We Learned</p> <p>2025-04-01: Add Building an Image Generation App: What We Learned</p> <p>2025-02-21: Add Stable Diffusion Safety Checker</p> <p>2024-12-20: Add Using Kubernetes Image Puller Operator to Speed Up Start-Up Times instructions.</p> <p>2024-10-14: Add Using AnythingLLM as custom workbench in ODH/RHOAI instructions.</p> <p>2024-08-21: Add Model serving type modification instructions.</p> <p>2024-07-29: Add Kueue preemption example.</p> <p>2024-07-16: Add ODH Tools and Extensions Companion documentation.</p> <p>2024-07-16: Add GPU pruner documentation.</p> <p>2024-07-02: Add Llama2 fine-tuning with Ray and DeepSpeed demo.</p> <p>2024-06-25: Add GPU partitioning and sharing.</p> <p>2024-06-18: Add Accelerator profiles documentation.</p> <p>2024-01-03: Add Ensemble models demo.</p> <p>2024-03-28: Add Zero to Workbench documentation.</p> <p>2024-03-04: Refactoring of the site. Update on the LLMs deployment and RAG demo. Single Stack Serving certificate how-to.</p> <p>2023-12-21: Name change for Red Hat OpenShift Data Science, which becomes Red Hat OpenShift AI.</p> <p>2023-10-16: Add documentation for LLMs deployment and RAG demo.</p> <p>2023-08-01: Update to Spark documentation to include usage without the operator Tools and Applications-&gt;Apache Spark</p> <p>2023-07-05: Add documentation on Time Slicing and Autoscaling for NVIDIA GPUs ODH/RHOAI How-Tos-&gt;NVIDIA GPUs</p> <p>2023-07-05: New example of how to configure a Custom Serving Runtime with Triton.</p> <p>2023-07-03: New Minio tutorial on how to quickly deploy a simple Object Storage inside your OpenShift Project, for quick prototyping.</p> <p>2023-06-30: New NVIDIA GPU installation documentation with Node tainting in ODH/RHOAI How-Tos-&gt;NVIDIA GPUs</p> <p>2023-06-02: NVIDIA Riva documentation in Tools and Applications-&gt;NVIDIA Riva</p> <p>NVIDIA\u00ae Riva is a GPU-accelerated SDK for building Speech AI applications that are customized for your use case and deliver real-time performance.</p> <p>2023-02-06: Rclone documentation in Tools and Applications-&gt;Rclone.</p> <p>Rclone is a program to manage files on cloud storage. It is a feature-rich alternative to cloud vendors' web storage interfaces. Over 40 cloud storage products support rclone including S3 object stores, business &amp; consumer file storage services, as well as standard transfer protocols.</p> <p>2023-02-02: Addition of VSCode and RStudio images to custom workbenches.</p> <p>2023-01-22: Addition of StarProxy to Patterns-&gt;Starburst/Trino Proxy.</p> <p>Starproxy is a fully HTTP compliant proxy that is designed to sit between clients and a Trino/Starburst cluster.</p>"}]}